#+TITLE: SP23M3607 Lectures
#+AUTHOR: Tae Eun Kim, Ph.D.
:DRAWER:
#+STARTUP: overview indent beamer
#+OPTIONS: H:4 toc:nil todo:nil tasks:("DOING") title:nil
#+EXPORT_EXCLUDE_TAGS: noexport
#+PROPERTY: header-args:matlab :eval never-export
#+MACRO: cont @@latex: \color{gray}{\normalsize (cont')} @@
#+TAGS: ignore(i) noexport(n)
:END:

* Config                                                                                :ignore:
#+LATEX_CLASS: beamer
# #+LATEX_CLASS_OPTIONS: [10pt,t,handout,ignorenonframetext,aspectratio=169]
#+LATEX_CLASS_OPTIONS: [10pt,t,presentation,ignorenonframetext,aspectratio=169]
#+LATEX_HEADER: \usepackage[default]{lato}
#+LATEX_HEADER_EXTRA: \tcbset{colback=blue!2.5!white}
#+LATEX_HEADER: \usepackage{tk_beamer1}
#+LATEX_HEADER: \input{tk_packages}
#+LATEX_HEADER: \input{tk_macros}
#+LATEX_HEADER: \input{tk_environ}
#+LATEX_HEADER: \input{tk_ximera}
#+LATEX_HEADER: \overfullrule=5pt
#+LATEX_HEADER: \graphicspath{
#+LATEX_HEADER:   {./assets/}
#+LATEX_HEADER: }
#+LATEX_HEADER: \lstset{inputpath="../codes"}
#+LATEX_HEADER: %% empty frametitle
#+LATEX_HEADER: \newcommand{\eft}{\frametitle{\hfill}}
#+LATEX_HEADER: %% for this
#+LATEX_HEADER: \newcommand{\pp}{~\mathclap{p}p}
#+LATEX_HEADER: \usepackage[ugly]{nicefrac}
#+LATEX_HEADER: \DeclareMathOperator{\fl}{fl}
#+LATEX_HEADER: \newcommand{\rT}{\mathrm{T}}
#+LATEX_HEADER: \newcommand{\vshalf}{\vspace{0.5ex}}


#+LATEX_HEADER: \usepackage{import}
#+LATEX_HEADER: \usepackage{xifthen}
#+LATEX_HEADER: \usepackage{pdfpages}
#+LATEX_HEADER: \usepackage{transparent}

#+LATEX_HEADER: \newcommand{\incfig}[2]{%
#+LATEX_HEADER:   \def\svgwidth{#2}
#+LATEX_HEADER:   \import{../img}{#1.pdf_tex}
#+LATEX_HEADER: }


#+BEAMER_HEADER: \setbeamertemplate{theorems}[numbered]
#+BEAMER_HEADER: %% When in handout mode, show notes as well
#+BEAMER_HEADER: \mode<handout>{%
#+BEAMER_HEADER:   \setbeameroption{show notes}
#+BEAMER_HEADER: }

#+BEAMER_HEADER: \setbeamerfont{alerted text}{series=\bfseries}
#+BEAMER_HEADER: \setbeamercolor{alerted text}{fg=black}
#+BEAMER_HEADER: \setbeamertemplate{frametitle continuation}[from second][\color{gray}\insertcontinuationtext]
# #+BEAMER_HEADER: \title[\course]{\lecTitle}
# #+BEAMER_HEADER: \institute[Ohio State]{\medskip}
# #+BEAMER_HEADER: \date{\lecDate}
# #+BEAMER_HEADER: \author{Tae Eun Kim, Ph.D.}

#+BIND: org-beamer-frame-default-options "allowframebreaks"

# increasing itemsep globally
# (https://tex.stackexchange.com/questions/225736/latex-beamer-define-itemsep-globally)
#+LATEX_HEADER: \usepackage{xpatch}
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \newcommand{\my@beamer@setsep}{%
#+LATEX_HEADER: \ifnum\@itemdepth=1\relax
#+LATEX_HEADER:      \setlength\itemsep{\my@beamer@itemsepi}% separation for first level
#+LATEX_HEADER:    \else
#+LATEX_HEADER:      \ifnum\@itemdepth=2\relax
#+LATEX_HEADER:        \setlength\itemsep{\my@beamer@itemsepii}% separation for second level
#+LATEX_HEADER:        \setlength\topsep{\my@beamer@itemsepi}% separation for second level
#+LATEX_HEADER:      \else
#+LATEX_HEADER:        \ifnum\@itemdepth=3\relax
#+LATEX_HEADER:          \setlength\itemsep{\my@beamer@itemsepiii}% separation for third level
#+LATEX_HEADER:          \setlength\topsep{\my@beamer@itemsepii}% separation for second level
#+LATEX_HEADER:    \fi\fi\fi}
#+LATEX_HEADER: \newlength{\my@beamer@itemsepi}\setlength{\my@beamer@itemsepi}{1.25ex}
#+LATEX_HEADER: \newlength{\my@beamer@itemsepii}\setlength{\my@beamer@itemsepii}{1ex}
#+LATEX_HEADER: \newlength{\my@beamer@itemsepiii}\setlength{\my@beamer@itemsepiii}{0.75ex}
#+LATEX_HEADER: \newcommand\setlistsep[3]{%
#+LATEX_HEADER:     \setlength{\my@beamer@itemsepi}{#1}%
#+LATEX_HEADER:     \setlength{\my@beamer@itemsepii}{#2}%
#+LATEX_HEADER:     \setlength{\my@beamer@itemsepiii}{#3}%
#+LATEX_HEADER: }
#+LATEX_HEADER: \xpatchcmd{\itemize}
#+LATEX_HEADER:   {\def\makelabel}
#+LATEX_HEADER:   {\my@beamer@setsep\def\makelabel}
#+LATEX_HEADER:  {}
#+LATEX_HEADER:  {}
#+LATEX_HEADER:
#+LATEX_HEADER: \xpatchcmd{\beamer@enum@}
#+LATEX_HEADER:   {\def\makelabel}
#+LATEX_HEADER:   {\my@beamer@setsep\def\makelabel}
#+LATEX_HEADER:  {}
#+LATEX_HEADER:  {}
#+LATEX_HEADER: \makeatother

# Footnotes don't appear, possibly because of the =setspace= package. Below is a workaround found in https://tex.stackexchange.com/questions/27626/beamer-footnote-not-showing.
#+LATEX_HEADER: \let\oldfootnote\footnote
#+LATEX_HEADER: \renewcommand\footnote[1][]{\oldfootnote[frame,#1]}

* References                                                                          :noexport:
** FNC
** Gander, Gander, and Kwok: Scientific Computing
** Sauer: Numerical Analysis
** Cheney and Kincaid: Numerical Mathematics and Computing
** Trangenstein: Scientific Computing
** Van Loan: Insight Through Computing
** Van Loan: Introduction to Scientific Computing
** Check this out
http://web.pdx.edu/~gjay/teaching/mth271_2020/html/_CONTENTS.html

** Mark Embree
https://personal.math.vt.edu//embree/
https://personal.math.vt.edu/embree/cmda3606_notes_s20.pdf
https://personal.math.vt.edu/embree/math5466/nanotes.pdf
https://www.caam.rice.edu/~caam553/repo/public_html/index.html
https://www.caam.rice.edu/~caam553/repo/public_html/caam453.pdf

** Nathaniel Johnston (advanced linear algebra)
https://youtube.com/playlist?list=PLOAf1ViVP13jdhvy-wVS7aR02xnDxueuL

** lab materials
https://www-users.cse.umn.edu/~arnold/455.f97/labs.html

** Some disasters attributable to bad numeric computing
https://www-users.cse.umn.edu/~arnold/disasters/disasters.html

* Todos                                                                               :noexport:
** Elementary Mathematical Functions
*** tabulated summary

** Review of Calculus
*** References
- Sauer: Section 0.5
- Cheney and Kincaid: Section 1.2

*** Important Functions
**** Exp and log functions
**** Trig and inverse trig functions
**** Hyperbolic trig and inverse hyperbolic trig functions

*** Limits and Continuity
**** intermediate value theorem
**** L'Hopital

*** Derivatives
**** Linearization
**** optimization
**** Mean Value Theorem
**** gradients

*** Integrals
**** area under curve
**** accumulation
**** Fundamental Theorem of Calculus

*** Sequence and Series
**** growth rate
**** convergence and divergence
**** power series
**** Taylor theorem

*** Parametrization
**** curves
**** surfaces

** Review of Linear Algebra
*** References
- Sauer: Appx A
- Cheney and Kincaid: Appx D
- linear algebra bootcamp

*** fundamental concepts
**** scalars
**** vectors
**** matrices
**** linear combination
**** transpose
**** inner and outer products
**** orthogonality
**** linear transformation
***** scaling
***** rotation
***** translation
***** shear
**** linear independence
**** basis
**** dimension
**** inverse
**** determinant

*** fundamental problems
**** square system of linear equations
**** overdetermined system of linear equations (least squares)
**** eigenvalue problem

** Review of Diff Eqs
*** Complex numbers
*** First-order equations
*** Second-order equations
*** Fourier series

** Plotting
*** function plots
*** data plots
**** log-linear
**** log-log
**** interpretation
*** specialized plots
**** arrows: =compass=
**** vector fields: =quiver=
**** lines: =line=
** Scientific Visualization
*** RGB colors
RGB color cube: See Trangenstein, Vol 1.
** Good introductory examples for each big topics
** Comparison of matrix multiplication
Ref: Sec 1.3 of Bornemann (numerical linear algebra)

5 ways to code =A*B=.

| program       | # for-loop | MATLAB | C & BLAS |
|---------------+------------+--------+----------|
| A*B           |            |        |          |
| column-wise   |            |        |          |
| row-wise      |            |        |          |
| outer product |            |        |          |
| inner product |            |        |          |
| componentwise |            |        |          |

1. column-wise
   #+BEGIN_SRC matlab
C = zeros(m,n);
for j = 1:n
    C(:,j) = A*B(:,j);
end
   #+END_SRC

2. row-wise
   #+BEGIN_SRC matlab
C = zeros(m,n);
for i = 1:m
    C(i,:) = A(i,:)*B;
end
   #+END_SRC

** Introductory Example to Gaussian Elimination
Ref: Sec 3.1 of Gander et al
- Calculation of determinant and inverse using Laplace expansion and Cramer's rule.

** Applications of Interpolation
Ref: Ch 4 of Gander et al
- finding explicit formula for a function underlying a set of data
- image processing: enlarging images, blurring images, etc
- compute intermediate values of a tabulated function
- data compression

* Sample Schedule
|  / | <10>             | <30>                                            |    <10> | <7>   |
| WK | DATE             | TOPIC                                           |     REF | ASMT  |
|----+------------------+-------------------------------------------------+---------+-------|
|    | <2023-01-09 Mon> | Introduction to MATLAB                          |         |       |
|  1 | <2023-01-11 Wed> | Elementary Functions and Conditional Statements |         |       |
|    | <2023-01-13 Fri> | Loops                                           |         |       |
|----+------------------+-------------------------------------------------+---------+-------|
|    | <2023-01-16 Mon> | \mybold{No class (MLK)}                         |         |       |
|  2 | <2023-01-18 Wed> | Arrays                                          |         |       |
|    | <2023-01-20 Fri> | More on Arrays                                  |         | HW1   |
|----+------------------+-------------------------------------------------+---------+-------|
|    | <2023-01-23 Mon> | More on Arrays (continued)                      |         |       |
|  3 | <2023-01-25 Wed> | 2-D Graphics                                    |         |       |
|    | <2023-01-27 Fri> | 3-D Graphics                                    |         | HW2   |
|----+------------------+-------------------------------------------------+---------+-------|
|    | <2023-01-30 Mon> | Floating Point Numbers                          |     1.1 |       |
|  4 | <2023-02-01 Wed> | Problems and Conditioning                       |     1.2 |       |
|    | <2023-02-03 Fri> | Stability of Algorithms                         |     1.3 | PROJ1 |
|----+------------------+-------------------------------------------------+---------+-------|
|    | <2023-02-06 Mon> | Polynomial Interpolation and Linear Systems     | 2.1,2.3 |       |
|  5 | <2023-02-08 Wed> | LU Factorization                                |     2.4 |       |
|    | <2023-02-10 Fri> | PLU Factorization                               |     2.5 | HW3   |
|----+------------------+-------------------------------------------------+---------+-------|
|    | <2023-02-13 Mon> | PLU Factorization (continued)                   |     2.6 |       |
|  6 | <2023-02-15 Wed> | Vector and Matrix Norms                         |     2.7 |       |
|    | <2023-02-17 Fri> | Matrix Condition Number                         |     2.8 | HW4   |
|----+------------------+-------------------------------------------------+---------+-------|
|    | <2023-02-20 Mon> | Least Squares and Overdetermined Systems        |     3.1 |       |
|  7 | <2023-02-22 Wed> | Normal Equation                                 |     3.2 |       |
|    | <2023-02-24 Fri> | Orthogonality and QR Factorization              |     3.3 | HW5   |
|----+------------------+-------------------------------------------------+---------+-------|
|    | <2023-02-27 Mon> | Review for Midterm 1                            |         |       |
|  8 | <2023-03-01 Wed> | \mybold{Midterm 1} (1.1--3.3)                   |         |       |
|    | <2023-03-03 Fri> | Householder Transformation                      |     3.4 |       |
|----+------------------+-------------------------------------------------+---------+-------|
|    | <2023-03-06 Mon> | Computing QR                                    |     3.4 |       |
|  9 | <2023-03-08 Wed> | The Rootfinding Problem                         |     4.1 |       |
|    | <2023-03-10 Fri> | Fixed Point Iteration                           |     4.2 | PROJ2 |
|----+------------------+-------------------------------------------------+---------+-------|
|    | <2023-03-13 Mon> |                                                 |         |       |
|    | <2023-03-15 Wed> | \mybold{Spring Break}                           |         |       |
|    | <2023-03-17 Fri> |                                                 |         |       |
|----+------------------+-------------------------------------------------+---------+-------|
|    | <2023-03-20 Mon> | Convergence of Rootfinding Algorithms           |     4.2 |       |
| 10 | <2023-03-22 Wed> | Newton's Method / Secant Method                 | 4.3,4.4 |       |
|    | <2023-03-24 Fri> | Newton's Method for Nonlinear Systems           |     4.5 | HW6   |
|----+------------------+-------------------------------------------------+---------+-------|
|    | <2023-03-27 Mon> | The Interpolation Problem                       |     5.1 |       |
| 11 | <2023-03-29 Wed> | Piecewise Linear Interpolation                  |     5.2 |       |
|    | <2023-03-31 Fri> | Piecewise Cubic Interpolation                   |     5.3 | HW7   |
|----+------------------+-------------------------------------------------+---------+-------|
|    | <2023-04-03 Mon> | Review for Midterm 2                            |         |       |
| 12 | <2023-04-05 Wed> | \mybold{Midterm 2} (3.4--5.3)                   |         |       |
|    | <2023-04-07 Fri> | Finite Differences                              |     5.4 |       |
|----+------------------+-------------------------------------------------+---------+-------|
|    | <2023-04-10 Mon> | Convergence of Finite Differences               |     5.5 |       |
| 13 | <2023-04-12 Wed> | Numerical Integration                           |     5.6 |       |
|    | <2023-04-14 Fri> | Composite Quadrature Methods                    |     5.6 | HW8   |
|----+------------------+-------------------------------------------------+---------+-------|
|    | <2023-04-17 Mon> | Initial Value Problems                          |     6.1 |       |
| 14 | <2023-04-19 Wed> | Euler's Method                                  |     6.2 |       |
|    | <2023-04-21 Fri> | Runge-Kutta Method                              |     6.3 | HW9   |
|----+------------------+-------------------------------------------------+---------+-------|
| 15 | <2023-04-24 Mon> | Review for Final \rule[-1em]{0pt}{2.5em}        |         |       |

* DONE Chapter 0: Basics of MATLAB
** DONE Introduction to MATLAB
:PROPERTIES:
:EXPORT_FILE_NAME: 01-intro-to-matlab
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** MATLAB as a Calculator
**** Interfaces
- Command Window: interact with MATLAB
- Current Folder: directory view, navigation
- Workspace: list of saved variables

**** Arithmetic Operations
Basic arithmetic operations are handled using /familiar/ symbols.
- Addition (=+=)
- Subtraction (=-=)
- Multiplication (=*=)
- Division (=/=)
- Exponentiation (=^=)

\vs
MATLAB is equipped with an extensive *math library* which contains
- Exponential and logarithmic
- Trigonometric and inverse trigonometric
- and many more
More on this in the next lecture.

**** Useful Commands
- Clearing screen: =clc=
- =format short/long=
- =format loose/compact=
- =format rat=
- =help=

*** Variables
**** Variables
- Predefined variables: =pi=, =i=, =j=, =eps=, =realmax=, =realmin=, =Inf=, =NaN=
- User-defined variables: use the equal symbol (===)
  #+BEGIN_EXAMPLE
 <variable name> = <definition>
  #+END_EXAMPLE

\vs

*Rules of naming.*
- lowercase, uppercase[fn::Thus MATLAB is case sensitive.], numbers, and underscore are available
- no spaces: =my var= $\rightarrow$ =my_var=, =myVar=
- no number at the beginning: =2x= $\rightarrow$ =twoX=, =x2=

**** Clear Variables
- =clear <VAR>=
- =clear <VAR1> <VAR2> ...=
- =clear=

*** Scriting with MATLAB
**** Displaying Text and Numbers
- Your first ``Hello, World!'' program:
  #+ATTR_LATEX: :options style=matlab
  #+BEGIN_SRC matlab
disp('Hello, World!')
  #+END_SRC

- To display text and number side by side:
  #+ATTR_LATEX: :options style=matlab
  #+BEGIN_SRC matlab
disp(['the number is ', num2str(rand())])
  #+END_SRC

**** Script M-File
Write multiple lines of MATLAB statements in a single file, called a script m-file. When asked to run the script, MATLAB executes all statements from top to bottom.

- Running a script: hitting ``Run'' button or calling a script by its name
- Commenting: =%= or =%%= at the beginning of a line
- Suppressing outputs: a semicolon(=;=) at the end of a statement
- Interactive program using ~input~ and ~disp~:
  #+BEGIN_EXAMPLE
 var_name = input('<PROMPT>');
  #+END_EXAMPLE

**** Example: Quadratic Equation Solver

Our first program as a script m-file:

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
% script m-file: quad eqn solver
a = input('the value of a: ');
b = input('the value of b: ');
c = input('the value of c: ');
D = b^2 - 4*a*c;
x1 = (-b + sqrt(D))/(2*a);
x2 = (-b - sqrt(D))/(2*a);
disp(['The first root: ', num2str(x1)])
disp(['The second root: ', num2str(x2)])
#+END_SRC

*** Functions
**** Functions
- A /function/ is a piece of code which
  - performs a specific task;
  - has specific input and output arguments;
  - is encapsulated to be independent of the rest of the program.
- In MATLAB, functions are defined in .m files just as scripts.
- The name of the file and that of the function must coincide.

**** Function M-File
A function m-file must be written in a specific manner.
- When there is no input/output argument
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
function myfun()
    ....
end    % <-- optional
  #+END_SRC

- Where there are multiple input and output arguments
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
function [out1, out2, out3] = myfun(in1, in2, in3, in4)
    ....
end    % <-- optional
  #+END_SRC

**** Calling a Function
- If the function m-file =myfun.m= is saved in your current working directory[fn::The =path= function gives more flexibility in this regard.], you can use it as you would use any other built-in functions:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
% when no input/output argument is required
myfun
  #+END_SRC
  or
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
% multiple inputs/outputs
[out1, out2, out3] = myfun(in1, in2, in3, in4)
  #+END_SRC

- When not all output arguments are needed:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
out1 = myfun(in1, in2, in3, in4)         % only 1st output
[~, ~, out3] = myfun(in1, in2, in3, in4) % only 3rd output
  #+END_SRC
  Note that tilde ( =~= ) is used as a placeholder.

**** Example: Quadratic Equation Solver (Revisited)

We can simply turn the previous quadratic solver script into a function:
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
function myQuadEqnSolverFun(a,b,c)
  D = b^2 - 4*a*c;
  x1 = (-b + sqrt(D))/(2*a);
  x2 = (-b - sqrt(D))/(2*a);
  disp(['The first root: ', num2str(x1)])
  disp(['The second root: ', num2str(x2)])
end
#+END_SRC
*** Live Scripts and Live Functions
**** Basic Interface
- ``New Live Script''
- Text vs code: Toggle using buttons or shortcut
- Output

**** Markup
*Text styles:*
- bold, italic, underlined, monospaced
- left/right aligned, centered
- itemization/enumeration

\vs
*Document structure:*
- Title
- Headings
- Normal

**** Scripts from Live Script
- Print out contents of an external script m-file: use the =type= function
- Run an external script: call by its filename
**** Functions inside Live Script
- Instead of writing an external function m-file, simply write it _at the end of the live script_.
- The functions written in a live script is only available within the live script.

**** Miscellaneous
- Insert: table of contents, images, maths
- Tying maths: \LaTeX syntax works
- Non-executable code block
- Exporting To PDF
- For further information about live script, check out [[https://www.mathworks.com/help/matlab/live-scripts-and-functions.html][MathWorks Help Center]].

** DONE Elementary Functions
:PROPERTIES:
:EXPORT_FILE_NAME: 02-elemfuns
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Trig and Inverse Trig Functions
**** Trig and Inverse Trig Functions
*Trig functions.*
 - =sin=, =cos=, =tan=: inputs in radians
 - =sind=, =cosd=, =tand=: inputs in degrees

\vs
*Inverse trig functions.*
 - =asin=, =acos=, =atan=: outputs in radians
 - =asind=, =acosd=, =atand=: outputs in degrees

**** Two Version of Inverse Tangent
 - =atan(y/x)= computes $\tan^{-1}(y/x)$. It outputs a result in $[-\pi/2, \pi/2]$. The outputs $\pm \pi/2$ are attained when =y/x= evaluates to $\pm$ =Inf=.
 - =atan2(y,x)= computes the angle between the positive $x\text{-axis}$ and the vector $\langle x, y \rangle$. Its output lies in $[-\pi, \pi]$.

*** Exponential and Logarithmic Functions
**** Exponential Functions
 - =exp(x)= for $e^x$
 - The Euler number $e$ is not predefined in MATLAB. Use =exp(1)=.

**** Logarithmic Functions
 - =log(x)= for $\log(x) = \ln(x)$ (natural log function)
 - =log2(x)= for $\log_2(x)$ (base 2)
 - =log10(x)= for $\log_{10}(x)$ (base 10)
 - For logarithms with other bases, use
   \[
   \log_b(a) = \frac{\log(a)}{\log(b)}.
   \]

*** Complex Numbers
**** Complex Numbers
 - =i= and =j= denote $i = \sqrt{-1}$.
 - Since =i= and =j= are commonly used as indices, it is recommended to used =1i= or =1j=, e.g., =3 + 1i=.
 - Even =(number)i= or =(number)j= works, e.g., =3 + 4i=.
**** Cartesian and Polar Components
Given a complex number $z = x + iy$, stored as =z=:
 - =real(z)= calculates $\Re(z) = x$ \hfill (real part)
 - =imag(z)= calculates $\Im(z) = y$ \hfill (imaginary part)
 - =abs(z)= calculates $|z| = \sqrt{x^2+y^2}$ \hfill (modulus)
 - =angle(z)= calculates $\theta \in [-\pi, \pi]$ such that $\tan(\theta) = y/x$ \hfill (argument)

*** Roots
**** Roots
 - =sqrt(a)= for $\sqrt{a}$
 - =nthroot(a,n)= for $\sqrt[n]{a}$
\vs

*Note.* Let $n \in \NN$ and $a \in \RR$. Then =nthroot(a,n)= is computed according to the following:
\begin{equation*}
\sqrt[n]{a} =
\begin{dcases*}
  a^{1/n} & if $a \ge 0$ \\
  -|a|^{1/n} & if $a < 0$
\end{dcases*}
= \sign(a) \abs{a}^{1/n}.
\end{equation*}
The =nthroot= function only returns *at most* one _real-valued root_. In particular, if $a<0$ but $n$ is even, =nthroot(a,n)= returns an error message.

*** Miscellaneous Functions                                                         :noexport:
*** Another Way to Display Text                                                     :noexport:
**** =FPRINTF=: Alternate Displaying Function
Combine literal text with numeric data.
\vs
- Number of digits to display
  #+ATTR_LATEX: :options style=matlab
  #+BEGIN_SRC matlab
fprintf('There are %d days in a year.\n', 365)
  #+END_SRC

  \vs
- Complex number
  #+ATTR_LATEX: :options style=matlab
  #+BEGIN_SRC matlab
z = exp(1i*pi/4);
fprintf('%f+%fi\n', real(z), imag(z));
  #+END_SRC

**** =FPRINTF=: Formatting Operator

# @@latex:\tcbset{colback=white}@@
#+BEGIN_CENTER
#+ATTR_LATEX: :options [width=\linewidth,arc=0mm,boxrule=0.7pt]
#+BEGIN_tcolorbox
#+BEGIN_EXAMPLE
%[field width][precision][conversion character]
#+END_EXAMPLE
#+END_tcolorbox
#+END_CENTER
/e.g./ =%12.5f=.
- =%=: marks the beginning of a formatting operator
- =[field width]=: maximum number of characters to print; optional
- =[precision]= number of digits to the right of the decimal point; optional
- =[conversion character]=

#+LATEX: \begingroup\small
#+ATTR_LATEX: :align c|l
|------+----------------------------------|
| =%d= | integer                          |
|------+----------------------------------|
| =%f= | fixed-point notation             |
|------+----------------------------------|
| =%e= | exponential notation             |
|------+----------------------------------|
| =%g= | the more compact of =%f= or =%e= |
|------+----------------------------------|
| =%s= | string array                     |
|------+----------------------------------|
| =%x= | hexadecimal                      |
|------+----------------------------------|
#+LATEX: \endgroup

** DONE Conditional Statements
:PROPERTIES:
:EXPORT_TITLE: Conditional Statements
:EXPORT_FILE_NAME: 03-conditional
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Relational and Logical Operators
**** Relational Operators

How are two numbers X and Y related?
- =[X>Y]= Is X greater than Y?
- =[X<Y]= Is X less than Y?
- =[X>=Y]= Is X greater than or equal to Y?
- =[X<=Y]= Is X less than or equal to Y?
- =[X==Y]= Is X equal to Y?
- =[X~=Y]= Is X not equal to Y?

The symbols used between X and Y are called the *relational operators*.

**** Logical Variables and Logical Operators

- A relational statement evaluates to either *True(1)* or *False(0)*; these are called *logical variables* or *boolean variables*.

- As arithmetic operators (=+,-,*,/=) put together two numbers and produce other numbers, *logical operators* combine two logical variables to produce other logical variables.

- *Logical Operators*: /and/ (=&&=), /or/ (=||=), /not/ (=~=), /xor/ (=xor=)

**** Logical Operator: =&&= (AND)

Let =A= and =B= be two logical variables. The =&&= operation is completely defined by the following truth table:

#+ATTR_LATEX: :align c|c|c
| =A= | =B= | =A && B= |
|-----+-----+----------|
| F   | F   | F        |
| F   | T   | F        |
| T   | F   | F        |
| T   | T   | T        |

Note that =A && B= is true if and only if both =A= and =B= are true.

**** Logical Operator: =||= (OR)

Let =A= and =B= be two logical variables. The =||= operation is completely defined by the following truth table:

#+ATTR_LATEX: :align c|c|c
| =A= | =B= | =A= \vert\vert =B= |
|-----+-----+--------------------|
| F   | F   | F                  |
| F   | T   | T                  |
| T   | F   | T                  |
| T   | T   | T                  |

Note that =A || B= is false if and only if both =A= and =B= are false.

**** Logical Operator: =xor= (exclusive or)

This is a special variant of the =||= operator.

#+ATTR_LATEX: :align c|c|c
| =A= | =B= | =xor(A,B)= |
|---+---+----------|
| F | F | F        |
| F | T | T        |
| T | F | T        |
| T | T | F        |

Note that =xor(A,B)= is true if only one of =A= or =B= is true.

**** Logical Operator: =~= (NOT)

This is a negation operator.

#+ATTR_LATEX: :align c|c
| =A= | =~A= |
|-----+------|
| F   | T    |
| T   | F    |

**** Combination of Logical Operations

Let =A= and =B= be logical variables. Then =~(A && B)= and =~A || ~B= are equivalent:
\vs

#+LATEX: \begin{minipage}[t]{0.45\linewidth}
#+ATTR_LATEX: :align c|c|c|c
| =A= | =B= | =A && B= | =~(A && B)= |
|-----+-----+----------+-------------|
| F   | F   | F        | T           |
| F   | T   | F        | T           |
| T   | F   | F        | T           |
| T   | T   | T        | F           |
#+LATEX: \end{minipage}
#+LATEX: \hfill
#+LATEX: \begin{minipage}[t]{0.45\linewidth}
#+ATTR_LATEX: :align c|c|c|c|c
| =A= | =B= | =~A= | =~B= | =~A= \vert\vert =~B= |
|-----+-----+------+------+----------------------|
| F   | F   | T    | T    | T                    |
| F   | T   | T    | F    | T                    |
| T   | F   | F    | T    | T                    |
| T   | T   | F    | F    | F                    |
#+LATEX: \end{minipage}

**** =IS=-Functions
There are a number of =is=-functions in MATLAB which evaluate to logical variables. These functions answer such questions as ``Is a number a prime?''; ``Is a number a scalar?''; ``Is an array empty?''

#+BEGIN_EXAMPLE
>> k = 17; isprime(k)
>> M = []; isempty(M)
>> n = 7; isscalar(n)
#+END_EXAMPLE

For more, see Tables 5.2 and 5.3 in *LM*.

*** Control Structure
**** Conditional Statement
***** =IF= Statement
In order to have MATLAB execute a block of statements under a certain condition, we can use an if-statement:
#+BEGIN_EXAMPLE
  if condition #1
    statements #1
  end
#+END_EXAMPLE

**** Conditional Statement {{{cont}}}
***** =IF-ELSE= Statement
If we want a different block of statements to be executed in case the condition was not met, we can modify the above as follows:
#+BEGIN_EXAMPLE
  if condition #1
    statements #1
  else
    statements #2
  end
#+END_EXAMPLE
**** Conditional Statement {{{cont}}}
***** =IF-ELSEIF-ELSE= Statement
The most general form of the conditional statement in MATLAB is the following:
#+BEGIN_EXAMPLE
  if condition #1
    statements #1
  elseif condition #2
    statements #2
  ...
  elseif condition #M
    statements #M
  else
    statements #M+1
  end
#+END_EXAMPLE

**** Notes on Conditional Statements
 - If an =else= statement is present, then exactly one of the statements be executed.
 - If it is omitted, then it is possible that no statements will be executed.
 - The condition must always be a scalar condition.
 - It is preferred that the result of a condition is a logical value. In case the result is a floating-point number:
   - any nonzero number is =true=;
   - the number 0 is =false=.
 - MATLAB checkes conditions sequentially until it finds the first one which is =true=, or till it finds an =else= or =end=.
 - =elseif= is different from =else if=.

*** Examples
**** =FPRINTF=: Alternate Displaying Function
Combine literal text with numeric data.
\vs
- Number of digits to display
  #+ATTR_LATEX: :options style=matlab
  #+BEGIN_SRC matlab
fprintf('There are %d days in a year.\n', 365)
  #+END_SRC

  \vs
- Complex number
  #+ATTR_LATEX: :options style=matlab
  #+BEGIN_SRC matlab
z = exp(1i*pi/4);
fprintf('%f+%fi\n', real(z), imag(z));
  #+END_SRC

**** =FPRINTF=: Formatting Operator

# @@latex:\tcbset{colback=white}@@
#+BEGIN_CENTER
#+ATTR_LATEX: :options [width=\linewidth,arc=0mm,boxrule=0.7pt]
#+BEGIN_tcolorbox
#+BEGIN_EXAMPLE
%[field width][precision][conversion character]
#+END_EXAMPLE
#+END_tcolorbox
#+END_CENTER
/e.g./ =%12.5f=.
- =%=: marks the beginning of a formatting operator
- =[field width]=: maximum number of characters to print; optional
- =[precision]= number of digits to the right of the decimal point; optional
- =[conversion character]=

#+LATEX: \begingroup\small
#+ATTR_LATEX: :align c|l
|------+----------------------------------|
| =%d= | integer                          |
|------+----------------------------------|
| =%f= | fixed-point notation             |
|------+----------------------------------|
| =%e= | exponential notation             |
|------+----------------------------------|
| =%g= | the more compact of =%f= or =%e= |
|------+----------------------------------|
| =%s= | string array                     |
|------+----------------------------------|
| =%x= | hexadecimal                      |
|------+----------------------------------|
#+LATEX: \endgroup

**** Quadratics Revisited

Consider a monic quadratic function $q(x) = x^2 + bx + c$ on a close interval $[L, R]$.
- Critical point: $x_c = -b/2$
- If $x_c \in (L, R)$, $q(x)$ attains the (global) minimum at $x_c$; otherwise, the minimum occurs at one of the endpoints $x = L$ or $x = R$.

***** Question
Write a program which determines whether the critical point of $q(x)$ falls on the interval.

**** Initialization
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
b = input('Enter b: ');
c = input('Enter c: ');
L = input('Enter L: ');
R = input('Enter R (L<R): ');
clc
fprintf('Function: x^2 + bx + c, b = %5.2f, c = %5.2f\n', b, c)
fprintf('Interval: [L, R], L = %5.2f, R = %5.2f\n', L, R)
xc = -b/2;
#+END_SRC

**** Main Fragment

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
if L < xc && xc < R
    fprintf('Interior critical point at x_c = %5.2f\n', xc)
else
    disp('Either xc <= L or xc >= R')
end
#+END_SRC

**** Main Fragment -- another way

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
if xc <= L || xc >= R
    disp('Either xc <= L or xc >= R')
else
    fprintf('Interior critical point at x_c = %5.2f\n', xc)
end
#+END_SRC

**** Main Fragment -- yet another way

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
if ~(xc <= L || xc >= R)
    fprintf('Interior critical point at x_c = %5.2f\n', xc)
else
    disp('Either xc <= L or xc >= R')
end
#+END_SRC

**** The simplest =if= statement?                                                  :noexport:

So far, we have seen
- =if-else= statement
- =if-elseif-else= statement

\vs

The simplest =if= statement is of the form
#+BEGIN_CENTER
#+ATTR_LATEX: :options [width=0.8\linewidth,arc=0mm,boxrule=0.7pt]
#+BEGIN_tcolorbox
#+BEGIN_EXAMPLE
if [condition]
  [statements to run]
end
#+END_EXAMPLE
#+END_tcolorbox
#+END_CENTER

**** Input Errors

If a user mistakenly provides $L$ that is larger than $R$, fix it silently by swapping $L$ and $R$.

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
if L > R
    tmp = L;
    L = R;
    R = tmp;
end
#+END_SRC

I will show you how to send an error message and halt a program later.

*** Exercises
**** Exercise 1: Simple Minimization Problem

# #+BEGIN_QUOTE
# "What is the minimum value of $q(x)$ on $[L, R]$ and where does it occur?"
# #+END_QUOTE

***** Question
Write a program that finds $x_{\rm min} \in [L, R]$ at which $q(x)$ is minimized and the minimum value $q(x_{\rm min})$.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
- This can be done with =if-elseif-else=

**** Exercise 2: Leap Year

***** Question
Write a script which determines whether a given year is a leap year or not.
A year is a leap year if
- it is a multiple of 4;
- it is not a multiple of 100;
- it is a multiple of 400.
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
*Useful:* =mod= function.

**** Pseudocode
#+BEGIN_CENTER
#+ATTR_LATEX: :options [width=0.8\linewidth,arc=0mm,boxrule=0.7pt]
#+BEGIN_tcolorbox
#+BEGIN_EXAMPLE
if [YEAR] is not divisible by 4
    it is a common year
elseif [YEAR] is not divisible by 100
    it is a leap year
elseif [YEAR] is not divisible by 400
    it is a common year
else
    it is a leap year
end
#+END_EXAMPLE
#+END_tcolorbox
#+END_CENTER

**** Exercise 3: Angle Finder
***** Question
Let $x$ and $y$ be given, not both zero. Determine the angle $\theta \in (-\pi, \pi]$ between the positive $x\text{-axis}$ and the line segment connecting the origin to $(x,y)$.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
Four quandrants:
- 1st or 4th ($x>=0$): $\theta = \tan^{-1}(y/x)$
- 2nd ($x<0, y>=0$): $\theta = \tan^{-1}(y/x) + \pi$
- 3rd ($x<0, y<0$): $\theta = \tan^{-1}(y/x) - \pi$

\vs

*Useful*: =atan= (inverse tangent function)

**** Extended Inverse Tangent
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
if x > 0
    theta = atan(y/x)
elseif y >= 0
    theta = atan(y/x) + pi
else
    theta = atan(y/x) - pi
end
#+END_SRC

\vs

- MATLAB provides a function that exactly does this: =atan2(x,y)=.
- *Further Exploration*: What would you do if you are asked to find the angle $\theta \in [0, 2\pi)$, with =atan= alone or with =atan2=?

** DONE Loops
:PROPERTIES:
:EXPORT_FILE_NAME: 04-loops
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Opening Example
**** Approximating $\pi$
:PROPERTIES:
:BEAMER_opt: shrink=15
:END:
***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** Problem                                                                      :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.7
:END:

Suppose the circle $x^2 + y^2 = n^2$, $n \in \NN$, is drawn on graph paper.

\vs

- The area of the circle can be approximated by counting the number uncut grids, $N_{\rm in}$.
  \[
  \pi n^2 \approx N_{\rm in},
  \]
  and so
  \[
  \pi \approx \frac{N_{\rm in}}{n^2}.
  \]
- Using symmetry, may only count the grids in the first quadrant and modify the formula accordingly:
  \[
  \pi \approx \frac{4 N_{\rm in,1}}{n^2},
  \]
  where $N_{\rm in,1}$ is the number of inscribed grids in the first quadrant.

****** figure                                                                       :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:

#+ATTR_LATEX: :width 0.99\textwidth
[[../img/approx_pi.pdf]]

**** Approximating $\pi$
***** Problem Statement
Write a script that inputs an integer $n$ and displays the approximation of $\pi$ by
\[
\rho_n = \frac{4 N_{\rm in,1}}{n^2},
\]
along with the (absolute) error $\abs{\rho_n - \pi}$.

***** Note                                                                 :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs

*Note.*
The approximation gets enhanced and approaches the true value of $\pi$ as $n \to \infty$.

*** Introduction to =FOR=-Loop
**** Strategy: Iterate
:PROPERTIES:
:BEAMER_opt: shrink=15
:END:
***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** text                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.65
:END:
The key to this problem is to count the number of uncut grids in the first quadrant programmatically.
# One way is to count row by row.
\vfill

#+LATEX: \bgroup\small \begin{tcolorbox}[arc=0mm,boxrule=0.7pt]
Set $N_{\rm in,1} = 0$.

\vs
\hspace{0.12\linewidth}
#+ATTR_LATEX: :options [width=0.85\linewidth,arc=0mm,boxrule=0.7pt,colback=white,nobeforeafter]
#+BEGIN_tcolorbox
Count the number of uncut grids in =row 1=.
Add that to $N_{\rm in,1}$.
#+END_tcolorbox

\vspace{0.2em}
\hspace{0.12\linewidth}
#+ATTR_LATEX: :options [width=0.85\linewidth,arc=0mm,boxrule=0.7pt,colback=white,nobeforeafter]
#+BEGIN_tcolorbox
Count the number of uncut grids in =row 2=.
Add that to $N_{\rm in,1}$.
#+END_tcolorbox

\vspace{0.2em}
@@latex:\hspace{0.2\linewidth}@@ \vspace{0.8em}\vdots
\vspace{0.2em}

\hspace{0.12\linewidth}
#+ATTR_LATEX: :options [width=0.85\linewidth,arc=0mm,boxrule=0.7pt,colback=white,nobeforeafter]
#+BEGIN_tcolorbox
Count the number of uncut grids in =row 10=.
Add that to $N_{\rm in,1}$.
#+END_tcolorbox

\vs
Set $\rho_{10} = 4 N_{\rm in,1}/10^2$.
#+LATEX: \end{tcolorbox} \egroup

****** fig                                                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.35
:END:
#+ATTR_LATEX: :width 0.95\textwidth
[[../img/quad_circ_on_grids.pdf]]

**** MATLAB Way
:PROPERTIES:
:BEAMER_opt: shrink=15
:END:
***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** text                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.65
:END:
The repeated counting can be delegated to MATLAB using =for=-loop. The procedure outlined above turns into

\vs
#+LATEX: \bgroup\small\centering \begin{tcolorbox}[width=1.0\linewidth,arc=0mm,boxrule=0.7pt]
Assume =n= is initialized and set $N_{\rm in,1}$ to zero.

\vs
*=for k = 1:n=*

\vs
\hspace{0.12\linewidth}
#+ATTR_LATEX: :options [width=0.85\linewidth,arc=0mm,boxrule=0.7pt,colback=white,nobeforeafter]
#+BEGIN_tcolorbox
Count the number of uncut grids in =row k=.
Add that to $N_{\rm in,1}$.
#+END_tcolorbox

*=end=*

\vs
Set $\rho_{n} = 4 N_{\rm in,1}/n^2$.
#+LATEX: \end{tcolorbox} \egroup

**** Counting Uncut Tiles
***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** text                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
@@latex:\bgroup\small@@
The problem is reduced to counting the number of uncut grids in each row. \vs

- The $x\text{-coordinate}$ of the intersection of the top edge of the $k\text{th}$ row and the circle $x^2 + y^2 = n^2$ is
  \[
  x = \sqrt{n^2 - k^2}.
  \]
- The number of uncut grids in the $k\text{th}$ row is the largest integer less than or equal to this value, /i.e./,
  \[
  \lfloor \sqrt{n^2 - k^2} \rfloor. \tag{floor function}
  \]
- MATLAB provides =floor=.
@@latex:\egroup@@

****** fig                                                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
# [[../img/circ_kth_row.pdf]]
#+LATEX: \begin{figure}[ht] \centering
\incfig{circ_kth_row}{1.0\linewidth}
#+LATEX: \end{figure}

**** Main Fragment Using =FOR=-Loop

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
N1 = 0;
for k = 1:n
    m = floor(sqrt(n^2 - k^2));
    N1 = N1 + m;
end
rho_n = 4*N1/n^2;
#+END_SRC

\vs
*Exercise.* Complete the program.

**** Exercise 1: Overestimation
***** Question
Note that $\rho_n$ is always less than $\pi$. If $N_{1}$ denotes the total number of grids, both cut and uncut, within the quarter disk, then $\mu_n = 4N_1/n^2$ is always larger than $\pi$. Modify the previous (complete) script so that it prints $\rho_n, \mu_n$, and $\mu_n - \rho_n$.
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
- =ceil=, an analogue of =floor=, is useful.

**** Notes on =FOR=-Loop
- The construct is used when a code fragment needs to be repeatedly run. The number of repetition is known in advance.
  #+ATTR_LATEX: :options [numbers=none,backgroundcolor=\color{red!2!white},linewidth=0.92\linewidth]
  #+BEGIN_algorithm
  for '<loop variable>' = 1:'<arithmetic expression>'
  '<code fragment>'
  end
  #+END_algorithm
- Examples:\\
  #+LATEX: \begin{minipage}[t]{0.47\linewidth}
  #+ATTR_LATEX: :options style=matlab
  #+BEGIN_SRC matlab
for k = 1:3
    fprintf('k = %d\n', k)
end
  #+END_SRC
  #+LATEX: \end{minipage}
  \hfill
  #+LATEX: \begin{minipage}[t]{0.47\linewidth}
  #+ATTR_LATEX: :options style=matlab
  #+BEGIN_SRC matlab
nIter = 100;
for k = 1:nIter
    fprintf('k = %d\n', k)
end
  #+END_SRC
  #+LATEX: \end{minipage}

**** Caveats
Run the following script and observe the displayed result.
#+LATEX: \begin{center}\begin{minipage}[t]{0.5\linewidth}
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
for k = 1:3
    disp(k)
    k = 17;
    disp(k)
end
#+END_SRC
#+LATEX: \end{minipage}\end{center}

- The loop header =k = 1:3= guarantees that =k= takes on the values 1, 2, and 3, one at a time even if =k= is modified within the loop body.
- However, it is a recommended practice that the value of the loop variable is /never/ modified in the loop body.

*** Loops and Simulations
**** Simulation Using =rand=
=rand= is a built-in function which generate a (uniform) ``random'' number between 0 and 1. Try:

#+LATEX: \begin{center}\begin{minipage}{0.5\linewidth}
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
for k = 1:10
    x = rand();
    fprintf('%10.6f\n', x);
end
#+END_SRC
#+LATEX: \end{minipage}\end{center}

Let's use this function to solve:
***** Question
A stick with length 1 is split into two parts at a random breakpoint. /On average/, how long is the shorter piece?

**** Program Development -- Single Instance

Consider breaking /one/ stick. \vs

- Random breakage can be simulated with =rand=; denote by $x \in (0,1)$.
- The length of the shorter piece can be determined using =if=-construct; denote by $s \in (0,1/2)$. \vs

  #+LATEX: \begin{center}\begin{minipage}{0.85\linewidth}
  #+ATTR_LATEX: :options style=matlab
  #+BEGIN_SRC matlab
x = rand();     % $x$: the location of breakage
if x <= 0.5     % if $x \le 0.5$
    s = x;      % shorter part has length $x$
else            % otherwise
    s = 1-x     % shorter part has length $1-x$
end
  #+END_SRC
  #+LATEX: \end{minipage}\end{center}

**** Program Development -- Multiple Instances

- Repeat the previous multiple times using a =for=-loop. Pseudocode: if 1000 breaks are to be simulated:
  #+ATTR_LATEX: :options [numbers=none,backgroundcolor=\color{red!2!white},linewidth=0.92\linewidth]
  #+BEGIN_algorithm
  nBreaks = 1000;
  for k = 1:nBreaks
  '<code from previous page>'
  end
  #+END_algorithm
- But how are calculating the /average/ length of the shorter pieces?

**** Calculating Average Using Loop
:PROPERTIES:
:BEAMER_opt: shrink=15
:END:
***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** sum                                                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
Recall how the total number of uncut grids were calculated using iterations.

\vs
#+LATEX: \bgroup\small\centering \begin{tcolorbox}[width=1.0\linewidth,arc=0mm,boxrule=0.7pt]
Assume =n= is initialized and set $N_{\rm in,1}$ to zero.

\vs
*=for k = 1:n=*

\vs
\hspace{0.12\linewidth}
#+ATTR_LATEX: :options [width=0.85\linewidth,arc=0mm,boxrule=0.7pt,colback=white,nobeforeafter]
#+BEGIN_tcolorbox
Count the number of uncut grids in =row k=.
Add that to $N_{\rm in,1}$.
#+END_tcolorbox

*=end=*

\vs
The value of $N_{\rm in, 1}$ is the total numbers of uncut grids.
#+LATEX: \end{tcolorbox} \egroup

****** average                                                                      :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:

Similarly, we can compute an average by:

\vs
#+LATEX: \bgroup\small\centering \begin{tcolorbox}[width=1.0\linewidth,arc=0mm,boxrule=0.7pt]
Assume =n= is initialized and set $s$ to zero.

\vs
*=for k = 1:n=*

\vs
\hspace{0.12\linewidth}
#+ATTR_LATEX: :options [width=0.85\linewidth,arc=0mm,boxrule=0.7pt,colback=white,nobeforeafter]
#+BEGIN_tcolorbox
Simulate a break and find the length of the shorter piece. Add that to $s$.
#+END_tcolorbox

*=end=*

\vs
Set $s_{\rm avg} = s/\mathtt{n}$.
#+LATEX: \end{tcolorbox} \egroup

**** Complete Solution
#+LATEX: \begin{center}\begin{minipage}{0.9\linewidth}
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
nBreaks = 1000;
s = 0;
for k = 1:nBreaks
    x = rand();
    if x <= 0.5
        s = s + x;
    else
        s = s + (1-x);
    end
end
s_avg = s/nBreaks;
#+END_SRC
#+LATEX: \end{minipage}\end{center}

**** Exercise 2: Game of 3-Stick
***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** text                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.65
:END:
******* Game: 3-Stick                                                            :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
Pick three sticks each having a random length between 0 and 1. You win if you can form a triangle using the sticks; otherwise, you lose.
******* ...                                                              :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
******* Question                                                                 :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
Estimate the probability of winning a game of 3-Stick by simulating one million games and counting the number of wins.

****** fig                                                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.95\linewidth
[[../img/3-stick.pdf]]
*** Pop Quiz
**** Understanding Loops
***** Question 1
How many lines of output are produced by the following script?
#+ATTR_LATEX: :options style=matlab, frame=none
#+BEGIN_SRC matlab
for k = 100:200
    disp(k)
end
#+END_SRC

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+ATTR_LATEX: :options {4}
#+BEGIN_multicols
#+LATEX: \setbeamertemplate{enumerate items}[triangle]
#+ATTR_LATEX: :options [A]
 A. 99
 B. 100
 C. 101
 D. 200
#+END_multicols

**** Understanding Loops
***** Question 2
How many lines of output are produced by the following script?
#+ATTR_LATEX: :options style=matlab, frame=none
#+BEGIN_SRC matlab
for k = 100:200
    if mod(k,2) == 0
        disp(k)
    end
end
#+END_SRC

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+ATTR_LATEX: :options {4}
#+BEGIN_multicols
#+LATEX: \setbeamertemplate{enumerate items}[triangle]
#+ATTR_LATEX: :options [A]
 A. 50
 B. 51
 C. 100
 D. 101
#+END_multicols

**** =FOR=-Loop: Tips
- Basic loop header:
  #+ATTR_LATEX: :options [frame=single,linewidth=0.85\linewidth]
  #+BEGIN_algorithm
  for '<loop var> = 1:<ending value>'
  #+END_algorithm

- To adjust starting value:
  #+ATTR_LATEX: :options [frame=single,linewidth=0.85\linewidth]
  #+BEGIN_algorithm
  for '<loop var> = <starting value>:<ending value>'
  #+END_algorithm

- To adjust step size:
  #+ATTR_LATEX: :options [frame=single,linewidth=0.85\linewidth]
  #+BEGIN_algorithm
  for '<loop var> = <starting value>:<step size>:<ending value>'
  #+END_algorithm

**** Examples
- To iterate over 1, 3, 5, \ldots, 9: \hfill [ /step size = 2/ ]
  #+ATTR_LATEX: :options style=matlab
  #+BEGIN_SRC matlab
for k = 1:2:9
  #+END_SRC
  or
  #+ATTR_LATEX: :options style=matlab
  #+BEGIN_SRC matlab
for k = 1:2:10
  #+END_SRC
- To iterate over 10, 9, 8, \ldots, 1: \hfill [ /negative step size/ ]
  #+ATTR_LATEX: :options style=matlab
  #+BEGIN_SRC matlab
for k = 10:-1:1
  #+END_SRC

*** Introduction to =WHILE=-Loop
**** Need for Another Loop
- =For=-loops are useful when the number of repetitions is known in advance. \vs

  #+BEGIN_CENTER
  \color{blue!75!white}
  "/Simulate the tossing of a fair coin 100 times and print the number of Heads./"
  #+END_CENTER
  \vs

- It is not very suitable in other situations such as \vs

  #+BEGIN_CENTER
  \color{blue!75!white}
  "/Simulate the tossing of a fair coin until the gap between the number of Heads and that of Tails reaches 10./"
  #+END_CENTER
  \vs

  We need another loop construct that terminates as soon as $\abs{N_{\rm H} - N_{\rm T}}=10$.

**** =WHILE=-Loop Basics
=WHILE=-loop is used when a code fragment needs to be executed repeatedly /while/ a certain condition is true.
#+ATTR_LATEX: :options [numbers=none,backgroundcolor=\color{red!2!white},linewidth=0.92\linewidth]
#+BEGIN_algorithm
while '<continuation criterion>'
'<code fragment>'
end
#+END_algorithm

- The number of repetitions is /not/ known in advance.

- The continuation criterion is a boolean expression, which is evaluated at the start of the loop.
  - If it is true, the loop body is executed. Then the boolean expression is evaluated again.
  - If it is false, the flow of control is passed to the end of the loop.

**** Simple =WHILE=-Loop Examples
#+LATEX: \begin{minipage}[t]{0.47\linewidth}
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
k = 1; n = 10;
while k <= n
    fprintf('k = %d\n', k)
    k = k+1;
end
#+END_SRC
#+LATEX: \end{minipage}
\hfill
#+LATEX: \begin{minipage}[t]{0.47\linewidth}
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
k = 1;
while 2^k < 5000
    k = k+1;
end
fprintf('k = %d\n', k)
#+END_SRC
#+LATEX: \end{minipage}

**** =FOR=-Loop to =WHILE=-Loop
A =for=-loop can be written as a =while=-loop. For example,
\vs

#+LATEX: \begin{minipage}[t]{0.48\linewidth}
_*FOR*_
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
s = 0;
for k = 1:4
    s = s + k;
    fprintf('%2d %2d\n', k, s)
end
#+END_SRC
#+LATEX: \end{minipage}
\hfill
#+LATEX: \begin{minipage}[t]{0.48\linewidth}
_*WHILE*_
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
k = 0; s = 0;
while k < 4
    k = k + 1; s = s + k;
    fprintf('%2d %2d\n', k, s)
end
#+END_SRC
#+LATEX: \end{minipage}
\vs

- Note that =k= needed to be initialized before the =while=-loop.
- The variable =k= needed to be updated inside the =while=-loop body.
*** Examples
**** Up/Down Sequence
***** Question
Pick a random integer between 1 and 1,000,000. Call the number $n$ and repeat the following process:
- If $n$ is even, replace $n$ by $n/2$.
- If $n$ is odd, replace $n$ by $3n + 1$.
Does it ever take more than 1000 updates to reach 1?

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
- To generate a random integer between $1$ and $k$, use =randi=, /e.g./,
  #+ATTR_LATEX: :options style=matlab, frame=none
  #+BEGIN_SRC matlab
randi(k)
  #+END_SRC
- To test whether a number $n$ is even or odd, use =mod=, /e.g./,
  #+ATTR_LATEX: :options style=matlab, frame=none
  #+BEGIN_SRC matlab
mod(n, 2) == 0
  #+END_SRC

**** Attempt Using =FOR=-Loop
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
for step = 1:1000
    if mod(n,2) == 0
        n = n/2;
    else
        n = 3*n + 1;
    end
    fprintf(' %4d %7d\n', step, n)
end
#+END_SRC

\vs
- Note that once $n$ becomes $1$, the central process yields the following pattern:
  \[
  1, 4, 2, 1, 4, 2, 1, \ldots
  \]
- This program continues to run even after $n$ becomes 1.

**** Solution Using =WHILE=-Loop
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
step = 0;
while n > 1
    if mod(n,2) == 0
        n = n/2;
    else
        n = 3*n + 1;
    end
    step = step + 1;
    fprintf(' %4d %7d\n', step, n)
end
#+END_SRC

\vs
- This shuts down when $n$ becomes 1!

**** Exercise: Gap of 10
***** Question
Simulate the tossing of a fair coin until the gap between the number of Heads and that of Tails reaches 10.

**** Summary
:PROPERTIES:
:BEAMER_opt: fragile
:END:
- =For=-loop is a programming construct to execute statements repeatedly.
  #+ATTR_LATEX: :options [numbers=none,backgroundcolor=\color{red!2!white},linewidth=0.92\linewidth]
  #+BEGIN_algorithm
  for '<loop index values>'
  '<code fragment>'
  end
  #+END_algorithm

- =While=-loop is another construct to repeatedly execute statements. Repetition is controlled by the termination criterion.
  #+ATTR_LATEX: :options [numbers=none,backgroundcolor=\color{red!2!white},linewidth=0.92\linewidth]
  #+BEGIN_algorithm
  while '<termination criterion is not met>'
  '<repeat these statements>'
  end
  #+END_algorithm

** DONE Arrays
:PROPERTIES:
:EXPORT_FILE_NAME: 05-arrays
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Basics of Arrays in MATLAB
**** Introduction to Arrays

# In order to explore numerous interesting applications using MATLAB,
# it is essential to handle efficiently large amounts of data stored
# in the form of /vectors/ or /matrices/, which are often
# collectively called *arrays*.

Vectors and matrices are often collectively called *arrays*.
\vs

***** Notation
- $\RR^m$ (or $\CC^m$): the set of all real (or complex) *column vectors* with $m$ elements.
- $\RR^{m\times n}$ (or $\CC^{m\times n}$): the set of all real (or complex) $m \times n$ matrices.
- If $\mathbf{v} \in \RR^m$ with $\mathbf{v}=(v_1, v_2, \ldots, v_m)^{\rm T}$, then for $1 \le i \le m$, $v_i \in \RR$ is called the $i\text{th}$ /element/ or the $i\text{th}$ /index/ of $\mathbf{v}$.
- If $A \in \RR^{m \times n}$ with $A = (a_{i,j})$, then for $1 \le i \le m$ and $1 \le j \le n$, $a_{i,j} \in \RR$ is the element in the $i\text{th}$ row and $j\text{th}$ column of $A$.

**** Creating Arrays
***** text                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
- A /row vector/ is created by
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
x = [1 3 5 7];
x = [1,3,5,7];
  #+END_SRC
- A /column vector/ is created by
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
y = [6; 1; 4];
y = [6 1 4].';
  #+END_SRC
- A matrix is formed by
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
A = [3 1 2 3;
     1 5 6 5;
     4 9 5 8];
  #+END_SRC

***** fig                                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
[[../img/array_creation.pdf]]

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
@@latex:{\small@@
The MATLAB expression =x.'= means $\mathbf{x}^{\rm T}$ while =x'= means $\mathbf{x}^{\rm H} = (\mathbf{x}^*)^{\rm T}$.
@@latex:}@@

**** Shape of Arrays
***** text                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.62
:END:
- To find the number of elements of a vector:
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
length(x)
length(y)
  #+END_SRC
- To find the number of rows/columns of an array:
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
size(A,1) % # of rows
size(A,2) % # of cols
size(A)   % both
  #+END_SRC

- To find the total number of elements of an array:
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
numel(A)
  #+END_SRC

*****                                                                                :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.38
:END:
#+ATTR_LATEX: :width 0.7\linewidth
[[../img/array_shape.pdf]]

**** Shape of Arrays (Notes)
- For a matrix =A=, =length(A)= yields the larger of the two dimensions.
- The result of =size(A)= can be stored in two different ways:
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
szA = size(A)
[m, n] = size(A)
  #+END_SRC
  *Q.* How are they different?
- All of the following generate /empty arrays/.
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
[]
[1:0]
[1:0].'
  #+END_SRC
  *Q.* What are their /sizes/? What are their =numel= values?
**** Getting/Setting Elements of Arrays
***** text                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
- To access the $i\text{th}$ element of a vector:
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
x(2)
y(3)
  #+END_SRC
- To access the $(i,j)\text{-element}$ of a matrix:
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
A(2,4)
  #+END_SRC
- To assign values to a specific element:
  #+ATTR_LATEX: :options style=matlab,  linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
x(2) = 2
A(2,4) = 0
  #+END_SRC

- @@latex:\color{blue}@@ _Indices start at *1* in MATLAB, not at 0!_

***** fig                                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :width 0.7\linewidth
[[../img/array_index.pdf]]

**** Linear Indexation and Straightening of Matrix
***** text                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
- MATLAB uses /column-major/ layout by default, meaning that the elements of the columns are contiguous in memory.
- Consequently, one can get/set an element of a matrix using a single index.
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
A(8)
  #+END_SRC
- An array can be put into a column vector using
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
A(:)
  #+END_SRC

***** fig                                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :width 0.8\linewidth
[[../img/array_linear_index.pdf]]

*** Array Operations
**** Two Kinds of Transpose
***** text                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
- The transpose of an array: $A^{\rm T}$
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
A.'
  #+END_SRC
- The conjugate transpose of an array: $A^{\rm H} = A^* = \conj{A}^{\rm T}$
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
A'
  #+END_SRC
- If $A \in \RR^{m \times n}$, $A^{\rm H} = A^{\rm T}$. So, if =A= is a real array, =A.'= and =A'= are equivalent.
***** fig                                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :width 0.7\linewidth
[[../img/array_transpose.pdf]]

**** Standard Arithmetic Operation
# Keep in mind that we may view vectors as a special kind of matrices,
# e.g., $\RR^m = \RR^{m \times 1}$. Thus, for the sake of simplicity,
# we summarize operations among vectors and matrices solely in
# terms of matrices when no confusion arises. Also note that we will
# only work over $\RR$.

/Standard arithmetic operations/ seen in linear algebra are executed using the familiar symbols.
\vs

- Let =A,B= $\in \RR^{m \times n}$ and =c= $\in \RR$.
  - =A= $\pm$ =B=: elementwise addition/subtraction \hfill ($A \pm B$)
  - =A= $\pm$ =c=: /shifting/ all elements of =A= by $\pm$ =c= \hfill ($A \pm c$)
- Let =A= $\in \RR^{m \times p}$, =B= $\in \RR^{p \times n}$, and =c= $\in \RR$.
  - =A*B=: the $m \times n$ matrix obtained by the /linear algebraic/ multiplication \hfill ($AB$)
  - =c*A=: scalar multiple of =A= \hfill ($cA$)
- Let =A= $\in \RR^{m \times m}$ and =n= $\in \NN$.
  - =A^n=: the =n=-th power of =A=; the same as =A*A*= $\cdots$ =*A= (=n= times) \hfill ($A^n$)

**** Standard Arithmetic Operation -- Inner Products
Let $\bx, \by \in \RR^m$ be column vectors. The /inner product/ of $\bx$ and $\by$ is calculated by
\[
\bx^{\rm T} \by = x_1 y_1 + x_2 y_2 + \cdots + x_m y_m = \sum_{j=1}^{m} x_j y_j \in \RR.
\]
In MATLAB, simply type =x'*y=.

# #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
# #+BEGIN_SRC matlab
# x' * y
# #+END_SRC
\vs
#+ATTR_LATEX: :width 0.5\linewidth
[[../img/array_inner_product.pdf]]

**** Standard Arithmetic Operation -- Outer Products
Let $\bx \in \RR^m, \by \in \RR^n$ be column vectors. The /outer product/ of $\bx$ and $\by$ is calculated by
\begin{equation*}
\bx \by^{\rm T} =
\begin{bmatrix}
  x_1 y_1 & x_1 y_2 & \cdots & x_1 y_n \\
  x_2 y_1 & x_2 y_2 & \cdots & x_2 y_n \\
  \vdots & \vdots & \ddots & \vdots \\
  x_m y_1 & x_m y_2 & \cdots & x_m y_n
\end{bmatrix} \in \RR^{m \times n}.
\end{equation*}

In MATLAB, simply type =x*y'=.

# #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
# #+BEGIN_SRC matlab
# x' * y
# #+END_SRC
\vs
#+ATTR_LATEX: :width 0.5\linewidth
[[../img/array_outer_product.pdf]]

**** Elementwise Multiplication (=.*=)
***** text                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
@@latex:\bgroup\small@@
- To multiply entries of two arrays of same size, element by element:
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
x .* y
  #+END_SRC
@@latex:\egroup@@

***** fig                                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :width 0.75\linewidth
[[../img/array_elementwise_mult.pdf]]

**** Elementwise Division (=./=)
***** text                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
@@latex:\bgroup\small@@
- To divide entries of an array by corresponding entries of another same-sized array:
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
x ./ y
  #+END_SRC
- To divide a number by multiple numbers (specified by entries of an array):
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
s ./ y
  #+END_SRC
- To divide all entries of an array by a common number:
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
x / s
  #+END_SRC
@@latex:\egroup@@

***** fig                                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :width 0.7\linewidth
[[../img/array_elementwise_div.pdf]]

**** Elementwise Exponentiation (=.^=)

***** text                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
@@latex:\bgroup\small@@
- To raise all entries of an array to (different) powers:
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
x .^ y
  #+END_SRC
- To raise a number to multiple powers (specified by entries of an array):
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
s .^ x
  #+END_SRC
- To raise all entries of an array to a common power:
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
x .^ s
  #+END_SRC
@@latex:\egroup@@

***** fig                                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :width 0.7\linewidth
[[../img/array_elementwise_exp.pdf]]

**** Mathematical Functions

***** text                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
@@latex:\bgroup\small@@
- Built-in mathematical functions accept array inputs and return arrays of function evaluation, /e.g./,
  #+ATTR_LATEX: :options style=matlab, linewidth=0.7\linewidth
  #+BEGIN_SRC matlab
sqrt(A)
sin(A)
mod(A)
...
  #+END_SRC
@@latex:\egroup@@

***** fig                                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :width 0.75\linewidth
[[../img/array_elementwise_func.pdf]]

*** Array Constructors
**** Colon Operator
#+LATEX: \begingroup \small
Suppose =a < b=.
- To create an arithmetic progression from =a= to =b= (increment by 1):
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
a:b
  #+END_SRC
  The result is a row vector =[a, a+1, a+2, ..., a+m]=, where
  #+BEGIN_CENTER
  ~m =~ $\lfloor$ =b-a= $\rfloor$.
  #+END_CENTER
- To create an arithmetic progression from =a= to =b= with steps of size =d > 0=:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
a:d:b
  #+END_SRC
  The result is a row vector =[a, a+d, a+2*d, ..., a+m*d]=, where
  #+BEGIN_CENTER
  ~m =~ $\lfloor$ =(b-a)/d= $\rfloor$.
  #+END_CENTER
#+LATEX: \endgroup

**** =LINSPACE= and =LOGSPACE=
#+LATEX: \begingroup \small
- To create a row vector of =n= numbers evenly spaced between =a= and =b=:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
linspace(a, b, n)
  #+END_SRC
  The result is =[a, a+d, a+2*d, ..., b]=, where
  #+BEGIN_CENTER
  =d = (b-a)/(n-1)=.
  #+END_CENTER

- To create a row vector of =n= numbers that are logarithmically evenly spaced between $10^{\tt a}$ and $10^{\tt b}$:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
logspace(a, b, n)
  #+END_SRC
  The result is $[10^{\tt a},\, 10^{\tt a+d},\, 10^{\tt a+2d},\, \ldots,\, 10^{\tt b}]$, where
  # \begin{equation*}
  #   \begin{bmatrix}
  #     10^{\tt a} & 10^{\tt a+d} & 10^{\tt a+2d} & \ldots & 10^{\tt b}
  #   \end{bmatrix}
  # \end{equation*}
  #+BEGIN_CENTER
  =d = (b-a)/(n-1)=.
  #+END_CENTER


#+LATEX: \endgroup

**** =ZEROS=, =ONES=, and =EYE=
***** text                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
- To create an $(m \times n)$ zero matrix:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
zeros(m, n)
  #+END_SRC
- To create an $(m \times n)$ matrix all whose entries are one:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
ones(m, n)
  #+END_SRC
- To create the $(m \times m)$ identity matrix:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
eye(m)
  #+END_SRC
***** fig                                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :width 0.5\linewidth
[[../img/array_zeros_ones_eye.pdf]]

**** Random Arrays
Each of the following generates an $(m \times n)$ array of random numbers:
- =rand(m,n)=: uniform random numbers in $(0,1)$
- =randi(k,m,n)=: uniform random integers in $[1,k]$
- =randn(m,n)=: Gaussian random numbers with mean 0 and standard deviation 1

**** Random Arrays (Application)
To generate an $(m \times n)$ array of
- uniform random numbers in $(a,b)$:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
a + (b - a)*rand(m, n)
  #+END_SRC
- uniform random integers in $[k_1, k_2]$:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
randi([k1, k2], m, n)
  #+END_SRC
- Gaussian random numbers with mean $\mu$ and standard deviation $\sigma$:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
mu + sig*randn(m, n)
  #+END_SRC

*** Building Arrays Out Of Arrays
**** Concatenation
If two arrays =A= and =B= have /comparable/ sizes, we can concatenate them.
\vs

#+ATTR_LATEX: :options {2}
#+BEGIN_multicols
- horizontally by =[A B]=
  \vs
  #+ATTR_LATEX: :width 0.8\linewidth
  [[../img/array_stack_horz.pdf]]
- vertically by =[A; B]=
  \vs
  #+ATTR_LATEX: :width 0.8\linewidth
  [[../img/array_stack_vert.pdf]]
#+END_multicols

**** =RESHAPE= and =REPMAT=
#+ATTR_LATEX: :options {2}
#+BEGIN_multicols
- =reshape(A, m, n)= reshapes the array =A= into an $m \times n$ matrix whose elements are taken /columnwise/ from =A=.
  \vs
  #+ATTR_LATEX: :width 0.8\linewidth
  [[../img/array_reshape.pdf]]
- =repmat(A, m, n)= replicates the array =A=, $m$ times vertically and $n$ times horizontally.
  \vs
  #+ATTR_LATEX: :width 0.8\linewidth
  [[../img/array_repmat.pdf]]
#+END_multicols

**** =FLIP=
- Type =help flip= on the Command Window and learn about =flip= function.
- Do the same with its two variants, =flipud= and =fliplr=

**** Creating Diagonal Matrices
#+LATEX: \vspace{-1em}
***** text                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
- To create a diagonal matrix
  \begin{equation*}
  \begin{bmatrix}
  v_1 & 0 & 0 & \cdots & 0 \\
  0 & v_2 & 0 & \cdots & 0 \\
  0 & 0 & v_3 & \cdots & 0 \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & 0 & \cdots & v_n
  \end{bmatrix}:
  \end{equation*}
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
diag(v)
  #+END_SRC

***** fig                                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
[[../img/array_create_diag.pdf]]

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
*Note.*
- =diag(v,k)= puts the elements of =v= on the =k=-th super-diagonal.
- =diag(v,-k)= puts the elements of =v= on the =k=-th sub-diagonal.

**** Extracting Diagonal Elements
Use ~diag(A,k)~ to extract the =k=-th diagonal of =A=. ~diag(A)~ is short for ~diag(A,0)~.

#+ATTR_LATEX: :options {2}
#+BEGIN_multicols
- =k > 0= for super-diagonals:
  \vs
  #+ATTR_LATEX: :width 0.6\linewidth
  [[../img/array_extract_supdiag.pdf]]
- =k < 0= for sub-diagonals:
  \vs
  #+ATTR_LATEX: :width 0.6\linewidth
  [[../img/array_extract_subdiag.pdf]]
#+END_multicols

*** Slicing Arrays
**** Using Vectors as Indices
To get/set multiple elements of an array at once, use vector indices.
***** text                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.55
:END:
- To grab 3rd, 4th, and 5th elements of =x=:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
x(3:5)  % or x([3 4 5])
  #+END_SRC
- To grab 3rd to 8th elements of =x=:
  # that is, to grab elements from the 3rd to the last:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
x(3:8)
x(3:end)
  #+END_SRC
- To grab 3rd to 7th elements of =x=:
  # , that is, to grab elements from the 3rd to second to the last:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
x(3:7)
x(3:end-1)
  #+END_SRC

***** fig                                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.45
:END:
#+ATTR_LATEX: :width 0.8\linewidth
[[../img/array_slice_vec.pdf]]

**** Using Vectors as Indices -- Example
***** text                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
- To extract 2nd, 3rd, and 4th columns of the 2nd row of =A=:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
A(2,2:4)  % or A(2,[2 3 4])
  #+END_SRC
- To extract the entire 2nd row of =A=:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
A(2,1:5)
A(2,1:end)
A(2,:)
  #+END_SRC

***** fig                                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :width 0.6\linewidth
[[../img/array_slice_mat_row.pdf]]

**** Using Vectors as Indices -- Example
***** text                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
- To extract 2nd through 5th elements of the 4th column of =A=:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
A([2 3 4 5],4)
A(2:5,4)
A(2:end,4)
  #+END_SRC
- To extract the entire 4th column of =A=:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
A(1:5,4)
A(1:end,4)
A(:,4)
  #+END_SRC

***** fig                                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :width 0.6\linewidth
[[../img/array_slice_mat_col.pdf]]

**** Using Vectors as Indices -- Example
***** text                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
- To grab the /interior block/ of =A=:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
A(2:4,2:4)
A(2:end-1,2:end-1)
  #+END_SRC
- To extract every other elements on every other rows as shown:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
A(1:2:5,1:2:5)
A(1:2:end,1:2:end)
  #+END_SRC


***** fig                                                                            :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :width 0.6\linewidth
[[../img/array_slice_mat_misc.pdf]]

** DONE More on Arrays
:PROPERTIES:
:EXPORT_FILE_NAME: 06-more-on-arrays
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Recap: Creating Arrays Examples
**** Arithmetic Progressions
***** Question
Create the following /periodic/ arithmetic progressions using ONE MATLAB statement.
\[
(1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0).
\]

***** Answer                                                               :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
m = 5;
n = 15;
mod([1:n], m)
#+END_SRC

**** Exercise: Arithmetic Progressions
***** Question
Create each of the following /row/ vectors using ONE MATLAB statement.
- $\mathbf{v} = (1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0)$
- $\mathbf{w} = (1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4)$

**** Geometric and Other Progressions
***** Question
Create each of the following /column/ vectors using ONE MATLAB statement.
- $\mathbf{v} = (1, 2, 4, 8, \ldots, 1024)^{\rm T}$
- $\mathbf{w} = (1, 4, 9, 16, \ldots, 100)^{\rm T}$

***** Answer                                                               :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
Using the colon operator:
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
v = ( 2.^[0:10] )'
w = ( [1:10].^2 )'
#+END_SRC

Using the =linspace= function:
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
v = ( 2.^linspace(0, 10, 11) )'
w = ( linspace(1, 10, 10).^2 )'
#+END_SRC

**** Function Evaluation
Recall that mathematical functions such as =sin, sind, log, exp= accept array inputs and return arrays of function evaluation.
***** Question
Create each of the the following /row/ vectors using ONE MATLAB statement.
- $\mathbf{u} = (1!, 2!, 3!, \ldots, n!)$
- $\mathbf{v} = (\sin 0^\circ, \sin 30^\circ, \sin 60^\circ, \ldots, \sin 180^\circ)$
- $\mathbf{w} = (e^1, e^4, e^9, \ldots, e^{64})$
***** Answer                                                               :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
v = sind(0:30:180)
w = exp([1:8].^2)
#+END_SRC

**** Matrices with Patterns
***** Question
Generate each of the following matrices using ONE MATLAB statement.
\begin{equation*}
  A =
  \begin{bmatrix}
    1 & 2 & 3 & 4 \\
    5 & 6 & 7 & 8 \\
    9 & 10 & 11 & 12 \\
    13 & 14 & 15 & 16
  \end{bmatrix}, \quad
  B =
  \begin{bmatrix}
    1 & 1^2 & 1^3 & \cdots & 1^{10} \\
    2 & 2^2 & 2^3 & \cdots & 2^{10} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    10 & 10^2 & 10^3 & \cdots & 10^{10}
  \end{bmatrix}.
\end{equation*}
***** Answer                                                               :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
A = reshape(1:16, 4, 4)'
B = ((1:10)').^(1:10)
#+END_SRC

**** Matrices with Patterns
***** Question
Suppose =n= is already stored in MATLAB. Generate each of the following matrices using ONE MATLAB statement. All the elements not shown are 0's.
@@latex:\bgroup\small@@
\begin{equation*}
  C =
  \begin{bmatrix}
    2 & & & & \\
    & 4 & & & \\
    & & 6 & & \\
    & & & \ddots & \\
    & & & & 2n
  \end{bmatrix},
  D =
  \begin{bmatrix}
    \cos 1 & -3 & & & & \\
    & \cos 2 & -3 & & & \\
    & & \cos 3 & -3 & & \\
    & & & \ddots & \ddots & \\
    & & & & \cos (n-1) & -3 \\
    & & & & & \cos n
  \end{bmatrix}.
\end{equation*}
@@latex:\egroup@@

***** Answer                                                               :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
C = diag(2:2:2*n)
D = diag(cos(1:n)) - 3*diag(ones(n-1,1), 1)
#+END_SRC

*** Data Manipulation Functions
**** Data Manipulation Functions
There are a number of MATLAB functions with /spreadsheet functionalities/ that are suitable for data manipulation.
- =max= and =min=
- =sum= and =prod=
- =cumsum= and =cumprod= (cumulative sum and product)
- =diff=
- =mean, std,= and =var= (simple statistics)
- =sort=

**** Example 1: Finding the Maximum Value of a Vector
***** Question
Write a program to find the maximum value of a vector.

***** Answer                                                               :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
- *With loops:*
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
% input: x
% output: m	% DON'T USE max FOR THE VARIABLE NAME
m = x(1);	% CODE ABORTS IF THE VECTOR IS EMPTY
for r = 2:length(x)
    if m < x(r)
        m = x(r);
    end
end
  #+END_SRC

- *Vectorized code:*
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
m = max(x)
  #+END_SRC

**** Example 1: Finding the Maximum Value of a Vector {{{cont}}}
***** Question
Now modify the previous program to find both the maximum value of a vector and the corresponding index.
***** Answers                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** Answer 1                                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
- *With loops:*
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
% input: x
% output: m, index_m
m = x(1);
index_m = 1;
for r = 2:length(x)
    if m < x(r)
        m = x(r);
        index_m = r;
    end
end
  #+END_SRC

****** Answer 2                                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
- *Vectorized code:*
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
[m, index_m] = max(x)
  #+END_SRC

***** COMMENT Answer                                                       :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
- *With loops:*
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
% input: x
% output: m, index_m
m = x(1);
index_m = 1;
for r = 2:length(x)
    if m < x(r)
        m = x(r);
        index_m = r;
    end
end
  #+END_SRC

- *Vectorized code:*
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
[m, index_m] = max(x)
  #+END_SRC

**** Example 2: Summing Elements in a Vector
***** Question
Sum all elements in a vector.

***** Answer                                                               :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
- *With loops:*
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
% input: x
% output: s	% DON'T USE sum FOR THE VARIABLE NAME
s = 0;		% s begins before the first iteration
for el = 1:length(x)
    s = s + x(el);
end
  #+END_SRC

- *Vectorized code:*
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
s = sum(x)
  #+END_SRC

**** =FIND= Function
# To locate elements of an array satisfying simple conditions, use =find= command.
***** Basic Usage of =FIND=
Let =v= be an array of numbers (can be a vector or a matrix). Then

#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth, frame=none
#+BEGIN_SRC matlab
find(<condition>)
#+END_SRC

returns the (linear) indices of =v= satisfying =<condition>=.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
*Some examples of =<condition>=:*
#+BEGIN_CENTER
~v > k,   v >= k,   v < k,   v <= k,   v == k,   v ~= k~
#+END_CENTER

\vs
*To combine more than two conditions:* Use =&= (/and/) or =|= (/or/)

**** Example 3: Comparing Elements in Vectors
***** Question
Compare two real vectors of the same length, say =x= and =y=, elementwise and determine how many elements of the former are larger than the latter.

***** Answer                                                               :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
- *With loops:*
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
% input: x, y
% output: nr_gt
nr_gt = 0;
for k = 1:length(x)
    if x(k) > y(k)
        nr_gt = nr_gt + 1;
    end
end
  #+END_SRC

- *Vectorized code:*
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
nr_gt = length(find(x > y))
  #+END_SRC

*** Timing in MATLAB
**** CPU Time
=cputime= reads total CPU time used by MATLAB from the time it was started.
- Single measurement:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
ct = cputime;   % total cputime as of now
    <statements>
t = cputime - ct;
  #+END_SRC

- Average CPU time:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
ct = cputime;   % total cputime as of now
for i = 1:nr_reps
    <statements>
end
t_avg = (cputime - ct)/nr_reps;
  #+END_SRC

**** Elapsed Time
At the execution of =tic=, MATLAB records the internal time (in seconds); at =toc= command, MATLAB displays the elapsed time.

- Single measurement:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
tic     % starts a stopwatch timer
    <statements>
toc     % reads the elapsed time from tic
  #+END_SRC
- Average elapse time:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
tic    % starts a stopwatch timer
for i = 1:nr_reps
    <statements>
end
t_avg = toc/nr_reps;
  #+END_SRC

**** What Do You Think It Does?
Below is a modified version of an example code from MATLAB's Help documentation for =tic=. What do you think it's doing?

#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
REPS = 1000; minTime = Inf; nSum = 10;
tic;
for i  = 1:REPS
    tStart = tic;
    s = 0;
    for j = 1:nsum
        s = s + besselj(j,REPS);
    end
    tElpased = toc(tStart);
    minTime = min(tElapsed, minTime);
end
t_avg = toc/REPS;
#+END_SRC

**** Example 4: Timing Elementwise Operations
***** Question
Generate a $10^7 \times 1$ random vector and measure the internal time and CPU time when computing elementwise squares.

***** Answer                                                               :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
n = 1e7;
x = rand(n, 1);
t = cputime;
x1 = x.^2;
time1 = cputime - t;

tic
x2 = x.^2;
time2 = toc();
disp([time1, time2])
#+END_SRC

*** Exercises: Vectorization @@latex: \\ @@ (Going Loopless!)
**** Pythagorean Triples
***** Question
Given $n \in \NN$, find all triples $(a,b,c) \NN^3$, with $a, b \le n$, satisfying
\[
a^2 + b^2 = c^2.
\]
***** Notation                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
*Notation.*
- $\NN$: the set of all natural numbers, $1, 2, 3, \ldots$.
- $\NN[1,n] = \{1, 2, \ldots, n\}$.
- $\NN^3 = \{ (a,b,c) \,\mid\, a, b, c \in \NN \}$.
**** Pythagorean Triples -- Solution Using Loops
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
% input: n
% output: M
iM = 0;
M = [];
for a = 1:n
    for b = 1:n
        c = sqrt(a^2 + b^2);
        if mod(c, 1) == 0
            iM = iM + 1;
            M(iM, :) = [a, b, c];
        end
    end
end
#+END_SRC

**** Pythagorean Triples -- Solution Without Loops
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
% input: x
% output: M
A = repmat([1:n], n, 1);
B = repmat([1:n]', 1, n);
C = sqrt(A.^2 + B.^2);
M = [A(:), B(:), C(:)];
lM = ( mod(M(:, 3), 1) ~= 0 );
M(lM, :) = [];
#+END_SRC

**** Birthday Problem
***** Question
In a group of $n$ randomly chosen people, what is the probability that everyone has a different birthday?
1. Find this probability by hand.
2. Let $n = 30$. Write a script that generates a group of $n$ people randomly and determines if there are any matches.
3. Modify the script above to run a number of simulations and numerically calculate the sought-after probability. Try $1000, 10000$, and $100000$ simulations. Compare the result with the analytical calculation done in 1.
**** Birthday Problem (Hints)
- For simplicity, ignore leap years.
- Create a random (column) vector whose elements represent birthdays of individuals (denoted by integers between 1 and 365).
- Line up the birthdays in order and take the difference of successive pairs. What does the resulting vector tell you?
- For 3, to run simulation multiple times, consider creating a random matrix whose rows represent birthdays of individuals and the columns correspond to different simulations.

** DONE Graphics
:PROPERTIES:
:EXPORT_FILE_NAME: 07-graphics
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Anonymous Functions
**** Anonymous Functions
Mathematical functions such as
\begin{align*}
  f_1(x) &= \cos x \sin\bigl( \cos (\tan x) \bigr), \\
  f_2(\th) &= {( \cos 3\th + 2 \cos 2\th)}^2, \\
  f_3(x,y) &= \frac{\sin(x+y)}{1+x^2+y^2},
\end{align*}
can be defined in MATLAB using /anonymous functions/:
#+BEGIN_CENTER
#+ATTR_LATEX: :options {0.7\linewidth}
#+BEGIN_minipage
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
f1 = @(x) cos(x).*sin(cos(tan(x)));
f2 = @(th) ( cos(3*th) + 2*cos(2*th) ).^2;
f3 = @(x, y) sin(x + y)./(1 + x.^2 + y.^2);
#+END_SRC
#+END_minipage
#+END_CENTER

**** Anonymous Functions -- Syntax
Take a closer look at one of them.
\vs

#+BEGIN_CENTER
#+ATTR_LATEX: :options {0.7\linewidth}
#+BEGIN_minipage
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
f1 = @(x) cos(x).*sin(cos(tan(x)));
#+END_SRC
#+END_minipage
#+END_CENTER

\vs
- @@latex: \blue{@@ =f1= @@latex: }@@: the function name or the /function handle/
- @@latex: \blue{@@ =@= @@latex: }@@: marks the beginning of an anonymous function
- @@latex: \blue{@@ =(x)= @@latex: }@@: denotes the function (input) argument
- @@latex: \blue{@@ =cos(x).*sin(cos(tan(x)))= @@latex: }@@: MATLAB expression defining $f_1(x)$

**** Anonymous Functions -- Notes                                            :BMCOL:noexport:
:PROPERTIES:
:BEAMER_env: note
:END:

- The function handles are a new type of variables. Type =class(f1)=.
- The function argument can be an array; use the elementwise operations.
- The function definition can even involve other MATLAB functions.

**** Examples

Expressions in function definitions can get very complicated. For example,

#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
h1 = @(x) [2*x, sin(x)];
h2 = @(x) [2*x, sin(x); 5*x, cos(x); 10*x, tan(x)];
r = @(a,b,m,n) a + (b-a)*rand(m,n);
#+END_SRC

**** Exercise: Different Ways of Defining a Function

The function
\[
f_4(\theta; c_1, c_2, k_1, k_2) = (c_1 \cos k_1 \theta + c_2 \cos k_2 \theta)^2
\]
can be defined in two different ways:

#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
f4s = @(th,c1,c2,k1,k2) c1*cos(k1*th) + c2*cos(k2*th)
f4v = @(th,c,k) c(1)*cos(k(1)*th) + c(2)*cos(k(2)*th)
#+END_SRC

***** Question
Use =f4s= and =f4v= to define yet another anonymous functions for
- $g(\theta) = 3\cos(2\theta) - 2\cos(3\theta)$
- $h(\theta) = 3\cos(\theta/7) + \cos(\theta)$

**** Exercise: Understanding Anonymous Functions
Type in the following statements in MATLAB:
#+BEGIN_CENTER
#+ATTR_LATEX: :options {0.7\linewidth}
#+BEGIN_minipage
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
f1 = @(x) cos(x).*sin(cos(tan(x)));
f2 = @(th) ( cos(3*th) + 2*cos(2*th) ).^2;
x1 = 5; y1 = f1(x1)
x2 = [5:-2:1]; y2 = f1(x2)
TH = diag(0:pi/2:2*pi); R = f2(TH)
#+END_SRC
#+END_minipage
#+END_CENTER

***** Question
1. What are the types of the input and output variables?
   #+LATEX: \begin{multicols}{3}
   - =x1= and =y1=
   - =x2= and =y2=
   - =TH= and =R=
   #+LATEX: \end{multicols}
2. Which of the three outputs will be affected if elementwise operations were not used in the definition of =f1= and =f2=?

*** 2-D Graphics
**** The =PLOT= Function
To draw a curve in MATLAB:

- Construct a pair of $n\text{-vectors}$ =x= and =y= corresponding to the set of data points $\{ (x_i, y_i) \,\mid\, i = 1, 2, \ldots, n\}$ which are to appear on the curve in that order.
- Then type @@latex:\blue{@@ =plot(x, y)= @@latex:}@@.

For example:
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
x = linspace(0, 2*pi, 101);
y = sin(x);
plot(x, y)                      % or simply plot(x, sin(x))
#+END_SRC
or
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
f = @(x) 1 + sin(2*x) + cos(4*x);      % anonymous function
x = linspace(0, 2*pi, 101);
plot(x, f(x))
#+END_SRC

**** Example: Wiggly Curve
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:
First, run the following script.
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth, numbers=left, numberstyle={\tiny \color{gray}}, numbersep=7pt
#+BEGIN_SRC matlab
f1 = @(x) cos(x).*sin(cos(tan(x)));
x = 2*pi*[0:.0001:1];   % or  x = linspace(0, 2*pi, 10001);
plot(x, f1(x))
shg
#+END_SRC

***** Play Around!
Observe what happens after applying the following modifications one by one.
@@latex: \bgroup\small@@
#+ATTR_BEAMER: :overlay <+->
 - Change line 3 into =plot(x, f1(x), 'r')=.
 - Change line 3 into =plot(x, f1(x), 'r--')=.
 - After line 3, add =axis equal, axis tight=.
 - Then add =text(4.6, -0.3, 'very wiggly')=.
 - Then add \\
   =xlabel('x axis'), ylabel('y axis'), title('A wiggly curve')=.
@@latex: \egroup@@

**** Note: Line Properties
:PROPERTIES:
:BEAMER_opt: shrink=10
:END:
- To specify line properties such as colors, markers, and styles:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
plot(x, y, '--')                   % dashed line
plot(x, y, 'g:')                   % dotted line in green
plot(x, y, '.', 'MarkerSize', 3)   % adjust marker size
plot(x, y, 'b-', 'LineWidth', 5)   % adjust line width
  #+END_SRC

\vs
***** colors                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.33
:END:
_Colors_
#+ATTR_LATEX: :align c|l
| =b= | blue    |
| =g= | green   |
| =r= | red     |
| =c= | cyan    |
| =m= | magenta |
| =y= | yellow  |
| =k= | black   |
| =w= | white   |

***** markers                                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.33
:END:
_Markers_
#+ATTR_LATEX: :align c|l
| =.= | point   |
| =o= | circle  |
| =x= | x-mark  |
| =+= | plus    |
| =*= | star    |
| =s= | square  |
| =d= | diamond |

***** styles                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.33
:END:
_Line Styles_
#+ATTR_LATEX: :align c|l
| =-=  | solid   |
| =:=  | dotted  |
| =-.= | dashdot |
| =--= | dashed  |

**** Note: Labels and Saving
- To label the axes and the entire plot, add the following after =plot= statement:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
xlabel('x axis')
ylabel('y axis')
title('my awesome graph')
  #+END_SRC

- Save figures using =print= function. Multiple formats are supported.
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
print -dpdf 'wiggly'
% or print('-dpdf', 'wiggly')                        [pdf]

print -djpeg 'wiggly'
% or print('-djpeg', 'wiggly')                      [jpeg]

print -deps 'wiggly'
% or print('-deps', 'wiggly')                        [eps]
  #+END_SRC

**** Note: Drawing Multiple Figures
- To plot multiple curves:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
plot(x1, y1, x2, y2, x3, y3, ...)
  #+END_SRC

- To create a legend, add
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
legend('first graph', 'second graph', 'third graph', ...)
  #+END_SRC

**** Note: Miscellaneous Commands
- =shg=: (show graph) to bring Figure Window to the front
- =figure=: to open a new blank figure window
- =clf=: (clear figure) to clear previously drawn figures
- =axis equal=: to put axes in equal scaling
- =axis tight=: to remove margins around graphs
- =axis image=: same as =axis equal= and =axis tight=
- =grid on=: to put light gray grid lines

**** Exercise
***** Question
Do the following:
- Define $f(x) = x^3 + x$ as an anonymous function.
- Find $f'$ and $f''$ and define them as anonymous functions.
- Plot all three functions in one figure in the interval $[-1, 1]$.
- Include labels and title in your plot.
- Add legend to the graph.
- Save the graph as a pdf file.

**** Multiple Figures -- Stacking
To draw multiple curves in one plot window as in Figure [[fig:stacking]]:

***** col1                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
- One liner:
  #+ATTR_LATEX: :options style=matlab, linewidth=0.9\linewidth
  #+BEGIN_SRC matlab
plot(x1, y1, x2, y2, x3, y3)
  #+END_SRC
- Or, add curves one at a time using =hold= command.
  #+ATTR_LATEX: :options style=matlab, linewidth=0.9\linewidth
  #+BEGIN_SRC matlab
plot(x1, y1)
hold on
plot(x2, y2)
plot(x3, y3)
  #+END_SRC
  - =hold on=: holds the current plot for further overlaying
  - =hold off=: turns the /hold/ off
***** col2                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+CAPTION: Multiple curves in one plot window
#+NAME: fig:stacking
#+ATTR_LATEX: :placement [t]
[[../img/stacking.eps]]

**** Multiple Figures -- Subplots
To plot multiple curves separately and arrange them as in Figure [[fig:subplots]]: \\

\hspace{0.02\linewidth}
#+ATTR_LATEX: :options [t]{0.38\linewidth}
#+BEGIN_minipage
#+ATTR_LATEX: :options style=matlab, linewidth=0.9\linewidth
#+BEGIN_SRC matlab
subplot(1,3,1)
plot(x1, y1)
subplot(1,3,2)
plot(x2, y2)
subplot(1,3,3)
plot(x3, y3)
#+END_SRC
#+END_minipage
\hfill
#+ATTR_LATEX: :options [t]{0.58\linewidth}
#+BEGIN_minipage
\vspace{0.3em}
=subplot(m,n,p)=:
- =m=, =n=: determine grid dimensions
- =p=: determines grid is to be used
#+END_minipage

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vfill
#+CAPTION: Multiple plots in $1 \times 3$ grids
#+NAME: fig:subplots
#+ATTR_LATEX: :width 0.9\linewidth :placement [ht!]
[[../img/subplots.eps]]

**** Exercise: Multiple Figures
***** Do It Yourself
Generate Figures [[fig:stacking]] and [[fig:subplots]].

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs

- Common: Generating sample points
  #+ATTR_LATEX: :options style=matlab, linewidth=0.94\linewidth
  #+BEGIN_SRC matlab
x = linspace(0, 1, 101);
y1 = x.^2; y2 = x.^4; y3 = x.^6;
  #+END_SRC

#+ATTR_LATEX: :options [t]{0.47\linewidth}
#+BEGIN_minipage
- Figure [[fig:stacking]]:
  #+ATTR_LATEX: :options style=matlab, linewidth=0.9\linewidth
  #+BEGIN_SRC matlab
hold off
plot(x, y1, '*')
hold on
plot(x, y2, 'g:o')
plot(x, y3, 'r-s')
  #+END_SRC
#+END_minipage
\hspace{0.03\linewidth}
#+ATTR_LATEX: :options [t]{0.47\linewidth}
#+BEGIN_minipage
- Figure [[fig:subplots]]:
  #+ATTR_LATEX: :options style=matlab, linewidth=0.9\linewidth
  #+BEGIN_SRC matlab
subplot(1, 3, 1)
plot(x, y1)
subplot(1, 3, 2)
plot(x, y2, 'g')
subplot(1, 3, 3)
plot(x, y3, 'r')
  #+END_SRC
#+END_minipage

***** COMMENT ...                                                          :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
*Note:* In case a figure window is already open, you might want to start each block of codes with either @@latex:\blue{@@ =figure(), close= @@latex:}@@, or @@latex:\blue{@@ =clf= @@latex:}@@.

**** The =POLARPLOT= Function
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:
#+LATEX: \begingroup\small
#+ATTR_LATEX: :options [t]{0.49\linewidth}
#+BEGIN_minipage
To draw the polar curve
\[
r = f(\th), \quad \th \in [a, b],
\]
- Grab $n$ sample points $\{ (\th_i, r_i) \,\mid \, r_i = f(\th_i), \; 1 \le i \le n \}$ on the curve and form vectors =th= and =r=.
- Then type @@latex:\blue{@@ =polarplot(th, r)= @@latex:}@@.
- For example, to plot
  \[
  r = f_2(\th) = {( \cos 3\th + 2 \cos 2\th)}^2,
  \]
  for $\th \in [0, 2\pi]$:
#+END_minipage
\hfill
#+ATTR_LATEX: :options [t]{0.49\linewidth}
#+BEGIN_minipage
\vspace{0pt}
#+ATTR_LATEX: :width 0.9\linewidth
[[../img/polarcurve.eps]]
#+END_minipage
#+LATEX: \endgroup

\vs

\hspace{2em}
#+ATTR_LATEX: :options {\linewidth}
#+BEGIN_minipage
#+ATTR_LATEX: :options style=matlab, linewidth=0.75\linewidth
#+BEGIN_SRC matlab
th = linspace(0, 2*pi, 361);
f2 = @(th) (cos(3*th) + 2*cos(2*th)).^2;
polarplot(th, f2(th));
#+END_SRC
#+END_minipage


**** Exercise: Drawing Polar Curves
***** Question
1. Draw the graph of two-petal leaf given by
   \[
   r = f(\th) = 1 + \sin(2\th)\,, \quad \th \in [0, 2\pi] \,.
   \]
2. Draw the graphs of
   \[
   r = f(\th - \pi/4) \,, \quad
   r = f(\th - \pi/2) \,, \quad
   r = f(\th - 3\pi/4)
   \]
   on the same plotting window.
3. Does your figure make sense?
*** 3-D Graphics
**** Curves and the =PLOT3= Function
Curves in $\RR^3$ are plotted in an analogous fashion.
***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
- Grab $n$ sample points $\{ (x_i, y_i, z_i) \,\mid\, i = 1, 2, \ldots, n\}$ on the curve and form vectors =x=, =y=, and =z=.
- Then type @@latex:\blue{@@ =plot3(x, y, z)= @@latex:}@@.
- For example, to plot the helix given by the parametrized equation
  \[
  \mathbf{r}(t) = \langle 10 \cos(t), 10 \sin(t), t \rangle,
  \]
  for $t \in [0, 10\pi]$:
  \vs

  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
t = linspace(0, 10*pi, 1000);
plot3(10*cos(t), 10*sin(t), t);
  #+END_SRC

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.39
:END:
\vspace{-\baselineskip}
#+ATTR_LATEX: :width \linewidth
[[../img/helix.eps]]

**** Exercise: Corkscrew
***** Question
Modify the code to generate a corkscrew by putting the helix outside of an upside down cone.
***** hint                                                                 :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
/Hint:/ Use $\mathbf{r}(t) = \langle t \cos(t), t \sin(t), t \rangle$.

**** Surfaces and the =SURF= Function
To plot the surface of $z = f(x,y)$ on $R = [a,b] \times [c,d]$:

***** Columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
- Collect samples points on the intervals $[a,b]$ and $[c,d]$ and form vectors =x= and =y=.
- Based on =x= and =y=, generate grid points $\{ (x_i, y_j) \,\mid\, i = 1, 2, \ldots, m, j= 1, 2, \ldots, n\}$ on the domain $R$ and separate coordinates into matrices =X= and =Y= using =meshgrid=.
- Type @@latex:\blue{@@ =surf(X, Y, f(X,Y))= @@latex:}@@

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
\vspace{-\baselineskip}
#+NAME: fig:saddle
#+CAPTION: Graph of $z = \frac{2}{9} (x^2 - y^2)$ on $[-3, 3] \times [-2, 2]$
#+ATTR_LATEX: :width \linewidth
[[../img/saddle.eps]]

**** Note: How =MESHGRID= Work
#+ATTR_LATEX: :options [t]{0.49\linewidth}
#+BEGIN_minipage
\vspace{-\baselineskip}
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth, basicstyle=\scriptsize\ttfamily
#+BEGIN_SRC matlab
>> x = [1 2 3 4]; y = [5 6 7];
>> [X, Y] = meshgrid(x,y)
   X =
        1     2     3     4
        1     2     3     4
        1     2     3     4
   Y =
        5     5     5     5
        6     6     6     6
        7     7     7     7
#+END_SRC
#+END_minipage
\hfill
#+ATTR_LATEX: :options [t]{0.4\linewidth}
#+BEGIN_minipage
\vspace{-\baselineskip}
#+ATTR_LATEX: :width 0.7\linewidth :placement [t]
[[../img/matlab_meshgrid2.pdf]]
#+END_minipage


\vfill
#+ATTR_LATEX: :width 0.8\linewidth
[[../img/matlab_meshgrid.pdf]]

**** Example: Saddle
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:
***** Question
Plot the saddle parametrized by
\[
\frac{z}{c} = \frac{x^2}{a^2} - \frac{y^2}{b^2}
\]
for your choice of $a, b$, and $c$.

***** soln                                                                 :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
x = linspace(-3, 3, 13);
y = linspace(-2, 2, 9);
[X, Y] = meshgrid(x, y);
a = 1.5; b = 1.5; c = .5;
g2 = @(x,y) c*( x.^2 /a^2 - y.^2 /b^2);
surf(X, Y, g2(X,Y))
axis equal, box on
#+END_SRC

@@latex: {\footnotesize @@
Figure [[fig:saddle]] was generated using this code.
@@latex: } @@

**** Example: Oblate Spheroid

#+LATEX: \begin{minipage}[t]{0.49\linewidth}
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth, basicstyle=\footnotesize\ttfamily
#+BEGIN_SRC matlab
a = 1; b = 1.35; c = 1;
nr_th = 41; nr_ph = 31;
x = @(th, ph) a*cos(th).*sin(ph);
y = @(th, ph) b*sin(th).*sin(ph);
z = @(th, ph) c*cos(ph);
th = linspace(0, 2*pi, nr_th);
ph = linspace(0, pi, nr_ph);
[T, P] = meshgrid(th, ph);
surf(x(T,P), y(T,P), z(T,P))
colormap(winter)
axis equal, axis off, box off
#+END_SRC
\vs

@@latex: {\footnotesize @@ The code is originally from *LM*; some parameters and the color specs were modified. @@latex: }@@
#+LATEX: \end{minipage}
\hfill
#+LATEX: \begin{minipage}[t]{0.49\linewidth}
#+CAPTION: Oblate spheroid.
#+ATTR_LATEX: :width 0.95\linewidth :placement [t]
[[../img/oblate_spheroid.eps]]
#+LATEX: \end{minipage}
** DONE Function M-File
:PROPERTIES:
:EXPORT_FILE_NAME: 08-function-m-file
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** TODO Pop Quiz
**** How many operations are incorrect?
**** What does the following script do?
**** What is the last line of the output?
**** Drawing the unit circle
*** Function M-File
**** Basics
- A /function/ is a piece of code which
  - performs a specific task;
  - has specific input and output arguments;
  - is encapsulated to be independent of the rest of the program.
- In MATLAB, functions are defined in .m files just as scripts.
- The name of the file and that of the function must coincide.

**** Why Functions?
***** Two Important Principles
- A piece of code should only occur once in a program. If it tries to appear more times, put it into its own function.
- Each function should do precisely *one* task well. If it has to carry out a number of tasks, separate them into their own functions.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
*Benefits of writing functions:*
- elevated reasoning by hiding details
- facilitates top-down design
- ease of software management

**** Writing a Function
A function m-file must be written in a specific manner.
- When there is no input/output argument
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
function myfun()
    ....
end    % <-- optional
  #+END_SRC

- Where there are multiple input and output arguments
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
function [out1, out2, out3] = myfun(in1, in2, in3, in4)
    ....
end    % <-- optional
  #+END_SRC

**** Calling a Function
- If the function m-file =myfun.m= is saved in your current working directory[fn::The =path= function gives more flexibility in this regard.], you can use it as you would use any other built-in functions:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
% when no input/output argument is required
myfun
  #+END_SRC
  or
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
% multiple inputs/outputs
[out1, out2, out3] = myfun(in1, in2, in3, in4)
  #+END_SRC

- When not all output arguments are needed:
  #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
  #+BEGIN_SRC matlab
out1 = myfun(in1, in2, in3, in4)         % only 1st output
[~, ~, out3] = myfun(in1, in2, in3, in4) % only 3rd output
  #+END_SRC
  Note that tilde ( =~= ) is used as a placeholder.

**** Practical Matters: Specification
 - The comments written below =function= header statement
   #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
   #+BEGIN_SRC matlab
 function ... = myfun( ... )
 % MYFUN: this awesome function calculates ...
   ....
 end
   #+END_SRC
    can be easily accessed from the command line using =help myfun=.
 - Use this feature to write the function specification such as its purpose, usage, and syntax.
 - Write a clear, complete, yet concise specification.

**** Properties of Function M-Files
 - The variable names in a function are completely isolated from the calling code. Variables, other than outputs, used inside a function are unknown outside of the function; they are /local variables/.
 - The input arguments can be modified within the body of the function; no change is made on any variables in the calling code. (`` /pass by value/ '' as opposed to `` /pass by reference/ '')
# - In case the input arguments need to be modified, it can be done indirectly with the help of the calling statement.
 - Unlike script m-files, you *CANNOT* execute a function which has input arguments using the @@latex:{\sf@@ RUN @@latex:}@@ icon.

**** Example: Understanding Local Variables
#+ATTR_LATEX: :options [t]{0.45\textwidth}
#+BEGIN_minipage
 - The function on the right finds the maximum value of a vector and an index of the maximal element.
 - Run the following on the Command Window. Pay attention to =m=.
   \vs
   #+BEGIN_EXAMPLE
>> m = 33;
>> x = [1 9 2 8 3 7];
>> [M, iM] = mymax(x)
>> disp(m)
   #+END_EXAMPLE
#+END_minipage
\hfill
#+ATTR_LATEX: :options [t]{0.45\textwidth}
#+BEGIN_minipage
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
function [m, el] = mymax(x)
    if isempty(x)
        el = [];
        m = [];
        return
    end
    el = 1;
    m = x(el);
    for i = 2:length(x)
        if m < x(i)
            el = i;
            m = x(el);
        end
    end
end
#+END_SRC
#+END_minipage

**** Example: Understanding Pass-By-Value

Consider the function
\begin{equation*}
f(x) = \sin \abs{x} \frac{e^{-\abs{x}}}{1+\abs{x}^2}
+ \frac{\ln (1+\abs{x})}{1+2\abs{x}}
\end{equation*}
written as a MATLAB function:

#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
 function y = funky(x)
     x = abs(x);  % x is redefined to be abs(x)
     y = sin(x).*exp(-x)./(1 + x.^2)...
         + log(1 + x)./(1 + 2*x);
 end
#+END_SRC

Confirm that the function does not affect =x= in the calling routine:
#+BEGIN_EXAMPLE
 >> x = [-3:3]';
 >> y = funky(x);
 >> disp([x, y])
#+END_EXAMPLE

**** Quiz
***** Question
\hspace{0.03\linewidth}
#+ATTR_LATEX: :options [t]{0.45\linewidth}
#+BEGIN_minipage
*Script.*
#+ATTR_LATEX: :options style=matlab, linewidth=0.9\linewidth
#+BEGIN_SRC matlab
 x = 2;
 x = myfun(x);
 y = 2*x
#+END_SRC
#+END_minipage
\hfill
#+ATTR_LATEX: :options [t]{0.45\linewidth}
#+BEGIN_minipage
*Function.*
#+ATTR_LATEX: :options style=matlab, linewidth=0.9\linewidth
#+BEGIN_SRC matlab
 function y = myfun(x)
     x = 2*x;
     y = 2*x;
 end
#+END_SRC
#+END_minipage
\hspace{0.03\linewidth}

What is the output when the script on the left is run?
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+LATEX: \begin{multicols}{4}
 A. =y = 2=
 B. =y = 4=
 C. =y = 8=
 D. =y = 16=
#+LATEX: \end{multicols}

*** Example: Spiral Triangle
**** Description of Problem
***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** Problem                                                                      :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.55
:END:
******* Problem
Given $m$ and $\theta$ (in degrees), draw $m$ equilateral triangles so that for any two successive triangles,
 - the vertices of the smaller triangle are lying on the sides of the larger;
 - the angle between the two triangles is $\theta$ (degrees)

****** fig                                                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+CAPTION: A spiral triangle with $m=21$ and $\theta = 4.5^{\circ}$.
#+ATTR_LATEX: :width 0.8\linewidth
[[../img/spiral_triangle.eps]]

**** Generation and Transformation of Polygons
:PROPERTIES:
:BEAMER_opt: shrink=5
:END:
Let $(x_j, y_j)$, $j= 1, 2, \ldots, n$ be the coordinates of vertices of an $n\text{-gon}$.

\begingroup\small
\vs
 - To plot the polygon:
   #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
   #+BEGIN_SRC matlab
 x = [x1 x2 ... xn x1]; % note: x1 and y1 are repeated at
 y = [y1 y2 ... yn y1]; % end to enclose the polygon.
 plot(x, y)
   #+END_SRC

 - To plot the polygon obtained by scaling the original by a factor of =s=:
   #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
   #+BEGIN_SRC matlab
 plot(s*x, s*y)
   #+END_SRC

 - To plot the polygon obtained by rotating the original by an angle =theta= (in degrees) about the origin:
   #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
   #+BEGIN_SRC matlab
 V = [x; y];                       % all vertices
 R = [cosd(theta) -sind(theta);    % 2-D rotation matrix
      sind(theta)  cosd(theta)];
 Vnew = R*V;                       % rotated vertices
 plot(Vnew(1,:), Vnew(2,:))
   #+END_SRC
\endgroup

**** Special Case: Inscribed Regular Polygons
 - The vertices of the /regular/ $n\text{-gon}$ inscribed in the unit circle can be found easily using trigonometry, /e.g./,
   \[
   (x_j, y_j) = \left( \cos \frac{2\pi(j-1)}{n},\, \sin \frac{2\pi(j-1)}{n} \right), \quad j = 1, \ldots, n.
   \]
 - Thus we can plot it by
   #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
   #+BEGIN_SRC matlab
 theta = linspace(0, 360, n+1);
 V = [cosd(theta);     % 2-by-(n+1) matrix whose cols are
      sind(theta)];    % coordinates of the vertices
 plot(V(1,:), V(2,:)), axis equal
   #+END_SRC
 - To stretch and rotate the polygon:
   #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
   #+BEGIN_SRC matlab
 Vnew = s*R*V;
 plot(Vnew(1,:), Vnew(2,:)), axis equal
   #+END_SRC

**** Scale to Spiral
To create the desired spiraling effect, the scaling factor must be calculated carefully.
***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
 - Useful:
   #+ATTR_LATEX: :options [width=0.7\linewidth,arc=0mm,boxrule=0.7pt,colback=white]
   #+BEGIN_tcolorbox
   \vspace{-1em}
   \[
   \frac{\sin \alpha}{a} = \frac{\sin \beta}{b} = \frac{\sin \gamma}{c}
   \]
   #+END_tcolorbox
 - Compute the scaling factor $s$:

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+LATEX: \begin{figure}[ht] \centering
\incfig{../img/spiral_triangle_explained}{0.9\linewidth}
#+LATEX: \end{figure}

**** Put All Together
\begingroup\small
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
m = 21; d_angle = 4.5;
th = linspace(0, 360, 4) + 90;
V = [cosd(th);
     sind(th)];
C = colormap(hsv(m));
s = sind(150 - abs(d_angle))/sind(30);
R = [cosd(d_angle) -sind(d_angle);
     sind(d_angle) cosd(d_angle)];
hold off
for i = 1:m
    if i > 1
        V = s*R*V;
    end
    plot(V(1,:), V(2,:), 'Color', C(i,:))
    hold on
end
set(gcf, 'Color', 'w')
axis equal, axis off
#+END_SRC
\endgroup

**** Exercise: Script to Function
***** Question
Modify the script so that it can create a spiral $n\text{-gon}$ consisting of $m$ regular $n\text{-gons}$ successively rotated by $\theta$ degrees. Then turn the script into a function m-file =spiralgon.m=.
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
function V = spiralgon(n, m, d_angle)
% SPIRALGON plots spiraling regular n-gons
% input:   n = the number of vertices
%          m = the number of regular n-gons
%          d_angle = the degree angle between successive n-gons
%          (can be positive or negative)
% output:  V = the vertices of the outermost n-gon

% Fill in the rest.
....
#+END_SRC

*** Appendix: Supplementary Notes
**** Local Functions
:PROPERTIES:
:BEAMER_opt: shrink=5
:END:
@@latex:\begingroup\small@@
There are ``general purpose'' functions which may be used by a number of other functions. In this case, we put it into a separate file.  However, many functions are quite specific and will only be used in one program. In such a case, we keep them in whatever file calls them.
@@latex:\endgroup@@

 - The first function is called the /primary/ function and any function which follows the primary function is called a /local function/ or a /subfunction/.
   #+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
   #+BEGIN_SRC matlab
 function `<primary function>`
   ....
 end
 function `<local function \#1>`
   ....
 end
 `<any number of following local functions>`
   #+END_SRC
 - The primary function interact with local functions through the input and output arguments.
 - Only the primary function is accessible from outside the file.

**** Defining/Evaluating Mathematical Functions in MATLAB

 - using an anonymous function
 - using a local function
 - using a primary funciton
 - using a nested function
 - putting it /in situ/, i.e., write it out completely in place -- not recommended
   \vs

See =time_anon.m=.

**** Passing Function to Another Function
 - A function can be used as an input argument to another function.
 - The function must be stored as a function-handle variable in order to be passed as an argument.
 - This is done by prepending `` =@= '' to the function names. For instance, try
   #+BEGIN_EXAMPLE
 >> class(mymax)   % WRONG
 >> class(@mymax)  % CORRECT
   #+END_EXAMPLE

**** When a Function Is Called
Below is what happens internally when a function is called:

 - MATLAB creates a new, local workspace for the function.
 - MATLAB evaluates each of the input arguments in the calling statements (if there are any). These values are then assigned to the input arguments of the function. These are now local variables in the new workspace.
 - MATLAB executes the body of the function. If there are output arguments, they are assigned values before the execution is complete.
 - MATLAB deletes the local workspace, saving only the values in the output arguments. These values are then assigned to the output arguments in the calling statement.
 - The code continues following the calling statement.

* DONE Chapter 1: Preliminaries to Numerical Analysis
** Notes                                                                             :noexport:
*** Notes
**** References
 - FNC 1.1
 - LM 9.3
 - NCM 1.7 (floating-point arithmetic)

**** Objectives
 - Learn how numbers are represented in the computer
 - Examine consequences of floating-point arithmetic
 - Begin to study numerical algorithms
 - Learn to identify when problems can cause numerical problems:
   - from subtraction of nearby numbers
   - from the problem itself (conditioning)
   - from the numerical algorithm (stability)

**** Misc
 - <2021-02-09 Tue> It took two lectures to cover this topic.
 - [ ] figures to aid understanding floating-point numbers

*** Codes
#+BEGIN_SRC matlab :results silent
addpath('~/Documents/MATLAB/NCM')
clear
floatgui
rmpath('~/Documents/MATLAB/NCM')
#+END_SRC

#+RESULTS:
#+begin_example
addpath('~/Documents/MATLAB/NCM')
type floatgui.m

function floatgui(callbackarg)
%FLOATGUI  Show structure of floating point numbers.
%  The set of positive model floating point numbers is determined
%  by three parameters: t, emin, and emax.  It is the set of rational
%  numbers of the form x = (1+f)*2^e where f = (integer)/2^t,
%  0 <= f < 1, e = integer, and emin <= e <= emax.
%
%  IEEE 754 double precision has t = 52, emin = -1022, emax = 1023.

%   Copyright 2014 Cleve Moler
%   Copyright 2014 The MathWorks, Inc.

% Initialize parameters

if nargin == 0
   t = 3;
   emin = -4;
   emax = 2;
   logscale = 0;
   Fpos=[50 300 900 250];
else
   t = round(get(findobj('tag','t'),'value'));
   emin = round(get(findobj('tag','emin'),'value'));
   emax = round(get(findobj('tag','emax'),'value'));
   logscale = get(findobj('style','check'),'value');
   Fpos=get(gcf,'pos');
end

% Position figure window

shg
clf reset
set(gcf,'pos',Fpos,'name','floatgui', ...
   'resize','on','defaultuicontrolunits','normalized',...
   'numbertitle','off','menubar','none')

% Generate and plot floating point numbers

f = (0:2^t-1)/2^t;
F = [];
for e = emin:emax
   F = [F (1+f)*2^e];
end
for x = F
   text(x,0,'|','fontunits','normalized','fontsize',0.3)
end

% Set axes

set(gca,'pos',[.05 .6 .9 .2],'fontunits','normalized','fontsize',0.22)
if logscale
   set(gca,'xscale','log')
   xmin = 1/2^(-emin+.5);
   xmax = 2^(emax+1.5);
else
   set(gca,'xscale','linear')
   xmin = 0;
   xmax = 2^(emax+1);
end
axis([xmin xmax -1 1])

% Set tick marks

fmin = min(F);
fmax = max(F);
xtick = 1;
xticklab = {'1'};
if fmin < 1
   xtick = [1/2 xtick];
   xticklab = ['1/2' xticklab];
end
if logscale & (fmin < 1/4)
   xtick = [1/4 xtick];
   xticklab = ['1/4' xticklab];
end
if fmin < 1/2
   xtick = [fmin xtick];
   xticklab = [['1/' int2str(1/fmin)] xticklab];
end
if 2 < fmax
   xtick = [xtick 2];
   xticklab = [xticklab '2'];
end
if 4 < fmax
   xtick = [xtick 4];
   xticklab = [xticklab '4'];
end
if max(xtick) < fmax
   xtick = [xtick fmax];
   if fmax == round(fmax)
      fmaxlab = int2str(fmax);
   else
      over = 2^(emax+1);
      fmaxlab = [int2str(over) '-1/' int2str(1/(over-fmax))];
   end
   xticklab = [xticklab fmaxlab];
end
set(gca,'xtick',xtick,'xticklabel',xticklab,'xminortick','off','ytick',[])

% Create uicontrols

uicontrol('style','slider','tag','emin','value',emin, ...
   'min',-8,'max',0,...
   'pos',[0.15 0.26 0.13 0.07],'sliderstep',[1/8 1/8], ...
   'callback','floatgui(1)');
uicontrol('style','slider','tag','t','value',t, ...
   'min',0,'max',8,...
   'pos',[0.435 0.26 0.13 0.07],'sliderstep',[1/8 1/8], ...
   'callback','floatgui(1)');
uicontrol('style','slider','tag','emax','value',emax, ...
   'min',0,'max',8,...
   'pos',[0.72 0.26 0.13 0.07],'sliderstep',[1/8 1/8], ...
   'callback','floatgui(1)');
uicontrol('style','text','string',['emin = ' int2str(emin)], ...
   'pos',[0.15 0.35 0.13 0.07],'fontunits','normalized','fontsize',0.7)
uicontrol('style','text','string',['t = ' int2str(t)], ...
   'pos',[0.435 0.35 0.13 0.07],'fontunits','normalized','fontsize',0.7)
uicontrol('style','text','string',['emax = ' int2str(emax)], ...
   'pos',[0.72 0.35 0.13 0.07],'fontunits','normalized','fontsize',0.7)
uicontrol('style','check','string','log scale','value',logscale, ...
   'pos',[0.435 0.15 0.13 0.07],'fontunits','normalized','fontsize',0.7, ...
   'callback','floatgui(1)');
uicontrol('style','push','pos',[0.88 0.1 0.07 0.07], ...
   'fontunits','normalized','fontsize',0.7, ...
   'string','close','callback','close(gcf)')

% eps

if fmax > 1
   eps = 2^(-t);
   text(1,0,'|','color','r','fontunits','normalized','fontsize',0.3)
   text(1+eps,0,'|','color','r','fontunits','normalized','fontsize',0.3)
   if eps < 1
      text(1.0,1.5,['eps = 1/' int2str(1/eps)], ...
      'fontunits','normalized','fontsize',0.3,'fontweight','bold')
   else
      text(1.0,1.5,'eps = 1', 'fontunits','normalized','fontsize',0.3,'fontweight','bold')
   end
end

% Number of numbers

% Exercise:
% How many "floating point" numbers are in the set?
% Complete this statement.
% text(.9*xmax,2,num2str(???)
rmpath('~/Documents/MATLAB/NCM')
'org_babel_eoe'
ans =
    'org_babel_eoe'
#+end_example

** DONE Floating Point Numbers
:PROPERTIES:
:EXPORT_FILE_NAME: 09-fp-numbers
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

% \begin{frame}{Contents}
%   \tableofcontents
% \end{frame}
#+END_EXPORT

*** Errors
**** Absolute and Relative Errors
Let $x$ be some number of interest and let $\tilde{x}$ be an approximation to it obtained by an *algorithm*.
\vs

# #+LATEX: \only<handout>{
 - We estimate of the accuracy of the computed value via
  \begin{align*}
    &\text{(absolute error)}
      = \abs{\tilde{x}-x}, \qquad
    \text{(relative error)}
      = \abs{\frac{\tilde{x}-x}{x}}
      = \abs{\frac{\tilde{x}}{x} - 1}, \\
    &\text{(\# of accurate digits)}
      = -\log_{10} \abs{\frac{\tilde{x}-x}{x}}.
  \end{align*}
 - Absolute error has the same unit as $x$, while relative error is dimensionless.
 - If absolute error or relative error is small, we say that $\tilde{x}$ is *accurate*.
# #+LATEX: }

**** Example
***** Stirling's Formula                                                    :B_exampleblock:
:PROPERTIES:
:BEAMER_env: exampleblock
:END:
Stirling's formula provides a ``good'' approximation to $n!$ for large $n$:
\begin{equation}
  \label{eq:stirling}
  n! \approx \sqrt{2 \pi n} {\left( \frac{n}{e} \right)}^n \,.
  \tag{$\star$}
\end{equation}
Assuming that the exact value of $n!$ is found by =factorial=, estimate $n!$ using Stirling's formula. Show the accuracy of this approximation for various values of $n$.

**** Example {{{cont}}}
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
 n = ....;
 x = factorial(n)
 x0 = sqrt(2*pi*n)*(n/exp(1))^n
 abs_err = abs(x0-x)
 rel_err = abs((x0-x)/x)
 acc_digits = -log10(rel_err)
#+END_SRC
\vs

#+LATEX: \begingroup\scriptsize
#+ATTR_LATEX: :options {0.31\linewidth}
#+BEGIN_minipage
*Results with =n=20=*:
#+BEGIN_EXAMPLE
x  = 2.4329e+18
x0 = 2.4228e+18
abs_err = 3.0104e+04
rel_err = 0.0083
acc_digits = 2.0811
#+END_EXAMPLE
#+END_minipage
\hfill
#+ATTR_LATEX: :options {0.31\linewidth}
#+BEGIN_minipage
*Results with =n=100=*:
#+BEGIN_EXAMPLE
x  = 9.3326e+157
x0 = 9.3248e+157
abs_err = 7.7739e+154
rel_err = 8.3298e-04
acc_digits = 3.0794
#+END_EXAMPLE
#+END_minipage
\hfill
#+ATTR_LATEX: :options {0.31\linewidth}
#+BEGIN_minipage
*Results with =n=200=*:
#+BEGIN_EXAMPLE
x  = Inf
x0 = Inf
abs_err = NaN
rel_err = NaN
acc_digits = NaN
#+END_EXAMPLE
#+END_minipage
#+LATEX: \endgroup



***** code                                                                        :noexport:
#+BEGIN_SRC matlab
 n = 200;
 x = factorial(n)
 x0 = sqrt(2*pi*n)*(n/exp(1))^n
 abs_err = abs(x0-x)
 rel_err = abs((x0-x)/x)
 acc_digits = -log10(rel_err)
#+END_SRC

#+RESULTS:
#+begin_example
n = 200;
x = factorial(n)
x =
   Inf
x0 = sqrt(2*pi*n)*(n/exp(1))^n
x0 =
   Inf
abs_err = abs(x0-x)
abs_err =
   NaN
rel_err = abs((x0-x)/x)
rel_err =
   NaN
acc_digits = -log10(rel_err)
acc_digits =
   NaN
'org_babel_eoe'
ans =
    'org_babel_eoe'
#+end_example

**** Sources or Errors
\vs
#+LATEX: \begingroup\Large
*Big question*: Where do errors come from?
#+LATEX: \endgroup
\vspace{2em}

 - From how numbers are handled in the computer \hfill @@latex: {\color{gray}{(floting point numbers)}}@@
 - From the problem itself \hfill @@latex: {\color{gray}{(conditioning)}}@@
 - From the numerical algorithm \hfill @@latex: {\color{gray}{(stability)}}@@

*** Floating Point Numbers
**** Limitations of Digital Representations
A digital computer uses a finite number of bits to represent a real number and so it cannot represent all real numbers.
\vs
 - The represented numbers cannot be arbitrarily large or small;
 - There must be gaps between them.
\vs
So for all operations involving real numbers, it uses a subset of $\RR$ called the *floating point numbers*, $\mathbb{F}$.

**** Toy Model: Base-10
Consider a base-10 calculator which represents a nonzero number $x$ in the following format:
\begin{equation*}
x = (-1)^s \times d_0 . d_1 d_2 \times 10^e
\quad\text{where }
\left\{ \;
\begin{array}{rcl}
 0 \le & s   & \le 1 \\
 1 \le & d_0 & \le 9 \\
 0 \le & d_1 & \le 9 \\
 0 \le & d_2 & \le 9 \\
-9 \le & e   & \le 9
\end{array}
\right.
\end{equation*}
Such numbers are called the /floating point numbers/.
\vs

 - The precision of the calculator is determined by the length of $d_0 . d_1 d_2$.
   \[
   \pi \longrightarrow (3.14 \times 10^0).
   \]
 - Results of most arithmetic operations must be rounded.
   \[
   \underbrace{(1.23 \times 10^5) + (4.56 \times 10^2)}_{=123456} \longrightarrow (1.23 \times 10^5).
   \]

**** Toy Model: Base-10 {{{cont}}}

 - Zero must be represented with a special convention.
   \[
   0 \longrightarrow (0.00 \times 10^0).
   \]
 - There is a smallest positive floating point number $N_{\text{min}}$ and there is a largest positive floating point number $N_{\text{max}}$.
   \[
   N_{\text{min}} = (1.00 \times 10^{-9})
   \quad\text{and}\quad
   N_{\text{max}} = (9.99 \times 10^{9}).
   \]
 - The set of floating point numbers is finite. For this toy model, there are
   \[
   2 \times 9 \times 10 \times 10 \times 19 + 1 = 34201 \text{ floating point numbers.}
   \]
 - The spacing of floating numbers varies. Between $1.00 \times 10^e$ and $1.00 \times 10^{e+1}$, the spacing is $10^{e-2}$.

**** Floating Point Numbers: Theoretical Description
In the computer, numbers are stored using binary bits (0 or 1). So a floating point number must be expressed in base 2:
\[
x = (-1)^s \times 1.b_1 b_2 \ldots {b_d}_{(2)} \times 2^{a_1 a_2 \ldots {a_n}_{(2)} - \beta}
\]
where $s, a_1, \ldots, a_n, b_1, \ldots, b_d$ are either 0 or 1 and $\beta$ is some integer which introduces an offset in the exponent. If we let
\[
E = a_1 a_2 \ldots {a_n}_{(2)} - \beta = \sum_{i=1}^n a_i 2^{n-i} - \beta, \tag{exponent}
\]
and
\[
F = 0.b_1 b_2 \ldots {b_d}_{(2)} = \sum_{i=1}^{d} b_i 2^{-i}, \tag{mantissa}
\]
then a floating point number can be succinctly written as
\[
x = \pm (1 + F) \times 2^E.
\]

**** Exponent and the Extent of $\FF$
Since $E = a_1 a_2 \ldots {a_n}_{(2)} - \beta = \sum_{i=1}^n a_i 2^{n-i} - \beta$,
\[
E_{\text{min}} := -\beta \le E \le 2^n - 1 - \beta =: E_{\text{max}}.
\]
Thus if $x \in \FF$ is positive, then
\[
2^{E_\text{min}} \le x \le (2-2^{-d})2^{E_\text{max}}
\]

**** Mantissa and the Precision of $\FF$
Note that
\[
F = \sum_{i=1}^{d} b_i 2^{-i} = 2^{-d} \underbrace{\sum_{k=0}^{d-1} b_{d-k} 2^k}_{=:M},
\quad \text{where } M \in \NN[0, 2^d - 1].
\]
Consequently:

 - The smallest element of $\FF$ that is greater than $1$ is $1 + 2^{-d}$. We define *machine epsilon* $\meps$ as the gap between these two:
   \[
   \meps = 2^{-d}.
   \]
 - There are $2^d$ evenly-spaced numbers on $[2^E, 2^{E+1})$ in $\FF$. In other words:
   \vs
   #+BEGIN_CENTER
   #+ATTR_LATEX: :options [width=0.8\linewidth,arc=0mm,boxrule=0.7pt,colback=white,nobeforeafter]
   #+BEGIN_tcolorbox
   The spacing in $\FF$ between $2^E$ and $2^{E+1}$ is $2^{E-d}$.
   #+END_tcolorbox
   #+END_CENTER

**** Rounding Function and Relative Error
Let $\text{fl}:\RR \to \FF$ be a rounding function which maps every real number $x$ to the nearest element of $\FF$. Consider a positive real number $x \in [2^E, 2^{E+1})$ for some allowed exponent $E$.
 - Since the spacing of $\FF$ in the interval is $2^{E-d}$,
   \[
   \abs{ \text{fl}(x) - x } \le \frac{1}{2} 2^{E - d}.
   \]
 - It follows that
   \[
   \frac{ \abs{\text{fl}(x) - x} }{ \abs{x} } \le \frac{2^{E-d-1}}{2^E} = \frac{1}{2} \meps.
   \]
 - The previous equation is equivalent to
   \[
   \text{fl}(x) = x(1+\epsilon) \quad\text{for some } \abs{\epsilon} \le \frac{1}{2} \meps.
   \]

#+BEGIN_CENTER
#+ATTR_LATEX: :options [width=0.8\linewidth,arc=0mm,boxrule=0.7pt,colback=white,nobeforeafter]
#+BEGIN_tcolorbox
Every real number is represented as a floating point number with a uniformly bounded relative error.
#+END_tcolorbox
#+END_CENTER
**** IEEE 754 Standard: Double Precision Floating Point Numbers
MATLAB, by default, uses IEEE /double precision/ floating point numbers, stored in memory in 64 bits (or 8 bytes).
\begin{equation*}
  x = (-1)^s \times 1.b_1 b_2 \ldots {b_{52}}_{(2)} \times 2^{a_1 a_2 \ldots {a_{11}}_{(2)} - 1023}
\end{equation*}

 - one bit for the sign ($s$)
 - 11 bits for the exponent ($a_1, a_2, \ldots, a_{11}$, that is, $n = 11$)
 - 52 bits for the mantissa ($b_1, b_2, \ldots, b_{52}$, that is, $d = 52$)
 - the bias in the exponent is $\beta = 1023$.

**** IEEE 754 Standard: Double Precision Floating Point Numbers {{{cont}}}

*Note*. Since $n=11$ and $\beta = 1023$,
\[
-1023 \le E \le 1024.
\]

 - However, the two extreme values $E = -1023$ and $E = 1024$ are used for encoding special quantities.
 - The integers in $-1022 \le E \le 1023$ are used for normal representation of floating point numbers. Thus,
   \[
   E_{\text{min}} = -1022
   \quad\text{and}\quad
   E_{\text{max}} = 1023.
   \]
**** IEEE 754 Standard: Double Precision Floating Point Numbers {{{cont}}}
The following quantities capture important aspects of double precision numbers, so they are predefined in MATLAB:

 - *=eps=* = the *machine epsilon*
   \[
     \mathtt{eps} = 2^{-d}  = 2^{-52} \approx 2.2204 \times 10^{-16}.
   \]
 - *=realmin=* = the smallest positive floating point number[fn::The actual smallest positive number in MATLAB is =realmin/2^52= $\approx 4.9407 \times 10^{-324}$.]
   \[
     \mathtt{realmin} = 2^{E_\text{min}} = 2^{-1022} \approx 2.2251 \times 10^{-308}.
   \]
 - *=realmax=* = the largest positive floating point number
   \[
     \mathtt{realmax} = (2-2^{-d})2^{E_\text{max}} = (2-2^{-52}) 2^{1023} \approx 1.7977 \times 10^{308}.
   \]
 - Numerical results that should be larger than =realmax= /overflows/ to *=Inf=*.
 - The result of an undefined arithmetic operation yields *=NaN=*.

**** Machine Epsilon and Relative Errors
The IEEE standard guarantees that the /relative representation error/ and the /relative computational error/ have sizes smaller than $\meps$, the /machine epsilon/:
\vs

 - *Representation*: The floating point representation, $\hat{x} \in \FF$, of $x \in \RR$ satisfies
   \[
     \hat{x} = x(1 + \epsilon_1), \qquad \text{for some $\abs{\epsilon_1} \le \frac{1}{2}\,\meps$.}
   \]

 - *Arithmetic*: The floating point representation, $\hat{x} \oplus \hat{y}$, of the result of $\hat{x} + \hat{y}$ with $\hat{x}, \hat{y} \in \FF$ satisfies
  \[
    \hat{x} \oplus \hat{y} = (\hat{x} + \hat{y})(1 + \epsilon_2), \qquad \text{for some $\abs{\epsilon_2} \le \frac{1}{2}\, \meps$.}
  \]
  Similarly with $\ominus, \otimes, \odiv$ corresponding to $-, \times, \div$, respectively.

**** Round-Off Errors
*Computers CANNOT usually*
 - @@latex: {\color{black} @@ represent a number correctly; @@latex: } @@
 - @@latex: {\color{black} @@ add, subtract, multiply, or divide correctly!! @@latex: } @@

\vs
Run the following and examine the answers:
#+BEGIN_CENTER
#+ATTR_LATEX: :options [t]{0.8\linewidth}
#+BEGIN_minipage
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
 format long
 1.2345678901234567890
 12345678901234567890
 (1 + eps) - 1
 (1 + .5*eps) - 1
 (1 + .51*eps) - 1
 n = input(' n = '); ( n^(1/3) )^3 - n
#+END_SRC
#+END_minipage
#+END_CENTER

***** ...                                                                         :noexport:
 - change last line to a script with multiple =n= values. Print out results using =fprintf=.
 - put code on the left, result on the right

** DONE Conditioning and Stability
:PROPERTIES:
:EXPORT_FILE_NAME: 10-conditioning-and-stability
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

% \begin{frame}{Contents}
%   \tableofcontents
% \end{frame}
#+END_EXPORT

*** From old ones                                                                   :noexport:
**** Catastrophic Cancellation

In finite precision storage, two numbers that are close to each other are indistinguishable.  So subtraction of two nearly equal numbers on a computer can result in loss of many significant digits.

#+LATEX: \bgroup\small
***** Catastrophic Cancellation
Consider two real numbers stored with 10 digits of precision:
\begin{align*}
  e & = 2.7182818284, \\
  b & = 2.7182818272.
\end{align*}

 - Suppose the actual numbers $e$ and $b$ have additional digits that are not stored.
 - The stored numbers are good approximations of the true values.
 - However, if we compute $e - b$ based on the stored numbers, we obtain $0.0000000012 = 1.2\times 10^{-9}$, a number with only two significant digits.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+LATEX: \egroup

**** Example 1: Cancellation for Large Values of $x$
***** Question
Compute $f(x) = e^x (\cosh x - \sinh x)$ at $x = 1, 10, 100$, and $1000$.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs

# #+LATEX: \only<handout>{
*Numerically:*
#+BEGIN_CENTER
#+ATTR_LATEX: :options [t]{0.9\linewidth}
#+BEGIN_minipage
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
 format long
 x = input(' x = ');
 y = exp(x) * ( cosh(x) - sinh(x) );
 disp([x, y])
#+END_SRC
#+END_minipage
#+END_CENTER
# #+LATEX: }

**** Example 2: Cancellation for Small Values of $x$
***** Question
Compute $\displaystyle f(x) = \frac{\sqrt{1+x} - 1}{x}$ at $x = 10^{-12}$.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
*Numerically:*
#+BEGIN_CENTER
#+ATTR_LATEX: :options [t]{0.9\linewidth}
#+BEGIN_minipage
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
 x = 1e-12;
 fx = (sqrt(1+x) - 1)/x;
 disp( fx )
#+END_SRC
#+END_minipage
#+END_CENTER

**** To Avoid Such Cancellations @@latex: \ldots @@
 - Unfortunately, there is no universal way to avoid loss of precision.
 - One way to avoid catastrophic cancellation is to remove the source of cancellation by simplifying the given expression before computing numerically.
 - For Example 1, rewrite the given expression recalling that
   \[
     \cosh x = ({e^x + e^{-x}})/{2}
     \quad\text\quad
     \sinh x = ({e^x - e^{-x}})/{2}.
   \]
 - For Example 2, try again after rewriting $f(x)$ as
   \begin{equation*}
     f(x)
     = \frac{\sqrt{1+x} - 1}{x} \cdot
     \frac{\sqrt{1+x} + 1}{\sqrt{1+x} + 1}
     = \frac{1}{\sqrt{1+x} + 1} \,.
   \end{equation*}
 - Do you now have an improved accuracy?
**** Extra Material                                                                :noexport:
***** Theorem on Loss of Precision                                               :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Let $x$ and $y$ be normalized floating point numbers, where $x > y > 0$. If $2^{-p} \le 1 - (y/x) \le 2^{-q}$ for some positive integers $p$ and $q$, then at most $p$ and at least $q$ binary bits are lost in the subtraction $x-y$.
*** Catastrophic Cancellation
**** Catastrophic Cancellation

In finite precision storage, two numbers that are close to each other are indistinguishable.  So subtraction of two nearly equal numbers on a computer can result in loss of many significant digits. This phenomenon is known as *catastrophic cancellation* or *subtractive cancellation*.
\vfill

#+LATEX: \bgroup\small
***** Catastrophic Cancellation                                             :B_exampleblock:
:PROPERTIES:
:BEAMER_env: exampleblock
:END:
Consider two real numbers stored with 10 digits of precision:
\begin{align*}
  e & = 2.7182818284, \\
  b & = 2.7182818272.
\end{align*}
If we compute $e - b$ based on the stored numbers, we obtain $0.0000000012 = 1.2\times 10^{-9}$, a number with only two significant digits.
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+LATEX: \egroup

\vfill

Catastrophic cancellation is unavoidable once numbers are represented to a fixed number of digits.

**** Example 1: Cancellation for Large Values of $x$
***** Question
Compute $f(x) = e^x (\cosh x - \sinh x)$ at $x = 1, 10, 100$, and $1000$.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs

# #+LATEX: \only<handout>{
*Numerically:*
#+BEGIN_CENTER
#+ATTR_LATEX: :options [t]{0.9\linewidth}
#+BEGIN_minipage
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
 format long
 x = input(' x = ');
 y = exp(x) * ( cosh(x) - sinh(x) );
 disp([x, y])
#+END_SRC
#+END_minipage
#+END_CENTER
# #+LATEX: }

**** Example 2: Cancellation for Small Values of $x$
***** Question
Compute $\displaystyle f(x) = \frac{\sqrt{1+x} - 1}{x}$ at $x = 10^{-12}$.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
*Numerically:*
#+BEGIN_CENTER
#+ATTR_LATEX: :options [t]{0.9\linewidth}
#+BEGIN_minipage
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
 x = 1e-12;
 fx = (sqrt(1+x) - 1)/x;
 disp( fx )
#+END_SRC
#+END_minipage
#+END_CENTER

**** To Avoid Such Cancellations @@latex: \ldots @@
 - Unfortunately, there is no universal way to avoid loss of precision.
 - One way to avoid catastrophic cancellation is to remove the source of cancellation by simplifying the given expression before computing numerically.
 - For Example 1, rewrite the given expression recalling that
   \[
     \cosh x = ({e^x + e^{-x}})/{2}
     \quad\text\quad
     \sinh x = ({e^x - e^{-x}})/{2}.
   \]
 - For Example 2, try again after rewriting $f(x)$ as
   \begin{equation*}
     f(x)
     = \frac{\sqrt{1+x} - 1}{x} \cdot
     \frac{\sqrt{1+x} + 1}{\sqrt{1+x} + 1}
     = \frac{1}{\sqrt{1+x} + 1} \,.
   \end{equation*}
 - Do you now have an improved accuracy?
*** Conditioning

 - FNC 1.2
 - LM 9.3

**** Problems and Conditioning
 - A mathematical /problem/ can be viewed as a function $f:X \to Y$ from a data/input space $X$ to a solution/output space $Y$.
 - We are interested in changes in $f(x)$ caused by small perturbations of $x$.
 - A /well-conditioned/ problem is one with the property that all small perturbations of $x$ lead to only small changes in $f(x)$

**** Condition Number

Let $f:\RR \to \RR$. For $x \in \RR$, let $\text{fl}(x) = x(1+\epsilon)$ where $\abs{\epsilon} \le \meps/2$.

 - The ratio of the relative error in the output of $f$ due to the change in the input from $x$ to $\text{fl}(x)$ simplifies to
   \[
     \frac{\abs{f(x(1+\epsilon)) - f(x)}}{\abs{\epsilon f(x)}}.
   \]
 - In the limit of small error as $\epsilon \to 0$ (ideal computer), we obtain
   \begin{multline*}
     \kappa_{f}(x) := \lim_{\epsilon \to 0}\frac{\abs{f(x(1+\epsilon)) - f(x)}}{\abs{\epsilon f(x)}} \\
     = \abs{ \lim_{\epsilon \to 0} \frac{f(x+\epsilon x) - f(x)}{\epsilon x} \cdot \frac{x}{f(x)} }
     = \abs{ \frac{x f'(x)}{f(x)} }, \tag{$\star$}
   \end{multline*}
   which is called the *(relative) condition number* of the problem $f$.

**** Condition Number {{{cont}}}
 - It can be shown that if $h(x) = f(g(x))$, then
   \[
   \kappa_h(x) = \kappa_f(g(x)) \cdot \kappa_g(x).
   \]
 - From the definition of the condition number, we can see that
   \[
   \abs{ \frac{ f(x(1+\epsilon)) - f(x) }{ f(x) } } \approx \kappa_f(x) \abs{\epsilon}, \quad\text{for small }\abs{\epsilon}.
   \]
   This allows us to view $\kappa_f(x)$ as the factor by which the relative change in $x$ is magnified in the result.
 - We say that a problem is /ill-conditioned/ when $\kappa_f(x)$ is large.
 - If $\kappa_f(x) \approx 10^d$, then about $d$ decimal digits of accuracy are expected to be lost in computation of $f(x)$.

**** Example: Conditioning of Subtraction

Here we revisit catastrophic cancellation in light of conditioning of subtraction.
\vs

Consider $f(x) = x - c$ where $c$ is some constant. Using the formula ($\star$), we find that the associated condition number is
\[
  \kappa(x) = \abs{\frac{xf'(x)}{f(x)}} = \abs{\frac{x}{x-c}}.
\]

 - It is large when $x \approx c$.

**** Example: Conditioning of Multiplication
The condition number of $f(x) = cx$ is
\[
  \kappa(x) = \abs{ \frac{xf'(x)}{f(x)} } = \abs{\frac{x \cdot c}{cx}} = 1.
\]
 - No magnification of error.

**** Example: Conditioning of Function Evaluation
The condition number of $f(x) = \cos(x)$ is
\[
  \kappa(x) = \abs{ \frac{xf'(x)}{f(x)} } = \abs{\frac{-x \sin x}{\cos x}} = \abs{x \tan x}.
\]
 - The condition number is large when $x = (n + 1/2)\pi$, where $n \in \ZZ$.

**** Condition Numbers of Elementary Functions

#+ATTR_LATEX: :align ll
| Function         | Condition Number                  |
|------------------+-----------------------------------|
| $f(x) = x+c$     | $\kappa_f(x) = \abs{x}/\abs{x+c}$ |
| $f(x) = cx$      | $\kappa_f(x) = 1$                 |
| $f(x) = x^p$     | $\kappa_f(x) = \abs{p}$           |
| $f(x) = e^x$     | $\kappa_f(x) = \abs{x}$           |
| $f(x) = \sin(x)$ | $\kappa_f(x) = \abs{x \cot(x)}$   |
| $f(x) = \cos(x)$ | $\kappa_f(x) = \abs{x \tan(x)}$   |
| $f(x) = \log(x)$ | $\kappa_f(x) = 1/\abs{\log(x)}$   |

**** Example: Conditioning of Root-Finding

Let $r = f(a; b,c)$ be a root of $ax^2 + bx + c = 0$. Instead of direct differentiation, use implicit differentiation
\[
  r^2 + 2ar \frac{dr}{da} + b \frac{dr}{da} = 0.
\]
Solve for the derivative,
\[
  f'(a) = \frac{dr}{da} = - \frac{r^2}{2ar + b} = - \frac{r^2}{\pm \sqrt{b^2 - 4ac} },
\]
then compute the condition number using the formula ($\star$) to get
\[
  \kappa(a) = \abs{\frac{a f'(a)}{f(a)}}
  = \abs{\frac{a r^2}{\pm r \sqrt{b^2-4ac}}}
  = \abs{\frac{ar}{\sqrt{b^2-4ac}}}.
\]

 - Conditioning is poor for small discriminant, \textit{i.e.,} near repeated roots.

*** Stability

 - FNC 1.3

**** Algorithms
Let $f$ be a mathematical function, which we think of as a problem.
 - A complete set of computational instructions which maps an input $x$ to the output $f(x)$ is called an *algorithm*.
 - Even a simple problem can be solved using different algorithms which have surprisingly different characteristics.

**** Example: Polynomial Evaluation
Consider the two algorithms below which evaluates a polynomial
\[
p(x) = c_1 + c_2 x + c_3 x^2 + \cdots + c_n x^{n-1}.
\]

#+ATTR_LATEX: :options [t]{0.45\linewidth}
#+BEGIN_minipage
*Algorithm 1*.
#+ATTR_LATEX: :options style=matlab, linewidth=0.9\linewidth
#+BEGIN_SRC matlab
y = c(1);
xpow = 1;
for i = 2:n
    xpow = xpow * x;
    y = y + c(i)*xpow;
end
#+END_SRC
#+END_minipage
\hfill
#+ATTR_LATEX: :options [t]{0.45\linewidth}
#+BEGIN_minipage
*Algorithm 2*. (Horner's method)
#+ATTR_LATEX: :options style=matlab, linewidth=0.9\linewidth
#+BEGIN_SRC matlab
y = c(n);
for j = n-1:-1:1
    y = y*x + c(j);
end
#+END_SRC
#+END_minipage
\vs

Assuming that =x= is a scalar, how many additions and multiplications are needed for each of the two algorithms?

**** Example: Polynomial Evaluation {{{cont}}}
Note that the polynomial
\[
p(x) = c_n x^{n-1} + c_{n-1} x^{n-2} + \cdots + c_2 x + c_1.
\]
can be written as
\[
p(x) = (\cdots((c_n x + c_{n-1}) x + c_{n-2})x + \cdots +c_2)x + c_1,
\]
which yields Horner's method. It is written as a MATLAB function below.
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs

#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
function p = horner(c, x)
% HORNER evaluates polynomial using Horner's method.
    n = length(c);
    p = c(n);
    for k = n-1:-1:1
        p = p*x + c(k);
    end
end
#+END_SRC

**** Perspective: Algorithms as Functions
 - Recall that we viewed a /problem/ as a function $f: X \to Y$.
 - An /algorithm/ can be viewed as another map $\tilde{f}:X \to Y$ between the same two spaces, which involves errors arising in
   - representing the actual input $x$ as $\hat{x}=\text{fl}(x)$;
   - implementing the function $f$ numerically on a computer.
 - So the (relative) error of our interest is
   \[
     \abs{\frac{\tilde{f}(\hat{x})-f(x)}{f(x)}}
     \quad\text{relative to}\quad
     \abs{ \frac{\hat{x}-x}{x}  }.
   \]

**** Breakdown of Errors and Stability of Algorithm
Note that
\begin{multline*}
  \abs{\frac{\tilde{f}(\hat{x})-f(x)}{f(x)}}
  \le
  \abs{\frac{\tilde{f}(\hat{x}) - f(\hat{x})}{f(x)}} +
  \abs{\frac{f(\hat{x}) - f(x)}{f(x)}} \\
  \lessapprox
  \underbrace{\abs{\frac{\tilde{f}(\hat{x}) - f(\hat{x})}{f(\hat{x})}}}_{\text{numerical error}} +
  \underbrace{\abs{\frac{f(\hat{x}) - f(x)}{f(x)}}}_{\text{perturbation error}} \le (\hat{\kappa}_{\rm num} + \kappa_f) \, \meps.
\end{multline*}
where $\kappa = \kappa_f$ is the (relative) condition number of the exact problem $f$ and
\[
  \hat{\kappa}_{\rm num} = \max \left. \abs{\frac{\tilde{f}(\hat{x}) - f(\hat{x})}{f(\hat{x})}} \,\middle/\, \abs{\frac{\hat{x} - x}{x}} \right. .
\]

*Upshot*. When solving a problem using a computer algorithm results in an error which is larger than what conditioning can explain, we say that the algorithm is *unstable*.

**** Example: Root-Finding Revisited
Consider again solving the quadratic equation $ar^2 + br + c = 0$. \vs
 - Taking $a = c = 1$ and $b = -(10^6 + 10^{-6})$, the roots can be computed exactly by hand: $r_1 = 10^6$ and $r_2 = 10^{-6}$.
 - If numerically computed in MATLAB using the quadratic equation formula, $r_1$ is correct but $r_2$ has only 5 correct digits.
 - Fix it using $r_2 = (c/a)/r_1$.

**** Backward Errors                                                               :noexport:
If $f$ is a poorly conditioned problem, even a good algorithm $\tilde{f}$ may not have a small relative error
\[
\abs{ \frac{\tilde{f}(x) - f(x)}{f(x)} }.
\]
In such a case, the *backward error* may serve as a good measurement of the error.
* DONE Chapter 2: Square Linear Systems
:PROPERTIES:
:EXPORT_TITLE: Square Linear Systems
:EXPORT_FILE_NAME: square-linear-systems
:EXPORT_OPTIONS: H:3 date:nil
:EXPORT_AUTHOR:
:END:
** DONE Introduction to Square Linear Systems
:PROPERTIES:
:EXPORT_FILE_NAME: 11-square-linear-systems
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Notes                                                                           :noexport:
**** References
 - FNC 2.1-2.3
 - LM 10.1
 - NCM 1.7 (floating-point arithmetic)

**** Objectives
 - where these systems may arise
 - efficient representation in MATLAB
 - learn algorithms to solve simple triangular systems

 - learn increasingly sophisticated algorithms to solve these systems
 - learn tools to measure the performance of these algorithms
 - learn to recognize where difficulties may arise in solving linear systems
 - learn to recognize where structures of the problem can be exploited for fast and accurate solutions

**** Misc

*** Opening Example: Polynomial Interpolation
**** Polynomial Interpolation

***** Formal Statement
Given a set of $n$ data points $\{(x_j,y_j) \mid j \in \NN[1,n]\}$ with distinct $x_j$'s, not necessarily sorted, find a polynomial of degree $n-1$,
\[
  p(x) = c_1 + c_2 x + c_3 x^2 + \cdots + c_n x^{n-1},
  \tag{$\star$}
\]
which interpolates the given points, /i.e./,
\[
  p(x_j) = y_j, \quad \text{for } j = 1, 2, \ldots, n \,.
\]

*****                                                                      :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs

 - The goal is to determine the coefficients $c_1, c_2, \ldots, c_n$.
 - Note that the total number of data point is 1 larger than the degree of the interpolating polynomial.
**** Why Do We Care?
 - to find the values between the discrete data points;
 - to approximate a (complicated) function by a polynomial, which makes such computations as differentiation or integration easier.

**** Interpolation to Linear System
:PROPERTIES:
:BEAMER_opt: shrink=5
:END:
Writing out the $n$ /interpolating conditions/ $p(x_j) = y_j$:
\vs

***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.48
:END:
#+LATEX: \hspace*{\fill}
_*Equations*_
#+LATEX: \hspace*{\fill}
\begin{equation*}
  \left\{ \,
  \begin{array}{*{9}{@{}c@{}}}
    c_1 & {}+{} & c_2 x_1 & {}+{} & \cdots & {}+{} & c_n x_1^{n-1} & {}\mathrel{=}{} & y_1 \\
    c_1 & {}+{} & c_2 x_2 & {}+{} & \cdots & {}+{} & c_n x_2^{n-1} & {}\mathrel{=}{} & y_2 \\
    \vdots         &       & \vdots         &      &   &       & \vdots         &                 & \vdots \\
    c_1 & {}+{} & c_2 x_n & {}+{} & \cdots & {}+{} & c_n x_n^{n-1} & {}\mathrel{=}{} & y_n
  \end{array}
  \, \right\}
\end{equation*}

****** center                                                                       :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.04
:END:

\vspace{3.4em}

$\rightarrow$

****** Vandermonde Matrix                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.48
:END:
#+LATEX: \hspace*{\fill}
_*Matrix equation*_
#+LATEX: \hspace*{\fill}
\begin{equation*}
  \underbrace{
    \begin{bmatrix}
      1      & x_1    & \cdots &  x_1^{n-1} \\
      1      & x_2    & \cdots &  x_2^{n-1} \\
      \vdots & \vdots &        & \vdots \\
      1      & x_n    & \cdots &  x_n^{n-1}
    \end{bmatrix}
  }_{V}
  \underbrace{
    \begin{bmatrix}
      c_1 \\ c_2 \\ \vdots \\ c_n
    \end{bmatrix}
  }_{\bc}
  =
  \underbrace{
    \begin{bmatrix}
      y_1 \\ y_2 \\ \vdots \\ y_n
    \end{bmatrix}
  }_{\by}
\end{equation*}

# \begin{equation*}
#   V =
#   \begin{bmatrix}
#     1      & x_1    & \cdots & x_1^{n-2} & x_1^{n-1} \\
#     1      & x_2    & \cdots & x_2^{n-2} & x_2^{n-1} \\
#     \vdots & \vdots &        & \vdots   & \vdots \\
#     1      & x_n    & \cdots & x_n^{n-2} & x_n^{n-1}
#   \end{bmatrix},
#   \quad
#   \bc =
#   \begin{bmatrix}
#     c_1 \\ c_2 \\ \vdots \\ c_n
#   \end{bmatrix},
#   \quad
#   \by =
#   \begin{bmatrix}
#     y_1 \\ y_2 \\ \vdots \\ y_n
#   \end{bmatrix} \,. \,.
# \end{equation*}

*****                                                                      :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - This is a linear system of $n$ equations with $n$ unknowns.
 - The matrix $V$ is called a *Vandermonde matrix*.

**** Example: Fitting Population Data
U.S. Census data are collected every 10 years.

#+ATTR_LATEX: :align c|c
| Year | Population (millions) |
|------+-----------------------|
| 1980 |               226.546 |
| 1990 |               248.710 |
| 2000 |               281.422 |
| 2010 |               308.746 |
| 2020 |               332.639 |

\vs
*Question.* How do we estimate population in other years?
\vs
 - Interpolate available data to compute population in intervening years.

**** Example: Fitting Population Data {{{cont}}}
***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.55
:END:
 - Input data.
 - Match up notation (optional).
 - Note the shift in Line 7.
 - Construct the Vandermonde matrix $V$ by /broadcasting/.
 - Solve the system using the backslash (=\=) operator.

****** spacer                                                                       :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.05
:END:

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :options style=matlab, numbers=left, numberstyle={\tiny \color{gray}}, numbersep=7pt
#+BEGIN_SRC matlab
 year = (1980:10:2020)';
 pop = [226.546;
        248.710;
        281.422;
        308.746;
        332.639];
 x = year - 1980;
 y = pop;
 n = length(x);
 V = x.^(0:n-1);
 c = V \ y;
#+END_SRC

#+RESULTS:
#+begin_example
year = (1980:10:2020)';
pop = [226.546;
        248.710;
        281.422;
        308.746;
        332.639];
x = year - 1980;
y = pop;
n = length(x);
V = x.^(0:n-1);
c = V \ y;
'org_babel_eoe'
ans =
    'org_babel_eoe'
#+end_example

**** Post-Processing

#+ATTR_LATEX: :options style=matlab, numbers=left, numberstyle={\tiny \color{gray}}, numbersep=7pt
#+BEGIN_SRC matlab
 xx = linspace(0, 40, 100)';
 yy = polyval(flip(c), xx);
 clf
 plot(1980+x, y, '.', 1980+xx, yy)
 title('US Population'),
 xlabel('year'), ylabel('population (millions)')
 legend('data', 'interpolant', 'location', 'northwest')
#+END_SRC

***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+LATEX: \begingroup\small
 - Use the =polyval= function to evaluate the polynomial.
 - MATLAB expects coefficients to be in descending order. (=flip=)
#+LATEX: \endgroup

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.48
:END:
\vspace{-1.5em}
#+ATTR_LATEX: :width 0.9\linewidth
[[../img/us_pop_interp.eps]]

*** Square Linear Systems
**** Overview
Let $A \in \RR^{n \times n}$ and $\bb \in \RR^n$. Then the equation $A\bx = \bb$ has the following possibilities:

 - If $A$ is invertible (or nonsingular), then $A\bx = \bb$ has a unique solution $\bx = A^{-1} \bb$, or
 - If $A$ is not invertible (or singular), then $A\bx = \bb$ has either no solution or infinitely many solutions.

\vs

***** The Backslash Operator `` =\= ''
To solve for $\bx$ in MATLAB, we use the backslash symbol `` =\= '':
#+BEGIN_EXAMPLE
  >> x = A \ b
#+END_EXAMPLE

This produces the solution without explicitly forming the inverse of $A$.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
*Warning:* Even though $\bx = A^{-1}\bb$ analytically, don't use =x = inv(A)*b=!

**** Triangular Systems
#+LATEX: \begingroup \small
Systems involving triangular matrices are easy to solve.
\vs
 - A matrix $U \in \RR^{n \times n}$ is *upper triangular* if all entries below main diagonal are zero:
   \[
     U =
     \begin{bmatrix}
       u_{11} & u_{12} & u_{13} & \cdots & u_{1n} \\
       0 & u_{22} & u_{23} & \cdots & u_{2n} \\
       0 & 0 & u_{33} & \cdots & u_{3n} \\
       \vdots & \vdots & \vdots & \ddots & \vdots \\
       0 & 0 & 0 & \cdots & u_{nn} \\
     \end{bmatrix}.
   \]
 - A matrix $L \in \RR^{n \times n}$ is *lower triangular* if all entries above main diagonal are zero:
   \[
     L =
     \begin{bmatrix}
       \ell_{11} & 0 & 0 & \cdots & 0 \\
       \ell_{21} & \ell_{22} & 0 & \cdots & 0 \\
       \ell_{31} & \ell_{32} & \ell_{33} & \cdots & 0 \\
       \vdots & \vdots & \vdots & \ddots & \vdots \\
       \ell_{n1} & \ell_{n2} & \ell_{n3} & \cdots & \ell_{nn}
     \end{bmatrix}.
   \]

#+LATEX: \endgroup

**** Example: Upper Triangular Systems
Solve the following $3 \times 3$ system
\begin{equation*}
  \begin{bmatrix}
    u_{11} & u_{12} & u_{13} \\
    0      & u_{22} & u_{23} \\
    0      & 0      & u_{33}
  \end{bmatrix}
  \begin{bmatrix}
    x_1 \\ x_2 \\ x_3
  \end{bmatrix} =
  \begin{bmatrix}
    b_1 \\ b_2 \\ b_3
  \end{bmatrix}.
\end{equation*}

**** Example: Upper Triangular Systems (4x4)                                       :noexport:
Solve the following $4 \times 4$ system
\begin{equation*}
  \begin{bmatrix}
    u_{11} & u_{12} & u_{13} & u_{14} \\
    0      & u_{22} & u_{23} & u_{24} \\
    0      & 0      & u_{33} & u_{34} \\
    0      & 0      & 0      & u_{44}
  \end{bmatrix}
  \begin{bmatrix}
    x_1 \\ x_2 \\ x_3 \\ x_4
  \end{bmatrix} =
  \begin{bmatrix}
    b_1 \\ b_2 \\ b_3 \\ b_4
  \end{bmatrix}.
\end{equation*}

**** General Results
#+LATEX: \begingroup\small
 - *Backward Substitution.* To solve a general $n \times n$ upper triangular system $U \bx = \bb$:
   \begin{equation*}
     \left\{ \;
       \begin{aligned}
         x_n & = \frac{b_n}{u_{nn}} \quad \text{and}  \\
         x_i & = \frac{1}{u_{ii}} \left( b_i - \sum_{j=i+1}^{n} u_{ij}x_j \right)
         \quad\text{for } i = n-1, n-2, \ldots, 1.
       \end{aligned}
     \right.
   \end{equation*}
 - *Forward Elimination.* To solve a general $n \times n$ lower triangular system $L \bx = \bb$:
   \begin{equation*}
     \left\{ \;
       \begin{aligned}
         x_1 & = \frac{b_1}{\ell_{11}} \quad \text{and}  \\
         x_i & = \frac{1}{\ell_{ii}} \left( b_i - \sum_{j=1}^{i-1} \ell_{ij}x_j \right)
         \quad\text{for } i = 2, 3, \ldots, n.
       \end{aligned}
     \right.
   \end{equation*}
#+LATEX: \endgroup
**** Does It Always Work?
***** Singularity of Triangular Matrix                                             :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
A triangular matrix is singular if and only if at least one of its diagonal elements is zero.

**** Implementation: Backward Substitution
#+ATTR_LATEX: :options style=matlab,numbers=left,numberstyle={\tiny \color{black}},numbersep=7pt
#+BEGIN_SRC matlab
 function x = backsub(U,b)
 % BACKSUB x = backsub(U,b)
 % Solve an upper triangular linear system.
 % Input:
 %   U    upper triangular square matrix (n by n)
 %   b    right-hand side vector (n by 1)
 % Output:
 %   x    solution of Ux=b (n by 1 vector)
     n = length(U);
     x = zeros(n,1); % preallocate
     for i = n:-1:1
         x(i) = ( b(i) - U(i,i+1:n)*x(i+1:n) ) / U(i,i);
     end
 end
#+END_SRC

**** Implementation: Backward Substitution {{{cont}}}
We have encouraged to avoid (nested) loops in MATLAB and write vectorized codes instead whenever possible, for performance reasons.
\vs

In the backward substitution formula, the sum  $\sum_{j=i+1}^{n} u_{ij}x_j$ can be viewed as an inner product
\begin{equation*}
  \begin{bmatrix}
    u_{i,i+1} & u_{i,i+2} & \cdots & u_{i,n}
  \end{bmatrix}
  \begin{bmatrix}
    x_{i+1} \\ x_{i+2} \\ \vdots \\ x_{n}
  \end{bmatrix}
  = \underbrace{\mathtt{U(i,i+1:n)}}_{\text{row vector}} \cdot \underbrace{\mathtt{x(i+1:n)}}_{\text{column vector}}.
\end{equation*}
This viewpoint is directly implemented in the previous code in line 12.

**** Implementation: Forward Elimination
*Exercise.* Complete the code below.
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
 function x = forelim(L,b)
 % FORELIM x = forelim(L,b)
 % Solve a lower triangular linear system.
 % Input:
 %   L    lower triangular square matrix (n by n)
 %   b    right-hand side vector (n by 1)
 % Output:
 %   x    solution of Lx=b (n by 1 vector)





 end
#+END_SRC

** DONE LU Factorization
:PROPERTIES:
:EXPORT_FILE_NAME: 12-lu-fact
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Notes                                                                           :noexport:
**** References
 - FNC 2.4, 2.6
*** Gaussian Elimination
**** General Method: Gaussian Elimination
 - /Gaussian elimination/ is an algorithm for solving a general system of linear equations that involves a sequence of row operations performed on the associated matrix of coefficients.
 - This is also known as the method of row reduction.
 - There are three variations to this method:
   - G.E. without pivoting
   - G.E. with partial pivoting (that is, row pivoting)
   - G.E. with full pivoting (that is, row and column pivoting)

**** Gaussian Elimination: Example
***** Key Example
<<keyexample>>
#+LATEX: \begingroup\small
Solve the following system of equations.
\begin{equation*}
  \left\{
  \begin{alignedat}{4}
    2x_1  & {}+{} & 2x_2 & {}+{} & x_3  & {}={} & 6 \\
    -4x_1 & {}+{} & 6x_2 & {}+{} & x_3  & {}={} & -8 \\
    5x_1  & {}-{} & 5x_2 & {}+{} & 3x_3 & {}={} & 4
  \end{alignedat}
  \right.
  \quad
  \xrightarrow{\text{ matrix equation }}
  \quad
  \underbrace{
    \begin{bmatrix*}[r]
      2  & 2  & 1 \\
      -4 & 6  & 1 \\
      5  & -5 & 3
    \end{bmatrix*}
  }_{A}
  \underbrace{
    \begin{bmatrix*}[r]
      x_1 \\ x_2 \\ x_3
    \end{bmatrix*}}_{\bx}
  =
  \underbrace{
    \begin{bmatrix*}[r]
      6 \\ -8 \\ 4
    \end{bmatrix*}}_{\bb}
\end{equation*}
#+LATEX: \endgroup

**** Gaussian Elimination: Example {{{cont}}}
***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** Step1                                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.55
:END:
*Step 1:* Write the corresponding /augmented matrix/ and row-reduce to an echelon form.

#+LATEX: \begingroup \footnotesize
  \begin{align*}
    &
    \begin{bmatrix*}
      \begin{array}{rrr|r}
        2  & 2  & 1 & 6 \\
        -4 & 6  & 1 & -8 \\
        5  & -5 & 3 & 4
      \end{array}
    \end{bmatrix*} \\[1em]
    &\xrightarrow{\substack{R_2 \to R_2 + 2 R_1 \\ R_3 \to R_3 - (5/2)R_1}}
    \begin{bmatrix*}
      \begin{array}{rrr|r}
        2 & 2  & 1   & 6 \\
        0 & 10 & 3   & 4 \\
        0 & -10 & 0.5 & -11
      \end{array}
    \end{bmatrix*} \\[1em]
    &\xrightarrow[\phantom{R_3 \to R_3 - (5/2)R_1}]{R_3 \to R_3 + R_2}
    \begin{bmatrix*}
      \begin{array}{rrr|r}
        2 & 2  & 1   & 6 \\
        0 & 10 & 3   & 4 \\
        0 & 0  & 3.5 & -7
      \end{array}
    \end{bmatrix*}.
  \end{align*}
#+LATEX: \endgroup
****** Step2                                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.45
:END:
*Step 2:* Solve for $x_3$, then $x_2$, and then $x_1$ via /backward substitution/.

#+LATEX: \begingroup \footnotesize
\begin{equation*}
  \left\{
  \begin{alignedat}{4}
    2x_1 & {}+{} & 2x_2  & {}+{} & x_3    & {}={} & 6 \\
         &       & 10x_2 & {}+{} & 3x_3   & {}={} & 4 \\
         &       &       &       & 3.5x_3 & {}={} & -7
  \end{alignedat}
  \right.
\end{equation*}

By backward substitution:
\[
  \bx =
  \begin{bmatrix}[r]
    3 \\ 1 \\ -2
  \end{bmatrix}.
\]
#+LATEX: \endgroup

**** Gaussian Elimination: General Procedure
<<back-subs>>

As shown in the example, Gaussian elimination involves two steps:
\vs

#+LATEX: \begingroup\small
 1. *Row reduction:* Transform $A \bx = \bb$ to $U \bx = \bbeta$ where
    \begin{equation*}
      U =
      \left[
        \begin{array}{ccccc}
          u_{11}   & u_{12} & \cdots &  u_{1n} \\
                   & u_{22} & \cdots  &  u_{2n} \\
                   &        & \ddots & \vdots \\
          \bigzero &        &        & u_{nn} \\
        \end{array}
       \right]
      \quad \text{and} \quad
      \bbeta =
      \begin{bmatrix*}
        \beta_1 \\ \beta_2 \\ \vdots \\ \beta_n
      \end{bmatrix*}.
    \end{equation*}
 2. *Backward substitution:* Solve $U \bx = \bbeta$ for $\bx$ by
    \begin{equation*}
      \left\{\;
        \begin{aligned}
          x_n & = \frac{\beta_n}{u_{nn}} \quad \text{and}  \\
          x_i & = \frac{1}{u_{ii}} \left( \beta_i - \sum_{j=i+1}^{n} u_{ij}x_j \right),
          \quad \text{for} \; i=n-1, n-2, \dots, 1 \,.
        \end{aligned}
      \right.
    \end{equation*}
#+LATEX: \endgroup

**** MATLAB Implementation of Gaussian Elimination
#+NAME: code:GEnp.m
#+ATTR_LATEX: :options style=matlab, numbers=left, numberstyle={\tiny \color{gray}}, numbersep=7pt
#+BEGIN_SRC matlab
 function x = GEnp(A, b)
   % Step 1: Row reduction to upper tri. system
   S = [A, b];          % augmented matrix
   n = size(A, 1);
   for j = 1:n-1
       for i = j+1:n
           mult = -S(i,j)/S(j,j);
           S(i,:) = S(i,:) + mult*S(j,:);
       end
   end
   % Step 2: Backward substitution
   U = S(:,1:end-1);
   beta = S(:,end);
   x = backsub(U, beta);
 end
#+END_SRC

*Exercise.* Rewrite Lines 6--9 without using a loop. (Think /vectorized/!)

*** LU Factorization
**** Emulation of Gaussian Elimination
The goal of this section is to express Gaussian elimination using matrices. The main tool in this process is the matrix which emulates a row replacement of the form
\[
R_i \rightarrow R_i + c R_j.
\]

**** Elementary Matrices
Throughout this section, let $n \in \NN$ be fixed. Let $1 \le j < i \le n$.
\vs

The row operation $R_i \to R_i + c R_j$ on $A \in \RR^{n \times n}$, for some $c \in \RR$, can be emulated by a matrix multiplication
\[
  (I + c \, \be_i \be_j\tp) A.
\]
The matrix in parentheses is often called an /elementary matrix/.

**** Properties of Elementary Matrices
***** Properties of Elementary Matrices
Let $1 \le j < n$ be fixed and let $j < i \le n$.
 1. The matrix $(I + c\, \be_i \be_j\tp)$ is a /unit/ lower triangular matrix.
 2. $(I + c\, \be_i \be_j\tp)^{-1} = I - c\, \be_i \be_j\tp$.
 3. $(I + c\, \be_i \be_j\tp)(I + d\, \be_k \be_j\tp) = I + c\, \be_i \be_j\tp + d\, \be_k \be_j\tp$, for some $j < k \le n$, $k \neq i$.

**** Gaussian Transformation Matrices
Let $1 \le j < n$. In the context of Gaussian elimination, the operation of introducing zeros below the $j\text{th}$ diagonal entry can be done via
\[
\underbrace{\prod_{i=j+1}^{n} (I + c_{i,j} \, \be_i \be_j\tp) }_{=G_j} A.
\]
By the third property of elementary matrices from the previous slide,
\[
G_j = I + \sum_{i=j+1}^{n} c_{i,j}\, \be_i \be_j\tp.
\]
The matrix $G_j$ is called a /Gaussian transformation matrix/ (GTM).

**** Properties of Gaussian Transformation Matrices
***** Properties of Gaussian Transformation Matrices
For $1 \le j < n$, let
\[
G_j = I + \sum_{i=j+1}^{n} c_{i,j}\, \be_i \be_j\tp.
\]
 1. $G_j$ is a /unit/ lower triangular matrix.
 2. $G_j^{-1} = I - \sum_{i=j+1}^{n} c_{i,j}\, \be_i \be_j\tp$.
 3. $\prod_{j=1}^{n-1} G_j = I + \sum_{j=1}^{n-1} \sum_{i=j+1}^{n} c_{i,j} \, \be_i \be_j\tp$.

**** Elementary Row Operation and GTM
 - To emulate $(I + c \be_i \be_j\tp)A$ in MATLAB:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 A(i,:) = A(i,:) + c*A(j,:);
   #+END_SRC
 - To emulate
   \[
   G_j A = \left(I + \sum_{i=j+1}^n c_{i,j} \be_i \be_j\tp\right)A
   \]
   in MATLAB:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 for i = j+1:n
     c = ....
     A(i,:) = A(i,:) + c*A(j,:);
 end
   #+END_SRC
   This can be done without using a loop. How?

**** Key Example Revisited
:PROPERTIES:
:BEAMER_opt: shrink=5
:END:
#+LATEX: \begingroup\footnotesize
Let's work out the key example from last time once again, now in matrix form $A\bx = \bb$.

#+LATEX: \begingroup\footnotesize
\begin{equation*}
  \begin{bmatrix*}[r]
    2  & 2  & 1 \\ -4 & 6  & 1 \\ 5 & -5 & 3
  \end{bmatrix*}
  \begin{bmatrix*}
    x_1 \\ x_2 \\ x_3
  \end{bmatrix*} =
  \begin{bmatrix*}[r]
    6 \\ -8 \\ 4
  \end{bmatrix*} \,.
\end{equation*}
#+LATEX: \endgroup
\vfill

_$j=1$_: Introduce zeros below the first diagonal by $R_2 \to R_2 + 2 R_1$ and $R_3 \to R_3 - (5/2)R_1$:
#+LATEX: \begingroup\footnotesize
\begin{equation*}
  \underbrace{\begin{bmatrix*}[r]
      1 & 0 & 0 \\ 2 & 1 & 0 \\ -5/2 & 0 & 1
    \end{bmatrix*}}_{G_1}
  {\color{black!70}\left[\rule{0pt}{20pt}
      {\normalcolor
        \begin{bmatrix*}[r]
          2  & 2  & 1 \\ -4 & 6  & 1 \\ 5 & -5 & 3
        \end{bmatrix*}
        \begin{bmatrix*}
          x_1 \\ x_2 \\ x_3
        \end{bmatrix*} =
        \begin{bmatrix*}[r]
          6 \\ -8 \\ 4
        \end{bmatrix*} }
    \right]}
  \longrightarrow
  \begin{bmatrix*}[r]
    2 & 2 & 1 \\ 0 & 10 & 3 \\ 0 & -10 & 0.5
  \end{bmatrix*}
  \begin{bmatrix*}
    x_1 \\ x_2 \\ x_3
  \end{bmatrix*} =
  \begin{bmatrix*}[r]
    6 \\ 4 \\ -11
  \end{bmatrix*}
\end{equation*}
#+LATEX: \endgroup
\vfill

_$j=2$_: Introduce a zero below the second diagonal by $R_3 \to R_3 + R_2$:
#+LATEX: \begingroup\footnotesize
\begin{equation*}
  \underbrace{
    \begin{bmatrix*}[r]
      1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 1 & 1
    \end{bmatrix*}
  }_{G_2}
  {\color{black!70} \left[\rule{0pt}{20pt}
      {\normalcolor
        \begin{bmatrix*}[r]
          2 & 2 & 1 \\ 0 & 10 & 3 \\ 0 & -10 & 0.5
        \end{bmatrix*}
        \begin{bmatrix*}
          x_1 \\ x_2 \\ x_3
        \end{bmatrix*} =
        \begin{bmatrix*}[r]
          6 \\ 4 \\ -11
        \end{bmatrix*}}
    \right]}
  \longrightarrow
  \underbrace{
    \begin{bmatrix*}[r]
      2 & 2 & 1 \\ 0 & 10 & 3 \\ 0 & 0 & 3.5
    \end{bmatrix*}
  }_{U}
  \begin{bmatrix*}
    x_1 \\ x_2 \\ x_3
  \end{bmatrix*} =
  \begin{bmatrix*}[r]
    6 \\ 4 \\ -7
  \end{bmatrix*}
\end{equation*}
#+LATEX: \endgroup
#+LATEX: \endgroup

**** Analysis of Example
The previous calculations can be summarized as
\[
  G_2 G_1 A = U \;\Longrightarrow\; A = \underbrace{G_1^{-1} G_2^{-1}}_{L} U.
\]
Note that $L$ is a unit lower triangular matrix according to the properties of GTMs.
\vs


#+ATTR_LATEX: :options [width=\linewidth,arc=0mm,boxrule=0.7pt,colback=white,nobeforeafter]
#+BEGIN_tcolorbox
*Upshot*. Gaussian elimination finds a unit lower triangular matrix $L$ and an upper triangular matrix $U$ such that $A = LU$. In particular,
 - the lower triangular entries of $L$ are the row multipliers in row reduction steps, and
 - the entries of $U$ are found at the end of elimination process.
#+END_tcolorbox

**** Generalization: LU Factorization
For a general matrix $A \in \RR^{n \times n}$, the procedure is summarized as
\[
  G_{n-1} \cdots G_2 G_1  A = U \,.
\]
which also leads to $A = LU$:
\[
  A =
  \underbrace{
    \left( G_1^{-1} G_2^{-1} \cdots G_{n-1}^{-1} \right)}_{=:L} U \,.
\]
This is called an *LU factorization* of the matrix $A$.

**** Implementation of LU Factorization
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
 function [L,U] = mylu(A)
 % MYLU   LU factorization (demo only--not stable!).
 % Input:
 %   A    square matrix
 % Output:
 %   L,U  unit lower triangular and upper triangular such that LU=A
   n = length(A);
   L = eye(n);   % ones on diagonal
   % Gaussian elimination
   for j = 1:n-1
     for i = j+1:n
       L(i,j) = A(i,j) / A(j,j);   % row multiplier
       A(i,j:n) = A(i,j:n) - L(i,j)*A(j,j:n);
     end
   end
   U = triu(A);
 end
#+END_SRC

**** Solving a Square System Using LU Factorization
Now the square linear system $A\bx = \bb$ can be solved in three steps.

***** Algorithm: Solving Square Systems Using LU Factorization
 1. Factor $A = LU$ using Gaussian elimination.
 2. Solve $L\by = \bb$ for $\by$ using forward elimination.
 3. Solve $U\bx = \by$ for $\bx$ using backward substitution.

** DONE PLU Factorization
:PROPERTIES:
:EXPORT_FILE_NAME: 13-plu-fact
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Gaussian Elimination with Partial Pivoting
**** G.E. with Partial Pivoting: Procedure
In this variation of G.E., reduction to echelon form is done slightly differently.
 - On the augmented matrix $[A \,|\, \bb]$,
   #+ATTR_LATEX: :options [width=0.9\linewidth,arc=0mm,boxrule=0.7pt,colback=white,title={Key Process (partial pivoting)}]
   #+BEGIN_tcolorbox
1. Find the entry in the first column with the largest absolute value. This entry is called the /pivot/.
2. Perform a row interchange, if necessary, so that the pivot is on the first diagonal position.
3. Use elementary row operations to reduce the remaining entries in the first column to zero.
   #+END_tcolorbox
 - Once done, ignore the first row and first column and repeat the *Key Process* on the remaining submatrix.
 - Continue this until the matrix is in a row-echelon form.

**** G.E. with Partial Pivoting: Example
#+LATEX: \begingroup\small

Let's solve the example from previous lecture again, now using G.E. with partial pivoting.

\vs

*1st column:*
#+LATEX: \begingroup\scriptsize
#+LATEX: \hfsetfillcolor{blue!10}
#+LATEX: \hfsetbordercolor{blue!10}
\begin{equation*}
  \begin{bmatrix}
    \begin{array}{rrr|r}
      \tikzmarkin{aa}(0.17,-0.15)(-0.2,0.23)
      2 & 2 & 1 & 6 \\
      -4 & 6 & 1 & -8 \\
      \hfsetbordercolor{blue!60}
      \tikzmarkin{a}(0.08,-0.07)(-0.08,0.22)5\tikzmarkend{a}\tikzmarkend{aa} & -5 & 3 & 4
    \end{array}
  \end{bmatrix}
  \xrightarrow{{\rm pivot}}
  \begin{bmatrix*}
    \begin{array}{rrr|r}
      5 & -5 & 3 & 4 \\ -4 & 6 & 1 & -8 \\ 2 & 2 & 1 & 6
    \end{array}
  \end{bmatrix*}
  \xrightarrow{{\rm zero}}
  \begin{bmatrix}
    \begin{array}{rrr|r}
      5 & -5 & 3 & 4 \\ 0 & 2 & 3.4 & -4.8 \\ 0 & 4 & -0.2 & 4.4
    \end{array}
  \end{bmatrix}
\end{equation*}
#+LATEX: \endgroup

*2nd column:*
#+LATEX: \begingroup\scriptsize
#+LATEX: \hfsetfillcolor{blue!10}
#+LATEX: \hfsetbordercolor{blue!10}
\begin{equation*}
  \begin{bmatrix}
    \begin{array}{rrr|r}
      5 & -5 & 3 & 4 \\
      0 & \tikzmarkin{bb}(0.15,-0.15)(-0.15,0.2)2 & 3.4 & -4.8 \\
      0 & \hfsetbordercolor{blue!60}\tikzmarkin{b}(0.08,-0.07)(-0.08,0.22)4\tikzmarkend{b}\tikzmarkend{bb} & -0.2 & 4.4
    \end{array}
  \end{bmatrix}
  \xrightarrow{{\rm pivot}}
  \begin{bmatrix}
    \begin{array}{rrr|r}
      5 & -5 & 3 & 4 \\ 0 & 4 & -0.2 & 4.4 \\ 0 & 2 & 3.4 & -4.8
    \end{array}
  \end{bmatrix}
  \xrightarrow{{\rm zero}}
  \begin{bmatrix}
    \begin{array}{rrr|r}
      5 & -5 & 3 & 4 \\ 0 & 4 & -0.2 & 4.4 \\ 0 & 0 & 3.5 & -7
    \end{array}
  \end{bmatrix}
\end{equation*}
#+LATEX: \endgroup

\vs

Now that the last matrix is upper triangular, we work up from the third equation to the second to the first and obtain the same solution as before.

#+LATEX: \endgroup

**** G.E. with Partial Pivoting: MATLAB Implementation
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:
***** Exercise
Write a MATLAB function =GEpp.m= which carries out G.E. with partial pivoting.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
#+LATEX: \begingroup\small
 - Modify =GEnp.m= to incorporate partial pivoting.
 - The only part that needs to be changed is the for-loop starting at Line 5.
   - Right after =for j = 1:n-1=, find the index of the pivot element of the $j\text{th}$ column of $A$ below the diagonal.
     #+ATTR_LATEX: :options style=matlab
     #+BEGIN_SRC matlab
 [~, iM] = max(abs(A(j:end,j)));
 iM = iM + j - 1;
     #+END_SRC
   - If the pivot element is not on the diagonal, swap rows so that it is on the diagonal.
     #+BEGIN_SRC matlab
 if j ~= iM
     S([j iM], :) = S([iM j], :)
 end
     #+END_SRC
#+LATEX: \endgroup

**** =GEpp.m= (Solution)                                                           :noexport:
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:
#+ATTR_LATEX: :options style=matlab, numbers=left, numberstyle={\tiny \color{gray}}, numbersep=7pt
#+BEGIN_SRC matlab
function x = GEpp(A, b)
    % Step 1: pivoted row reduction
    S = [A, b];
    n = size(A, 1);
    for j = 1:n-1
        [~, iM] = max(abs(S(j:n,j)));
        iM = iM + j - 1;                % adjustment
        if j ~= iM                      % pivot if necessary
            piv = [j iM];
            S(piv, :) = S(flip(piv), :);
        end
        for i = j+1:n
            mult = -S(i,j)/S(j,j);
            S(i,:) = S(i,:) + mult*S(j,:);
        end
    end
    % Step 2: backward substitution
    U = S(:,1:end-1);
    beta = S(:,end);
    x = backsub(U, beta);
end
#+END_SRC

**** Why Is Pivoting Necessary?
***** Example
#+LATEX: \begingroup \small
  Given $\e \ll 1$, solve the system
  \begin{equation*}
    \begin{bmatrix}[rr]
      -\e & 1 \\ 1 & -1
    \end{bmatrix}
    \begin{bmatrix}
      x_1 \\ x_2
    \end{bmatrix}
    =
    \begin{bmatrix}
      1-\e \\ 0
    \end{bmatrix}
  \end{equation*}
  using Gaussian elimination with and _without partial pivoting_.
#+LATEX: \endgroup

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
#+LATEX: \begingroup\footnotesize
*Without pivoting:* By $R_2 \rightarrow R_2 + (1/\e)R_1$, we have
\begin{equation*}
  \begin{bmatrix*}
    -\e & 1 \\ 0 & -1 + 1/\e
  \end{bmatrix*}
  \begin{bmatrix*}
    x_1 \\ x_2
  \end{bmatrix*}
  =
  \begin{bmatrix*}
    1-\e \\ 1/\e - 1
  \end{bmatrix*}
  \quad\Longrightarrow\quad
  \left\{\,
  \begin{aligned}
    x_2 & = 1, \\
    x_1 &= \frac{(1-\e) - 1}{-\e}.
  \end{aligned}
  \right.
\end{equation*}

 - In exact arithmetic, this yields the correct solution.
 - In floating-point arithmetic, calculation of $x_1$ suffers from catastrophic cancellation.
#+LATEX: \endgroup

**** Why Is Pivoting Necessary? {{{cont}}}
***** Example
#+LATEX: \begingroup \small
  Given $\e \ll 1$, solve the system
  \begin{equation*}
    \begin{bmatrix}[rr]
      -\e & 1 \\ 1 & -1
    \end{bmatrix}
    \begin{bmatrix}
      x_1 \\ x_2
    \end{bmatrix}
    =
    \begin{bmatrix}
      1-\e \\ 0
    \end{bmatrix}
  \end{equation*}
  using Gaussian elimination _with_ and without _partial pivoting_.
#+LATEX: \endgroup

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
#+LATEX: \begingroup\footnotesize
*With partial pivoting:* First, swap the rows $R_1 \leftrightarrow R_2$, and then do $R_2 \rightarrow R_2 + \e R_1$ to obtain
\begin{equation*}
  \begin{bmatrix*}
    1 & -1 \\ 0 & 1-\e
  \end{bmatrix*}
  \begin{bmatrix*}
    x_1 \\ x_2
  \end{bmatrix*}
  =
  \begin{bmatrix*}
    0 \\ 1 - \e
  \end{bmatrix*}
  \quad\Longrightarrow\quad
  \left\{\,
  \begin{aligned}
    x_2 & = 1, \\
    x_1 &= \frac{0 - (-1)}{1}.
  \end{aligned}
  \right.
\end{equation*}
 - Each of the arithmetic steps (to compute $x_1$, $x_2$) is well-conditioned.
 - The solution is computed stably.
#+LATEX: \endgroup

*** PLU Factorization
**** Emulation of Row Interchange
We already know that a row replacement of the form
\[
R_i \rightarrow R_i + c R_j
\]
is carried out by left-multiplication by an elementary matrix
\[
(I + c \be_i \be_j\tp) A.
\]
So the only remaining operation to consider is a row interchange
\[
R_i \leftrightarrow R_j,
\]
which, in terms of matrix multiplication, is done by
\[
P(i,j) A,
\]
where $P(i,j)$ is an /elementary permutation matrix/.

**** Elementary Permutation Matrices
***** Elementary Permutation Matrix                                           :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
For $i, j \in \NN[1,n]$ distinct, denote by $P(i,j)$ the $n \times n$ matrix obtained by interchanging the $i\text{th}$ and $j\text{th}$ rows of the $n\times n$ identity matrix. Such matrices are called /elementary permutation matrices/.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
*Example.* ($n = 4$)
\begin{equation*}
  P(1,2) =
  \begin{bmatrix}
    0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1
  \end{bmatrix},
  \quad
  P(1,3) =
  \begin{bmatrix}
    0 & 0 & 1 & 0 \\
    0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1
  \end{bmatrix},
  \quad \cdots
\end{equation*}

*Notable Properties.*
#+LATEX: \begin{multicols}{2}
 - $P(i,j)=P(j,i)$
 - $P(i,j)^2 = I$
#+LATEX: \end{multicols}

**** Row or Column Interchange
Elementary permutation matrices are useful in interchanging rows or columns.
\vs
#+ATTR_LATEX: :align c|c|c
| Operation                                                             | Mathematics | MATLAB                        |
|-----------------------------------------------------------------------+-------------+-------------------------------|
| $\balpha_i\tp \leftrightarrow \balpha_j\tp$ \rule[-0.5em]{0pt}{1.8em} | $P(i,j) A$  | ~A([i,j],:)=A([j,i],:)~       |
| $\ba_i \leftrightarrow \ba_j$                                         | $A P(i,j)$  | ~A(:,[i,j])=A(:,[j,i])~       |

**** Permutation Matrices
***** Permutation Matrix                                                      :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
A /permutation matrix/ $P \in \RR^{n \times n}$ is a square matrix obtained from the same-sized identity matrix by re-ordering of rows.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs

*Notable Properties.*
 - $P\tp = P^{-1}$
 - A product of /elementary permutation matrices/ is a permutation matrix.
\vs
*Row and Column Operations.* For any $A \in \RR^{n \times n}$,
 - $PA$ permutes the rows of $A$.
 - $AP$ permutes the columns of $A$.

**** Key Example Revisited
:PROPERTIES:
:BEAMER_opt: shrink=5
:END:
#+LATEX: \begingroup\footnotesize
Let's work out the key example from last time once again, now in matrix form $A\bx = \bb$.

#+LATEX: \begingroup\footnotesize
\begin{equation*}
  \begin{bmatrix*}[r]
    2  & 2  & 1 \\ -4 & 6  & 1 \\ 5 & -5 & 3
  \end{bmatrix*}
  \begin{bmatrix*}
    x_1 \\ x_2 \\ x_3
  \end{bmatrix*} =
  \begin{bmatrix*}[r]
    6 \\ -8 \\ 4
  \end{bmatrix*} \,.
\end{equation*}
#+LATEX: \endgroup
\vfill

\red{\bf [Pivot]} Switch $R_1$ and $R_3$ using $P(1,3)$:
#+LATEX: \begingroup\footnotesize
\begin{equation*}
  \underbrace{\begin{bmatrix*}[r]
      0 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 0
    \end{bmatrix*}}_{P(1,3)}
  {\color{black!70}\left[\rule{0pt}{20pt}
      {\normalcolor
        \begin{bmatrix*}[r]
          2  & 2  & 1 \\ -4 & 6  & 1 \\ 5 & -5 & 3
        \end{bmatrix*}
        \begin{bmatrix*}
          x_1 \\ x_2 \\ x_3
        \end{bmatrix*} =
        \begin{bmatrix*}[r]
          6 \\ -8 \\ 4
        \end{bmatrix*} }
    \right]}
  \longrightarrow
  \begin{bmatrix*}[r]
    5 & -5 & 3 \\ -4 & 6 & 1 \\ 2 & 2 & 1
  \end{bmatrix*}
  \begin{bmatrix*}
    x_1 \\ x_2 \\ x_3
  \end{bmatrix*} =
  \begin{bmatrix*}[r]
    4 \\ -8 \\ 6
  \end{bmatrix*}
\end{equation*}
#+LATEX: \endgroup
\vfill

\red{\bf [Zero]} Do row operations $R_2 \rightarrow R_2 + (4/5)R_1$ and $R_3 \rightarrow R_3 - (2/5)R_1$:
#+LATEX: \begingroup\footnotesize
\begin{equation*}
  \underbrace{
    \begin{bmatrix*}[r]
      1 & 0 & 0 \\ 4/5 & 1 & 0 \\ -2/5 & 0 & 1
    \end{bmatrix*}
  }_{G_1}
  {\color{black!70} \left[\rule{0pt}{20pt}
      {\normalcolor
        \begin{bmatrix*}[r]
          5 & -5 & 3 \\ -4 & 6 & 1 \\ 2 & 2 & 1
        \end{bmatrix*}
        \begin{bmatrix*}
          x_1 \\ x_2 \\ x_3
        \end{bmatrix*} =
        \begin{bmatrix*}[r]
          4 \\ -8 \\ 6
        \end{bmatrix*}}
    \right]}
  \longrightarrow
  \begin{bmatrix*}[r]
    5 & -5 & 3 \\ 0 & 2 & 3.4 \\ 0 & 4 & -0.2
  \end{bmatrix*}
  \begin{bmatrix*}
    x_1 \\ x_2 \\ x_3
  \end{bmatrix*} =
  \begin{bmatrix*}[r]
    4 \\ -4.8 \\ 4.4
  \end{bmatrix*}
\end{equation*}
#+LATEX: \endgroup
#+LATEX: \endgroup

**** Key Example Revisited {{{cont}}}
:PROPERTIES:
:BEAMER_opt: shrink=5
:END:
#+LATEX: \begingroup\footnotesize
\red{\bf [Pivot]} Switch $R_2$ and $R_3$ using $P(2,3)$:

#+LATEX: \begingroup\footnotesize
\begin{equation*}
  \underbrace{\begin{bmatrix*}[r]
      1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0
    \end{bmatrix*}}_{P(2,3)}
  {\color{black!70}\left[\rule{0pt}{20pt}
      {\normalcolor
        \begin{bmatrix*}[r]
          5 & -5 & 3 \\ 0 & 2 & 3.4 \\ 0 & 4 & -0.2
        \end{bmatrix*}
        \begin{bmatrix*}
          x_1 \\ x_2 \\ x_3
        \end{bmatrix*} =
        \begin{bmatrix*}[r]
          4 \\ -4.8 \\ 4.4
        \end{bmatrix*}}
  \right]}
  \longrightarrow
  \begin{bmatrix*}[r]
    5 & -5 & 3 \\ 0 & 4 & -0.2 \\ 0 & 2 & 3.4
  \end{bmatrix*}
  \begin{bmatrix*}
    x_1 \\ x_2 \\ x_3
  \end{bmatrix*} =
  \begin{bmatrix*}[r]
    4 \\ 4.4 \\ -4.8
  \end{bmatrix*}
\end{equation*}
#+LATEX: \endgroup
\vfill

\red{\bf [Zero]} Do a row operation $R_3 \rightarrow R_3 - (1/2)R_2$:
#+LATEX: \begingroup\footnotesize
\begin{equation*}
  \underbrace{
    \begin{bmatrix*}[r]
      1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & -1/2 & 1
    \end{bmatrix*}
  }_{G_2}
  {\color{black!70} \left[\rule{0pt}{20pt}
      {\normalcolor
        \begin{bmatrix*}[r]
          5 & -5 & 3 \\ 0 & 4 & -0.2 \\ 0 & 2 & 3.4
        \end{bmatrix*}
        \begin{bmatrix*}
          x_1 \\ x_2 \\ x_3
        \end{bmatrix*} =
        \begin{bmatrix*}[r]
          4 \\ 4.4 \\ -4.8
        \end{bmatrix*}}
    \right]}
  \longrightarrow
  \underbrace{
    \begin{bmatrix*}[r]
      5 & -5 & 3 \\ 0 & 4 & -0.2 \\ 0 & 0 & 3.5
    \end{bmatrix*}
  }_{U}
  \begin{bmatrix*}
    x_1 \\ x_2 \\ x_3
  \end{bmatrix*} =
  \begin{bmatrix*}[r]
    4 \\ 4.4 \\ -7
  \end{bmatrix*}
\end{equation*}
#+LATEX: \endgroup
#+LATEX: \endgroup

**** Analysis of Example
 - The previous calculations can be summarized as
   \[
     G_2 P(2,3) G_1 P(1,3) A = U \,.
     \tag{$\star$}
   \]
 - Using the noted properties of permutation matrices and GTMs, ($\star$) can be written as
   \begin{multline*}
     G_2 P(2,3) G_1 \underbrace{P(2,3) P(2,3)}_{=I} P(1,3) A = U \\
     \longrightarrow G_2 \underbrace{P(2,3) G_1 P(2,3)}_{=:\widetilde{G}_1}
     \underbrace{P(2,3)P(1,3)}_{=:P} A = U \,.
   \end{multline*}
 - The above can be summarized as $P A = L U$ where $L = (G_2 \widetilde{G}_1)^{-1}$ is a lower triangular matrix.

**** Generalization: PLU Factorization
For an arbitrary matrix $A \in \RR^{n \times n}$, the partial pivoting and row operations are intermixed as
\[
  G_{n-1}P(n-1, r_{n-1}) \cdots G_2 P(2, r_2) G_1 P(1, r_1) A = U \,.
\]
Going through the same calculations as above, it can always be written as
\[
  \left( \widetilde{G}_{n-1} \cdots \widetilde{G}_2 \widetilde{G}_1 \right)
  P(n-1, r_{n-1}) \cdots P(2, r_2) P(1, r_1) A = U \,,
\]
which again leads to $PA = LU$:
\[
  \underbrace{P(n-1, r_{n-1}) \cdots P(2, r_2) P(1, r_1)}_{=:P}
  A =
  \underbrace{
    \left( \widetilde{G}_{n-1} \cdots \widetilde{G}_2 \widetilde{G}_1 \right)^{-1}
  }_{=:L} U \,.
\]
This is called the *PLU factorization* of matrix $A$.

**** Implementation of PLU Factorization
*Exercise.* Write a MATLAB function =myplu= for PLU factorization by modifying =mylu.m= from previous lecture.
\vs
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
 function [L,U,P] = myplu(A)
 % MYPLU   PLU factorization (demo only--not stable!).
 % Input:
 %   A    square matrix
 % Output:
 %   P,L,U  permutation, unit lower triangular, and upper triangular such that LU=PA

 % Your code here.




 end
#+END_SRC

**** =myplu.m= (Solution)                                                          :noexport:

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
function [L,U,P] = myplu(A)
% MYPLU   PLU factorization (demo only--not stable!).
% Input:
%   A    square matrix
% Output:
%   P,L,U  permutation, unit lower triangular, and upper triangular such that LU=PA

    n = length(A);
    P = eye(n);   % preallocate P
    L = eye(n);   % preallocate L
                  % A will be overwritten below to be U
    for j = 1:n-1
        %% Pivoting
        [~, iM] = max(abs(A(j:n, j)));
        iM = iM + j - 1;                 % adjustment
        if j ~= iM
            piv = [j, iM];               % pivot vector (for convenience)
            P(piv, :) = P(flip(piv), :); % update permutation matrix
            A(piv, :) = A(flip(piv), :); % update A
            L(:, piv) = L(:, flip(piv)); % update L by emulating P*G*P
            L(piv, :) = L(flip(piv), :); %    where P is an elem. perm. mat.
        end
        %% Introducing Zeros below Diagonal
        % NOTE: this section is identical to the corresponding
        % block of `mylu' code, which means that the for-loop
        % below can be avoided.
        for i = j+1:n
            L(i,j) = A(i,j) / A(j,j);    % row multiplier
            A(i,j:n) = A(i,j:n) - L(i,j)*A(j,j:n);
        end
    end
    U = triu(A);  % to ensure that U come out as a clean upper-trian. mat.
end
#+END_SRC

**** Solving a Square System Using PLU Factorization
Multiplying $A\bx = \bb$ on the left by $P$ we obtain
\[
  \underbrace{PA}_{=LU} \bx = \underbrace{P\bb}_{=: \bm{\beta}} \quad\longrightarrow\quad LU \bx = \bm{\beta} \,,
\]
which can be solved in two steps: \vs

 - Define $U\bx = \by$ and solve for $\by$ in the equation
   \[
     L\by = \bm{\beta} \,. \tag{forward elimination}
   \]
 - Having calculated $\by$, solve for $\bx$ in the equation
   \[
     U\bx = \by \,. \tag{backward substitution}
   \]

**** Solving a Square System Using PLU Factorization {{{cont}}}
 - Using the instructional codes ( =backsub=, =forelim=, =myplu= ):
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 [L,U,P] = myplu(A);
 x = backsub( U, forelim(L, P*b) );
   #+END_SRC
 - Using MATLAB's built-in functions:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 [L,U,P] = lu(A);
 x = U \ (L \ (P*b));
   #+END_SRC
   - The backslash is designed so that triangular systems are solved with the appropriate substitution.
 - The most compact way:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 x = A \ b;
   #+END_SRC
   - The backslash does partial pivoting and triangular substitutions silently and automatically.

*** Cost of PLU Factorization Algorithm
**** Notation: Big-O and Asymptotic
Let $f, g$ be positive functions defined on $\NN$.

 - $f(n) = O\left( g(n) \right)$ @@latex:{\small\color{black!80}@@ (``$f$ is /big-O/ of $g$'') @@latex:}@@ as $n \to \infty$  if
   \[
     \frac{f(n)}{g(n)} \le C,\quad \text{for all sufficiently large $n$.}
   \]
 - $f(n) \sim g(n)$ @@latex:{\small\color{black!80}@@ (``$f$ is /asymptotic/ to $g$'') @@latex:}@@ as  $n \to \infty$ if
   \[
     \lim_{n \to \infty} \frac{f(n)}{g(n)} = 1.
   \]

**** FLOP Counting: Timing Vector/Matrix Operations
 - One way to measure the ``efficiency'' of a numerical algorithm is to count the number of floating-point arithmetic operations (FLOPS) necessary for its execution.
 - The number is usually represented by $\sim c n^p$ where $c$ and $p$ are given explicitly.
 - We are interested in this formula when $n$ is large.

**** FLOPS for Major Operations
***** Vector/Matrix Operations
Let $x,y\in\RR^n$ and $A,B \in \RR^{n \times n}$. Then

 - (vector-vector) $x^{\rm T}y$ requires $\sim 2n$ \flops.
 - (matrix-vector) $Ax$ requires $\sim 2n^2$ \flops.
 - (matrix-matrix) $AB$ requires $\sim 2n^3$ \flops.

**** Cost of Elimination Steps
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:

Note that we only need to count the number of \flops{} required to zero out elements below the diagonal of each column. The following is the relevant fragment from =mylu= function.

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
for j = 1:n-1
  for i = j+1:n
    L(i,j) = A(i,j) / A(j,j);   % row multiplier
    A(i,j:n) = A(i,j:n) - L(i,j)*A(j,j:n);
  end
end
#+END_SRC

\vs
 - For each $i > j$, we replace $R_i$ by $R_i - c R_j$ where $c = a_{i,j}/a_{j,j} = L_{i,j}$. This requires approximately $2(n-j+1)$ \flops:
   - 1 division to form =L(i,j)=.
   - $n-j+1$ multiplications and $n-j+1$ subtractions to form =A(i,j:n)=
 - Since $i \in \NN[j+1, n]$, the total number of \flops{} needed to zero out all elements below the diagonal in the $j\text{th}$ column is $2(n-j+1)(n-j)$.
 - Summing up over $j \in \NN[1,n-1]$, we need about $(2/3)n^3$ \flops:
   \begin{equation*}
     \sum_{j=1}^{n-1} 2(n-j+1)(n-j)
     \sim 2 \sum_{j=1}^{n-1} (n-j)^2
     = 2 \sum_{j=1}^{n-1} j^2 \sim \color{osured}{\frac{2}{3}n^3}
   \end{equation*}

**** Cost of Forward Elimination and Backward Substitution
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:

_*Backward Substitution*_. The cost of backward substitution is approximately $\color{osured}{n^2}$ \flops:

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
for i = n:-1:1
    x(i) = ( b(i) - U(i,i+1:n)*x(i+1:n) ) / U(i,i);
end
#+END_SRC

 - The calculation of =x(i)= for $i = n, n-1, \ldots, 1$ requires approximately $2(n-i)$ \flops:
   - 1 subtraction;
   - $n-i$ multiplications;
   - $n-i$ subtractions;
   - 1 division.
 - Summing over all $i \in \NN[1,n]$, we need about $n^2$ \flops:
   \[
   \sum_{i=1}^{n} 2(n-i) = 2\sum_{i=0}^{n-1} i \sim 2 \frac{n^2}{2} = \color{osured}{n^2}.
   \]

_*Forward Elimination*_. The cost of backward substitution is also approximately $\color{osured}{n^2}$ \flops, which can be shown in a similar fashion.

**** Cost of PLU Factorization                                                     :noexport:
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:

Note that we only need to count the number of \flops{} required to zero out elements below the diagonal of each column. The following is the relevant fragment from =mylu= function.

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
   for j = 1:n-1
     for i = j+1:n
       L(i,j) = A(i,j) / A(j,j);   % row multiplier
       A(i,j:n) = A(i,j:n) - L(i,j)*A(j,j:n);
     end
   end
#+END_SRC

\vs
 - For each $i>j$, we replace $R_i$ by $R_i - c R_j$ where $c = a_{i,j}/a_{j,j} = L_{i,j}$. This requires approximately $2(n-j+1)$ \flops:
   - 1 division to form $c$
   - $n-j+1$ multiplications to form $c R_j$
   - $n-j+1$ additions to form $R_i + c R_j$
 - Since $i \in \NN[j+1, n]$, the total number of \flops{} needed to zero out all elements below the diagonal in the $j\text{th}$ column is approximately $2(n-j+1)(n-j)$.
 - Summing up over $j \in \NN[1,n-1]$, we need about $(2/3)n^3$ \flops:
   \begin{equation*}
     \sum_{j=1}^{n-1} 2(n-j+1)(n-j)
     \sim 2 \sum_{j=1}^{n-1} (n-j)^2
     = 2 \sum_{j=1}^{n-1} j^2 \sim \color{osured}{\frac{2}{3}n^3}
   \end{equation*}

**** Cost of Forward Elimination and Backward Substitution                         :noexport:

_*Forward Elimination*_
 - The calculation of $y_i = \beta_i - \sum_{j=1}^{i-1} \ell_{ij}y_j$ for $i >1$ requires approximately $2i$ \flops:
   - 1 subtraction
   - $i-1$ multiplications
   - $i-2$ additions
 - Summing over all $i \in \NN[2,n]$, we need about $n^2$ \flops:
   \[
     \sum_{i=2}^{n} 2i \sim 2 \frac{n^2}{2} = \color{osured}{n^2} \,.
   \]

_*Backward Substitution*_
 - The cost of backward substitution is also approximately $\color{osured}{n^2}$ \flops, which can be shown in the same manner.

**** Total Cost of G.E. with Partial Pivoting
  Gaussian elimination with partial pivoting involves three steps:

   - PLU factorization: $\ds \sim (2/3) n^3$ \flops
   - Forward elimination: $\ds \sim n^2$ \flops
   - Backward substitution: $\ds \sim n^2$ \flops

\vfill

***** Summary
The total cost of Gaussian elimination with partial pivoting is approximately
\[
  \frac{2}{3}n^3 + n^2 + n^2 \sim \frac{2}{3}n^3
\]
\flops{} for large $n$.

**** Application: Solving Multiple Square Systems Simultaneously
To solve two systems $A\bx_1 = \bb_1$ and $A\bx_2 = \bb_2$.
\vs

***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
******                                                                              :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
*Method 1.*
 - Use G.E. for both.
 - It takes $\sim (4/3) n^3$ \flops.

\vs

*Method 2.*
 - Do it in two steps:
   1. Do PLU factorization $PA = LU$.
   2. Then solve $LU\bx_1 = P\bb_1$ and $LU\bx_2 = P\bb_2$.
 - It takes $\sim (2/3)n^3$ \flops.
******                                                                              :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.43
:END:
#+LATEX: \vspace{-1.5em}
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
%% method 1
 x1 = A \ b1;
 x2 = A \ b2;
#+END_SRC

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
%% method 2
 [L, U, P] = lu(A);
 x1 = U \ (L \ (P*b1));
 x2 = U \ (L \ (P*b2));
#+END_SRC

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
%% compact implementation
 X = A \ [b1, b2];
 x1 = X(:, 1);
 x2 = X(:, 2);
#+END_SRC

** DONE Conditioning of Square Linear Systems
:PROPERTIES:
:EXPORT_FILE_NAME: 14-conditioning-of-linear-systems
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Vector and Matrix Norms
**** Vector Norms
The ``length'' of a vector $\mathbf{v}$ can be measured by its *norm*.
***** $p\text{-Norm}$ of a Vector                                             :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
Let $p \in [1, \infty)$. The $p\text{-norm}$ of $\mathbf{v} \in \RR^m$ is denoted by $\Norm{\mathbf{v}}{p}$ and is defined by
\[
  \Norm{\mathbf{v}}{p}
  = {\biggl( \sum_{i=1}^{m} \abs{v_i}^p \biggr)}^{1/p} \,.
\]
When $p = \infty$,
\[
  \Norm{\mathbf{v}}{\infty} =\max_{1\le i \le m} \abs{v_i} \,.
\]
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
The most commonly used $p$ values are 1, 2, and $\infty$:
\begin{equation*}
  \Norm{\mathbf{v}}{1} = \sum_{i=1}^{m} \abs{v_i},
  \quad
  \Norm{\mathbf{v}}{2}  = \sqrt{\sum_{i=1}^{m} \abs{v_i}^2}.
\end{equation*}

**** Vector Norms
In general, any function $\Norm{\, \cdot \,}{}:\RR^m \to \RR^{+} \cup \{0\}$ is called a *vector norm* if it satisfies the following three properties:
\vs

 1. $\Norm{\mathbf{x}}{} = 0$ if and only if $\mathbf{x} = 0$.
 2. $\Norm{\alpha \mathbf{x}}{} = \abs{\alpha} \Norm{\mathbf{x}}{}$ for any constant $\alpha$ and any $\mathbf{x} \in \RR^m$.
 3. $\Norm{\mathbf{x} + \mathbf{y}}{} \le \Norm{\mathbf{x}}{} + \Norm{\mathbf{y}}{}$ for any $\mathbf{x}, \mathbf{y} \in \RR^m$. This is called the /triangle inequality/.

**** Unit Vectors
 - A vector $\mathbf{u}$ is called a *unit vector* if $\Norm{\mathbf{u}}{} = 1$.
 - Depending on the norm used, unit vectors will be different.
 - For instance:

\vs
#+BEGIN_EXPORT latex
\begin{minipage}[h]{0.3\linewidth}
  \begin{figure}[h]
    \centering
    \begin{tikzpicture}
      % [scale=0.5]
      \path[draw, fill=gray!25] (1,0) -- (0,1) -- (-1,0) -- (0,-1) -- cycle;
      \draw (-1.5, 0) -- (1.5, 0);
      \draw (0, -1.2) -- (0, 1.2);
    \end{tikzpicture}
    \caption{1-norm}
  \end{figure}
\end{minipage}
\hfill
\begin{minipage}[h]{0.3\linewidth}
  \begin{figure}[h]
    \centering
    \begin{tikzpicture}
      % [scale=0.5]
      \path[draw, fill=gray!25] (0,0) circle (1);
      \draw (-1.5, 0) -- (1.5, 0);
      \draw (0, -1.2) -- (0, 1.2);
    \end{tikzpicture}
    \caption{2-norm}
  \end{figure}
\end{minipage}
\hfill
\begin{minipage}[h]{0.3\linewidth}
  \begin{figure}[h]
    \centering
    \centering
    \begin{tikzpicture}
      % [scale=0.5]
      \path[draw, fill=gray!25] (1,1) -- (-1,1) -- (-1,-1) -- (1,-1) -- cycle;
      \draw (-1.5, 0) -- (1.5, 0);
      \draw (0, -1.2) -- (0, 1.2);
    \end{tikzpicture}
    \caption{$\infty$-norm}
  \end{figure}
\end{minipage}
#+END_EXPORT

**** Matrix Norms
The ``size'' of a matrix $A \in \RR^{m \times n}$ can be measured by its *norm* as well. As above, we say that a function $\Norm{\, \cdot \,}{}:\RR^{m \times n} \to \RR^{+} \cup \{0\}$ is a *matrix norm* if it satisfies the following three properties:

\vs

 1. $\Norm{A}{} = 0$ if and only if $A = 0$.
 2. $\Norm{\alpha A}{} = \abs{\alpha} \Norm{A}{}$ for any constant $\alpha$ and any $A \in \RR^{m \times n}$.
 3. $\Norm{A + B}{} \le \Norm{A}{} + \Norm{B}{}$ for any $A, B \in \RR^{m \times n}$. This is called the /triangle inequality/.

**** Matrix Norms {{{cont}}}
 - If, in addition to satisfying the three conditions, it satisfies
   \[
     \Norm{AB}{} \le \Norm{A}{}\Norm{B}{}
     \quad\text{for all $A \in \RR^{m \times n}$ and all $B \in \RR^{n \times p}$,}
   \]
   it is said to be *consistent*.
 - If, in addition to satisfying the three conditions, it satisfies
   \[
     \Norm{A\mathbf{x}}{} \le \Norm{A}{} \Norm{\mathbf{x}}{}
     \quad\text{for all $A \in \RR^{m \times n}$ and all $\mathbf{x} \in \RR^{n}$,}
   \]
   then we say that it is *compatible* with a vector norm.

**** Induced Matrix Norms

***** $p\text{-Norm}$ of a Matrix                                             :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
Let $p \in [1,\infty]$. The $p\text{-norm}$ of $A \in \RR^{m \times n}$ is given by
\[
  \Norm{A}{p}
  = \max_{\mathbf{x} \neq 0} \frac{ \Norm{A \mathbf{x}}{p} }{ \Norm{\mathbf{x}}{p} }
  = \max_{\Norm{\mathbf{x}}{p} = 1} \Norm{A \mathbf{x}}{p}\,.
\]

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - The definition of this particular matrix norm is *induced* from the vector $p\text{-norm}$.
 - By construction, matrix $p\text{-norm}$ is a compatible norm.
 - Induced norms describe how the matrix stretches unit vectors with respect to the vector norm.

**** Induced Matrix Norms
The commonly used $p\text{-norms}$ (for $p =1, 2, \infty$) can also be calculated by
\begin{align*}
  & \Norm{A}{1} = \max_{1 \le j \le n} \sum_{i=1}^{m} \abs{a_{ij}}, \\
  & \Norm{A}{2} = \sqrt{ \lambda_{\rm max}(A\tp A)} = \sigma_{\rm max}(A), \\
  & \Norm{A}{\infty} = \max_{1 \le i \le m} \sum_{j=1}^{n} \abs{a_{ij}}.
\end{align*}

In words,
 - The 1-norm of $A$ is the maximum of the 1-norms of all column vectors.
 - The 2-norm of $A$ is the square root of the largest eigenvalue of $A\tp A$.
 - The $\infty\text{-norm}$ of $A$ is the maximum of the 1-norms of all row vectors.

**** Non-Induced Matrix Norm -- Frobenius Norm

***** Frobenius Norm of a Matrix                                              :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
The Frobenius norm of $A \in \RR^{m \times n}$ is given by
\[
  \Norm{A}{F} = {\biggl( \sum_{i=1}^{m} \sum_{j=1}^{n}
    {\abs{a_{ij}}}^{2} \biggr)}^{1/2}.
\]

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - This is not induced from a vector $p\text{-norm}$.
 - However, both $p\text{-norm}$ and the Frobenius norm are consistent and compatible.
 - For compatibility of the Frobenius norm, the vector norm must be the 2-norm, that is, $\Norm{A\mathbf{x}}{2} \le \Norm{A}{F} \Norm{\mathbf{x}}{2}$.

**** Norms in MATLAB
 - Vector $p\text{-norms}$ can be easily computed:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 norm(v, 1)      % = sum(abs(v))
 norm(v, 2)      % = sqrt(v'*v)  if v is a column
 norm(v, 'inf')  % = max(abs(v))
   #+END_SRC
 - The same function =norm= is used to calculate matrix $p\text{-norms}$:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 norm(A, 1)       % = max(sum(abs(A), 1))
 norm(A, 2)       % = max(sqrt(eig(A'*A)))
 norm(A, Inf)     % = max(sum(abs(A), 2))
   #+END_SRC
 - To calculate the Frobenius norm:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 norm(A, 'fro')   % = sqrt(A(:)'*A(:))
                  % = norm(A(:), 2)
   #+END_SRC
*** Conditioning
**** Conditioning of Solving Linear Systems: Overview

 - Analyze how robust (or sensitive) the solutions of $A\bx = \bb$ are to perturbations of $A$ and $\bb$.

 - For simplicity, consider separately the cases where
   1. $\bb$ changes to $\bb + \delta \bb$, while $A$ remains unchanged, that is
      \[
        A\bx = \bb \quad \longrightarrow \quad
        A(\bx + \delta \bx) = \bb + \delta \bb.
      \]
   2. $A$ changes to $A + \delta A$, while $\bb$ remains unchanged, that is
      \[
        A\bx = \bb \quad \longrightarrow \quad
        (A + \delta A) (\bx + \delta\bx) = \bb.
      \]

**** Sensitivity to Perturbation of RHS
*Case 1.* $A\bx = \bb \; \rightarrow \; A(\bx + \delta \bx) = \bb + \delta \bb$
\vs
 - Bound $\norm{\delta \bx}$ in terms of $\norm{\delta \bb}$:
   \begin{equation*}
     \begin{aligned}[c]
       A\bx + A\delta\bx &= \bb + \delta\bb \\
       A \delta \bx & = \delta \bb \\
       \delta \bx & = A^{-1} \delta \bb
     \end{aligned}
     \quad\Longrightarrow\quad
     \norm{\delta \bx} \le \norm[1]{A^{-1}} \norm[1]{\delta \bb}.
   \end{equation*}
 - Sensitivity in terms of relative errors:
   \begin{equation*}
     \frac{ \dfrac{\norm{\delta\bx}}{\norm{\bx}} }{ \dfrac{ \norm{\delta\bb}}{\norm{\bb}} }
     = \frac{\norm{\delta\bx}\norm{\bb}}{\norm{\delta\bb}\norm{\bx}}
     \le \frac{ \norm[1]{A^{-1}} \norm{\delta\bb} \cdot \norm[1]{A} \norm{\bx} }{\norm{\delta\bb}\norm{\bx}} = \norm[1]{A^{-1}} \norm[1]{A}.
   \end{equation*}

**** Sensitivity to Perturbation of Matrix
*Case 2.* $A\bx = \bb \; \rightarrow \; (A + \delta A) (\bx + \delta\bx) = \bb$
\vs
 - Bound $\norm{\delta \bx}$ now in terms of $\norm{\delta A}$:
   \begin{equation*}
     \begin{aligned}[c]
       A\bx & + A\delta\bx + (\delta A) \bx + (\delta A) \delta \bx = \bb \\
       A\delta\bx & = - (\delta A) \bx - (\delta A) \delta \bx \\
       \delta \bx & = - A^{-1} (\delta A) \bx - A^{-1} (\delta A) \delta \bx
     \end{aligned}
     \quad\Longrightarrow\quad
     \begin{aligned}[c]
       & \norm{\delta \bx} \lesssim \norm[1]{A^{-1}} \norm[1]{\delta A} \norm{\bx}.\\
       & \text{(first-order truncation)}
     \end{aligned}
   \end{equation*}
 - Sensitivity in terms of relative errors:
   \begin{equation*}
     \frac{ \dfrac{\norm{\delta\bx}}{\norm{\bx}} }{ \dfrac{ \norm[1]{\delta A}}{\norm[1]{A}} }
     = \frac{\norm{\delta\bx}\norm[1]{A}}{\norm[1]{\delta A}\norm{\bx}}
     \lesssim \frac{ \norm[1]{A^{-1}} \norm[1]{\delta A} \norm{\bx} \cdot \norm[1]{A} }{\norm[1]{\delta A}\norm{\bx}} = \norm[1]{A^{-1}} \norm[1]{A}.
   \end{equation*}

**** Matrix Condition Number
 - Motivated by the previous estimations, we define the *matrix condition number* by
   \[
     \kappa(A) = \norm[1]{A^{-1}}\norm[1]{A},
   \]
   where the norms can be any $p\text{-norm}$ or the Frobenius norm.
 - A subscript on $\kappa$ such as 1, 2, $\infty$, or F(robenius) is used if clarification is needed.

**** Matrix Condition Number {{{cont}}}
 - We can write
   \begin{equation*}
     \frac{\norm{\delta \bx}}{\norm{\bx}} \le \kappa(A) \frac{\norm{\delta \bb}}{\norm{\bb}},
     \quad
     \frac{\norm{\delta \bx}}{\norm{\bx}} \le \kappa(A) \frac{\norm[1]{\delta A}}{\norm[1]{A}},
   \end{equation*}
   where the second inequality is true only in the limit of infinitesimal perturbations $\delta A$.
 - The matrix condition number $\kappa(A)$ is equal to the condition number of solving a linear system of equation $A\bx = \bb$.
 - The exponent of $\kappa(A)$ in scientific notation determines the approximate number of digits of accuracy that will be lost in calculation of $\bx$.
 - Since $1 = \norm[1]{I} = \norm[1]{A^{-1}A} \le \norm[1]{A^{-1}} \norm[1]{A} = \kappa(A)$, a condition number of 1 is the best we can hope for.
 - If $\kappa(A) > \meps^{-1}$, then for computational purposes the matrix is singular.

**** Condition Numbers in MATLAB
 - Use =cond= to calculate various condition numbers:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 cond(A)          % the 2-norm; or  cond(A, 2)
 cond(A, 1)       % the 1-norm
 cond(A, Inf)     % the infinity-norm
 cond(A, 'fro')   % the Frobenius norm
   #+END_SRC
 - A condition number estimator (in 1-norm)
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 condest(A)       % faster than cond
   #+END_SRC
 - The fastest method to estimate the condition number is to use =linsolve= function as below:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 [x, inv_condest] = linsolve(A, b);
 fast_condest = 1/inv_condest;
   #+END_SRC

** TODO PLU Code -- Outer Product Form
#+BEGIN_SRC matlab
function [L,U] = lufact(A)
  n = length(A);
  L = zeros(n);
  U = zeros(n);
  for k = 1:n
    U(k,:) = A(k,:);
    L(:,k) = A(:,k)/U(k,k);
    A = A - L(:,k)*U(k,:);
  end
end
#+END_SRC

Re-written so that only necessary operations are done.
#+BEGIN_SRC matlab
function [L,U] = lufact(A)
  n = length(A);
  L = eye(n);
  U = zeros(n);
  for k = 1:n-1
    U(k,k:n) = A(k,k:n);
    L(k+1:n,k) = A(k+1:n,k)/U(k,k);
    A(k+1:n,k+1:n) = A(k+1:n,k+1:n) - L(k+1:n,k)*U(k,k+1:n);
  end
  U(n,n) = A(n,n);
end
#+END_SRC

#+BEGIN_SRC matlab
function [L,U,p,s] = plufact(A)
    n = length(A);
    L = zeros(n);
    U = zeros(n);
    p = zeros(1,n);
    s = 0;
    for k = 1:n
        [~,i] = max(abs(A(k:n,k)));
        if i ~= k                       % check this
            s = s + 1;
        end
        p(k) = i;
        U(k,:) = A(p(k),:);
        L(:,k) = A(:,k)/U(k,k);
        A = A - L(:,k)*U(k,:);
    end
end
#+END_SRC

** TODO Notes on (P)LU Factorization
In the future, spend one lecture on these topics.
*** Applications of PLU Factorization
**** Solving a Square System
Multiplying $A\bx = \bb$ on the left by $P$ we obtain
\[
  \underbrace{PA}_{=LU} \bx = \underbrace{P\bb}_{=: \bm{\beta}} \quad\longrightarrow\quad LU \bx = \bm{\beta} \,,
\]
which can be solved in two steps: \vs

 - Define $U\bx = \by$ and solve for $\by$ in the equation
   \[
     L\by = \bm{\beta} \,. \tag{forward elimination}
   \]
 - Having calculated $\by$, solve for $\bx$ in the equation
   \[
     U\bx = \by \,. \tag{backward substitution}
   \]

**** Solving a Square System {{{cont}}}
 - Using the instructional codes ( =backsub=, =forelim=, =myplu= ):
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 [L,U,P] = myplu(A);
 x = backsub( U, forelim(L, P*b) );
   #+END_SRC
 - Using MATLAB's built-in functions:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 [L,U,P] = lu(A);
 x = U \ (L \ (P*b));
   #+END_SRC
   - The backslash is designed so that triangular systems are solved with the appropriate substitution.
 - The most compact way:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 x = A \ b;
   #+END_SRC
   - The backslash does partial pivoting and triangular substitutions silently and automatically.

**** Computing Inverses
Observe that
\[
(PA)^{-1} = (LU)^{-1}
\longrightarrow A^{-1} P^{-1} = U^{-1} L^{-1}
\longrightarrow LU A^{-1} = P
\]

So solve $LU \ba_i = \bp_i$ with forward and backward substitution for each column $\bp_i$ of $P$. Then
\begin{equation*}
A^{-1} =
  \begin{bmatrix}[c|c|c|c]
    & & & \\
    & & & \\
    \ba_1 & \ba_2 & \cdots & \ba_n \\
    & & & \\
    & & &
  \end{bmatrix}.
\end{equation*}

**** Computing Determinants
Observe that
\[
\det(A) = \det(P^{-1}LU) = \det(P^{-1}) \det(L) \det(U)
= \frac{\det(L) \det(U)}{\det(P)}.
\]

*Useful facts.*
 - The determinant of a triangular matrix is the product of its diagonal entries. (What are diagonal entries of $L$?)
 - $P$ is a row permutation of the identity matrix (which has determinant 1), and each row swap negates the determinant. So if $s$ is the number of row swaps, then $\det(P) = (-1)^s$.
\vs

It follows that
\[
\det(A) = (-1)^s \prod_{i=1}^n u_{ii}.
\]
*** Cost of PLU Factorization Algorithm
**** Notation: Big-O and Asymptotic
Let $f, g$ be positive functions defined on $\NN$.

 - $f(n) = O\left( g(n) \right)$ @@latex:{\small\color{black!80}@@ (``$f$ is /big-O/ of $g$'') @@latex:}@@ as $n \to \infty$  if
   \[
     \frac{f(n)}{g(n)} \le C,\quad \text{for all sufficiently large $n$.}
   \]
 - $f(n) \sim g(n)$ @@latex:{\small\color{black!80}@@ (``$f$ is /asymptotic/ to $g$'') @@latex:}@@ as  $n \to \infty$ if
   \[
     \lim_{n \to \infty} \frac{f(n)}{g(n)} = 1.
   \]

**** FLOP Counting: Timing Vector/Matrix Operations
 - One way to measure the ``efficiency'' of a numerical algorithm is to count the number of floating-point arithmetic operations (FLOPS) necessary for its execution.
 - The number is usually represented by $\sim c n^p$ where $c$ and $p$ are given explicitly.
 - We are interested in this formula when $n$ is large.

**** FLOPS for Major Operations
***** Vector/Matrix Operations
Let $x,y\in\RR^n$ and $A,B \in \RR^{n \times n}$. Then

 - (vector-vector) $x^{\rm T}y$ requires $\sim 2n$ \flops.
 - (matrix-vector) $Ax$ requires $\sim 2n^2$ \flops.
 - (matrix-matrix) $AB$ requires $\sim 2n^3$ \flops.

**** Cost of Elimination Steps
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:

Note that we only need to count the number of \flops{} required to zero out elements below the diagonal of each column. The following is the relevant fragment from =mylu= function.

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
for j = 1:n-1
  for i = j+1:n
    L(i,j) = A(i,j) / A(j,j);   % row multiplier
    A(i,j:n) = A(i,j:n) - L(i,j)*A(j,j:n);
  end
end
#+END_SRC

\vs
 - For each $i > j$, we replace $R_i$ by $R_i - c R_j$ where $c = a_{i,j}/a_{j,j} = L_{i,j}$. This requires approximately $2(n-j+1)$ \flops:
   - 1 division to form =L(i,j)=.
   - $n-j+1$ multiplications and $n-j+1$ subtractions to form =A(i,j:n)=
 - Since $i \in \NN[j+1, n]$, the total number of \flops{} needed to zero out all elements below the diagonal in the $j\text{th}$ column is $2(n-j+1)(n-j)$.
 - Summing up over $j \in \NN[1,n-1]$, we need about $(2/3)n^3$ \flops:
   \begin{equation*}
     \sum_{j=1}^{n-1} 2(n-j+1)(n-j)
     \sim 2 \sum_{j=1}^{n-1} (n-j)^2
     = 2 \sum_{j=1}^{n-1} j^2 \sim \color{osured}{\frac{2}{3}n^3}
   \end{equation*}

**** Cost of Forward Elimination and Backward Substitution
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:

_*Backward Substitution*_. The cost of backward substitution is also approximately $\color{osured}{n^2}$ \flops:

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
for i = n:-1:1
    x(i) = ( b(i) - U(i,i+1:n)*x(i+1:n) ) / U(i,i);
end
#+END_SRC

 - The calculation of =x(i)= for $i = n, n-1, \ldots, 1$ requires approximately $2(n-i)$ \flops:
   - 1 subtraction;
   - $n-i$ multiplications;
   - $n-i$ subtractions;
   - 1 division.
 - Summing over all $i \in \NN[1,n]$, we need about $n^2$ \flops:
   \[
   \sum_{i=1}^{n} 2(n-i) = 2\sum_{i=0}^{n-1} i \sim 2 \frac{n^2}{2} = \color{osured}{n^2}.
   \]

_*Forward Elimination*_. The cost of backward substitution is also approximately $\color{osured}{n^2}$ \flops, which can be shown in a similar fashion.

**** Cost of PLU Factorization                                                     :noexport:
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:

Note that we only need to count the number of \flops{} required to zero out elements below the diagonal of each column. The following is the relevant fragment from =mylu= function.

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
   for j = 1:n-1
     for i = j+1:n
       L(i,j) = A(i,j) / A(j,j);   % row multiplier
       A(i,j:n) = A(i,j:n) - L(i,j)*A(j,j:n);
     end
   end
#+END_SRC

\vs
 - For each $i>j$, we replace $R_i$ by $R_i - c R_j$ where $c = a_{i,j}/a_{j,j} = L_{i,j}$. This requires approximately $2(n-j+1)$ \flops:
   - 1 division to form $c$
   - $n-j+1$ multiplications to form $c R_j$
   - $n-j+1$ additions to form $R_i + c R_j$
 - Since $i \in \NN[j+1, n]$, the total number of \flops{} needed to zero out all elements below the diagonal in the $j\text{th}$ column is approximately $2(n-j+1)(n-j)$.
 - Summing up over $j \in \NN[1,n-1]$, we need about $(2/3)n^3$ \flops:
   \begin{equation*}
     \sum_{j=1}^{n-1} 2(n-j+1)(n-j)
     \sim 2 \sum_{j=1}^{n-1} (n-j)^2
     = 2 \sum_{j=1}^{n-1} j^2 \sim \color{osured}{\frac{2}{3}n^3}
   \end{equation*}

**** Cost of Forward Elimination and Backward Substitution                         :noexport:

_*Forward Elimination*_
 - The calculation of $y_i = \beta_i - \sum_{j=1}^{i-1} \ell_{ij}y_j$ for $i >1$ requires approximately $2i$ \flops:
   - 1 subtraction
   - $i-1$ multiplications
   - $i-2$ additions
 - Summing over all $i \in \NN[2,n]$, we need about $n^2$ \flops:
   \[
     \sum_{i=2}^{n} 2i \sim 2 \frac{n^2}{2} = \color{osured}{n^2} \,.
   \]

_*Backward Substitution*_
 - The cost of backward substitution is also approximately $\color{osured}{n^2}$ \flops, which can be shown in the same manner.

**** Total Cost of G.E. with Partial Pivoting
  Gaussian elimination with partial pivoting involves three steps:

   - PLU factorization: $\ds \sim (2/3) n^3$ \flops
   - Forward elimination: $\ds \sim n^2$ \flops
   - Backward substitution: $\ds \sim n^2$ \flops

\vfill

***** Summary
The total cost of Gaussian elimination with partial pivoting is approximately
\[
  \frac{2}{3}n^3 + n^2 + n^2 \sim \frac{2}{3}n^3
\]
\flops{} for large $n$.

**** Application: Solving Multiple Square Systems Simultaneously
To solve two systems $A\bx_1 = \bb_1$ and $A\bx_2 = \bb_2$.
\vs

***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
******                                                                              :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
*Method 1.*
 - Use G.E. for both.
 - It takes $\sim (4/3) n^3$ \flops.

\vs

*Method 2.*
 - Do it in two steps:
   1. Do PLU factorization $PA = LU$.
   2. Then solve $LU\bx_1 = P\bb_1$ and $LU\bx_2 = P\bb_2$.
 - It takes $\sim (2/3)n^3$ \flops.
******                                                                              :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.43
:END:
#+LATEX: \vspace{-1.5em}
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
%% method 1
 x1 = A \ b1;
 x2 = A \ b2;
#+END_SRC

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
%% method 2
 [L, U, P] = lu(A);
 x1 = U \ (L \ (P*b1));
 x2 = U \ (L \ (P*b2));
#+END_SRC

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
%% compact implementation
 X = A \ [b1, b2];
 x1 = X(:, 1);
 x2 = X(:, 2);
#+END_SRC

** TODO Exploiting Matrix Structure
#+BEGIN_EXPORT latex
% \section{Symmetric Positive Definite Matrices}

\begin{frame}
  \frametitle{Symmetric Matrices -- LDLT Factorization}
  Let $A \in \RR^{n \times n}$ be \textbf{symmetric}, that is, $A\tp = A$.
  \vs
  \begin{itemize}
  \item The Gaussian elimination process without pivoting on this symmetric matrix yields
    \[
      A = LDL\tp,
    \]
    where $L$ is unit lower triangular and $D$ is diagonal. (LDL$^{\rm T}$ Factorization)
    \vs
  \item This factorization takes $\sim \frac{1}{3} n^3$ \flops.
  \item Row pivoting is needed to keep $LDL\tp$ stable, but it is tedious.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Symmetric Positive Definite Matrices -- Cholesky Factorization}
  Let $A \in \RR^{n \times n}$. \vs
  \begin{itemize}
  \item We say that $A$ is \textbf{positive definite} if the \textit{quadratic form} is positive, \textit{i.e.}, $\bx\tp A \bx > 0$ for all $\bx \neq \bzero$, \textit{i.e.},
    \[
      \bx\tp A \bx = \sum_{i=1}^{n} \sum_{j=1}^{n} A_{ij}x_i x_j > 0
      \quad\text{for $\bx \neq \bzero$.}
    \]
    \vs
  \item We say that $A$ is \textbf{symmetric positive definite} (SPD) if $A$ is symmetric and $A$ is positive definite.
    \vs
  \item \textbf{Useful.} A symmetric matrix is positive definite if and only if all its eigenvalues are real positive number\footnote{It follows that any SPD matrix is invertible.}.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Cholesky Factorization -- Connection to LDLT}
  Let $A \in \RR^{n \times n}$ be a SPD matrix.
  \vs
  \begin{itemize}
  \item Symmetry implies $A = LDL\tp$. \vshalf
  \item Positive definiteness implies $\bx\tp A \bx = \bx\tp L D L\tp \bx > 0$ for any $\bx \neq \bzero$. \vshalf
  \end{itemize}

  \vs
  Consequently, the diagonal element $d_{kk}$ of $D$ is positive for all $k \in \NN[1,n]$, which allows
  \[
    A = LDL\tp = (L D^{1/2}) (D^{1/2} L\tp) \equiv R\tp R,
  \]
  where $R = D^{1/2}L\tp$ is an upper triangular matrix whose diagonal entries are positive.


  % \begingroup
  % \tiny
  % \begin{equation*}
  %   \begin{bmatrix}
  %     a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
  %     a_{12} & a_{22} & a_{23} & \cdots & a_{2n} \\
  %     a_{13} & a_{23} & a_{33} & \cdots & a_{3n} \\
  %     \vdots & \vdots & \vdots & \ddots & \vdots \\
  %     a_{14} & a_{2n} & a_{3n} & \cdots & a_{nn}
  %   \end{bmatrix}
  %   =
  %   \begin{bmatrix}
  %     r_{11} &&&&\\
  %     r_{12} & r_{22} &&&\\
  %     r_{13} & r_{23} & r_{33} &&\\
  %     \vdots & \vdots & & \ddots &\\
  %     r_{1n} & r_{2n} & r_{3n} & \cdots & r_{nn}
  %   \end{bmatrix}
  %   \begin{bmatrix}
  %     r_{11} & r_{12} & r_{13} & \cdots & r_{1n} \\
  %     & r_{22} & r_{23} & \cdots & r_{2n} \\
  %     && r_{33} & \cdots & r_{3n} \\
  %     &&& \ddots & \vdots \\
  %     &&&& r_{nn}
  %   \end{bmatrix}
  % \end{equation*}
  % \endgroup
\end{frame}

\begin{frame}[fragile]
  \frametitle{Cholesky Factorization -- Implementation}
  The decomposition of a SPD matrix $A = R\tp R$ is called the \textbf{Cholesky factorization}.
  \vs
  \begin{itemize}
  \item The calculation of $R$ takes $\sim \frac{1}{3} n^3$ \flops.\vs
  \item Once $R$ is obtained, $R\tp R \bx = \bb$ can be solved by forward elimination and backward substitution in $\sim 2n^2$ \flops. \vs
  \item \textbf{General Formula for $R = [r_{jk}]$}: For derivation, see Section 10.3.
    \begingroup
    \footnotesize
    \begin{align*}
      r_{jj} & = \left( a_{jj} - \sum_{i=1}^{j-1} r_{ij}^2 \right)^{1/2} \\
      r_{jk} & = \left. \left( a_{jk} - \sum_{i=1}^{j-1} r_{ij}r_{ik} \right) \middle/ r_{jj} \right.
               \quad \text{for $k=j+1, \, j+2, \, \ldots, \, n$.}
    \end{align*}
    \endgroup
  \item In \matlab, $R$ is computed by
\begin{verbatim}
>> R = chol(A)
\end{verbatim}
  \end{itemize}
\end{frame}

% \section{Banded Matrices}
\begin{frame}[fragile]
  \frametitle{Banded Matrices}
  We say that $A \in \RR^{n \times n}$ has \vs
  \begin{itemize}
  \item \textbf{upper bandwidth} $b_u$ if $A_{ij} = 0$ for $j-i > b_u$; \vs
  \item \textbf{lower bandwidth} $b_\ell$ if $A_{ij} = 0$ for $i-j > b_\ell$. \vs
  \end{itemize}
  The \textbf{total bandwidth} of $A$ is $b_u + b_\ell + 1$.

  \vspace{2em}
  \textbf{Remarks.} \vs
  \begin{itemize}
  \item If no row pivoting is used, the LU factorization preserves the lower and upper bandwidths of $A$. (Why?) \vs
  \item Since the zeros appear predictably, the factorization and the triangular substitutions can be done with much less operations. ($O(b_u b_\ell n)$)
  \item Use \texttt{sparse} function so that \matlab{} can take advantage of the structure, \textit{e.g.},
\begin{verbatim}
>> [L, U, P] = lu( sparse(A) );
\end{verbatim}
  \end{itemize}

\end{frame}
#+END_EXPORT

** TODO Notes on Row and Column Operations
:PROPERTIES:
:EXPORT_FILE_NAME: notes-row-and-col-ops
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Notes                                                                           :noexport:
**** References
 - FNC 2.2, 2.4
 - LM 10.1
*** Notation
**** Unit Basis Vectors

Throughout this tutorial, suppose $n \in \NN$ is fixed. Let $I$ be the $n \times n$ identity matrix and denote by $\be_j$ its $j\text{th}$ column, /i.e./,
\begin{equation*}
  I =
  \begin{bmatrix}
    1 & 0 & \cdots & 0 \\
    0 & 1 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & 1
  \end{bmatrix}
  =
  \begin{bmatrix}[c|c|c|c]
    & & & \\
    & & & \\
    \be_1 & \be_2 & \cdots & \be_n \\
    & & & \\
    & & &
  \end{bmatrix}.
\end{equation*}

That is,

\begin{equation*}
  \be_1 =
  \begin{bmatrix}
    1 \\ 0 \\ \vdots \\ 0
  \end{bmatrix},\quad
  \be_2 =
  \begin{bmatrix}
    0 \\ 1 \\ \vdots \\ 0
  \end{bmatrix},\quad\cdots,\quad
  \be_n =
  \begin{bmatrix}
    0 \\ 0 \\ \vdots \\ 1
  \end{bmatrix}.
\end{equation*}
***** col1                                                                        :noexport:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
Then for any $A \in \RR^{n \times n}$,

\vs

 - $\be_i\tp A = \balpha_i\tp$, the $i\text{th}$ \blue{row} of $A$.
 - $A \be_j = \ba_j$, the $j\text{th}$ \blue{column} of $A$.
 - $\be_i\tp A \be_j = A_{i,j}$, the $(i,j)$ entry of $A$.

***** col2                                                                        :noexport:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
\begin{equation*}
  A =
  \begin{bmatrix}[c|c|c|c]
    & & & \\
    & & & \\
    \ba_1 & \ba_2 & \cdots & \ba_n \\
    & & & \\
    & & &
  \end{bmatrix}.
\end{equation*}

**** Concatenation
Let $A \in \RR^{n \times n}$. We can view it as a concatenation of its rows or columns as visualized below.
#+LATEX: \begingroup\small
\begin{equation*}
  A =
  \begin{bmatrix}
    \rule[-0.5em]{0pt}{1.3em} a_{11} & a_{12} & \cdots & a_{1n} \\
    \rule[-0.5em]{0pt}{1.5em} a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    \rule[-0.3em]{0pt}{1.3em} a_{n1} & a_{n2} & \cdots & a_{nn} \\
  \end{bmatrix}
  =
  \begin{bmatrix}[c|c|c|c]
    \rule[0pt]{0pt}{1.3em}& & & \\
    & & & \\
    \ba_1 & \ba_2 & \cdots & \ba_n \\
    & & & \\
    \rule[-0.3em]{0pt}{0pt}& & &
  \end{bmatrix}
  =
  \begin{bmatrix}
    \rule[-0.5em]{0pt}{1.3em}& && \balpha_1\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_2\tp && & \\ \hline
    & && \vdots && & \\ \hline
    \rule[-0.3em]{0pt}{1.3em}& && \balpha_n\tp && &
  \end{bmatrix}.
\end{equation*}
#+LATEX: \endgroup

*** Key Operations
**** Row or Column Extraction
A row or a column of $A$ can be extracted using columns of $I$.
#+ATTR_LATEX: :align l|c|c
| Operation                              | Mathematics        | MATLAB                              |
|----------------------------------------+--------------------+-------------------------------------|
| extract the $i\text{th}$ row of $A$    | $\be_i\tp A$       | =A(i,:)=  \rule[-0.5em]{0pt}{1.7em}  |
| extract the $j\text{th}$ column of $A$ | $A \be_j$          | =A(:,j)=  \rule[-0.5em]{0pt}{1.5em}  |
| extract the $(i,j)$ entry of $A$       | $\be_i\tp A \be_j$ | =A(i,j)=  \rule[-0.5em]{0pt}{1.5em} |

**** Elementary Permutation Matrices
***** Elementary Permutation Matrix                                           :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
For $i, j \in \NN[1,n]$ distinct, denote by $P(i,j)$ the $n \times n$ matrix obtained by interchanging the $i\text{th}$ and $j\text{th}$ rows of the $n\times n$ identity matrix. Such matrices are called /elementary permutation matrices/.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
*Example.* ($n = 4$)
\begin{equation*}
  P(1,2) =
  \begin{bmatrix}
    0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1
  \end{bmatrix},
  \quad
  P(1,3) =
  \begin{bmatrix}
    0 & 0 & 1 & 0 \\
    0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1
  \end{bmatrix},
  \quad \cdots
\end{equation*}

*Notable Properties.*
#+LATEX: \begin{multicols}{2}
 - $P(i,j)=P(j,i)$
 - $P(i,j)^2 = I$
#+LATEX: \end{multicols}

**** Row or Column Interchange
Elementary permutation matrices are useful in interchanging rows or columns.
\vs
#+ATTR_LATEX: :align c|c|c
| Operation                                                             | Mathematics | MATLAB                        |
|-----------------------------------------------------------------------+-------------+-------------------------------|
| $\balpha_i\tp \leftrightarrow \balpha_j\tp$ \rule[-0.5em]{0pt}{1.8em} | $P(i,j) A$  | ~A([i,j],:)=A([j,i],:)~       |
| $\ba_i \leftrightarrow \ba_j$                                         | $A P(i,j)$  | ~A(:,[i,j])=A(:,[j,i])~       |

**** Permutation Matrices
***** Permutation Matrix                                                      :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
A /permutation matrix/ $P \in \RR^{n \times n}$ is a square matrix obtained from the same-sized identity matrix by re-ordering of rows.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs

*Notable Properties.*
 - $P\tp = P^{-1}$
 - A product of /elementary permutation matrices/ is a permutation matrix.
\vs
*Row and Column Operations.* For any $A \in \RR^{n \times n}$,
 - $PA$ permutes the rows of $A$.
 - $AP$ permutes the columns of $A$.

**** Row or Column Rearrangement
***** Question
Let $A \in \RR^{6\times6}$, and suppose that it is stored in MATLAB. Rearrange rows of $A$ by moving 1st to 2nd, 2nd to 3rd, 3rd to 5th, 4th to 6th, 5th to 4th, and 6th to 1st, that is,
#+LATEX: \begingroup\small
\begin{equation*}
  \begin{bmatrix}
    \rule[-0.5em]{0pt}{1.2em}& && \balpha_1\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_2\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_3\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_4\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_5\tp && & \\ \hline
    \rule[-0.3em]{0pt}{1.5em}& && \balpha_6\tp && &
  \end{bmatrix}
  \;\longrightarrow\;
  \begin{bmatrix}
    \rule[-0.5em]{0pt}{1.2em}& && \balpha_6\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_1\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_2\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_5\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_3\tp && & \\ \hline
    \rule[-0.3em]{0pt}{1.5em}& && \balpha_4\tp && &
  \end{bmatrix}
\end{equation*}
#+LATEX: \endgroup

**** Row or Column Rearrangement
*Solution.*
\vs
 - Mathematically: $PA$ where
   #+LATEX: \begingroup\footnotesize
   \begin{equation*}
     P =
     \begin{bmatrix}
       \rule[-0.5em]{0pt}{1.2em}& && \be_6\tp && & \\ \hline
       \rule[-0.5em]{0pt}{1.5em}& && \be_1\tp && & \\ \hline
       \rule[-0.5em]{0pt}{1.5em}& && \be_2\tp && & \\ \hline
       \rule[-0.5em]{0pt}{1.5em}& && \be_5\tp && & \\ \hline
       \rule[-0.5em]{0pt}{1.5em}& && \be_3\tp && & \\ \hline
       \rule[-0.3em]{0pt}{1.5em}& && \be_4\tp && &
     \end{bmatrix}
     =
     \begin{bmatrix}
       \rule[-0.5em]{0pt}{1.5em} 0 & 0 & 0 & 0 & 0 & 1 \\
       \rule[-0.5em]{0pt}{1.5em} 1 & 0 & 0 & 0 & 0 & 0 \\
       \rule[-0.5em]{0pt}{1.5em} 0 & 1 & 0 & 0 & 0 & 0 \\
       \rule[-0.5em]{0pt}{1.5em} 0 & 0 & 0 & 0 & 1 & 0 \\
       \rule[-0.5em]{0pt}{1.5em} 0 & 0 & 1 & 0 & 0 & 0 \\
       \rule[-0.3em]{0pt}{1.5em} 0 & 0 & 0 & 1 & 0 & 0
     \end{bmatrix}.
   \end{equation*}
   #+LATEX: \endgroup
 - MATLAB:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 A = A([6 1 2 5 3 4], :)
 % short for  A([1 2 3 4 5 6], :) = A([6 1 2 5 3 4], :)
   #+END_SRC

*** Gaussian Transformation Matrices (GTM)
**** Elementary Row Operation and GTM
Let $1 \le j < i \le n$.
\vs
 - The row operation $R_i \to R_i + c R_j$ on $A \in \RR^{n \times n}$, for some $c \in \RR$, can be emulated by a matrix multiplication[fn::Many linear algebra texts refer to the matrix in parentheses as an /elementary matrix/.]
   \[
     (I + c \, \be_i \be_j\tp) A.
   \]
 - In the context of Gaussian elimination, the operation of introducing zeros below the $j\text{th}$ diagonal entry can be done via
   \[
     \underbrace{(I + \sum_{i=j+1}^{n} c_{i,j}\, \be_i \be_j\tp)}_{=G_j}A, \quad 1 \le j < n.
   \]
   The matrix $G_j$ is called a /Gaussian transformation matrix/ (GTM).

**** Elementary Row Operation and GTM {{{cont}}}
 - To emulate $(I + c \be_i \be_j\tp)A$ in MATLAB:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 A(i,:) = A(i,:) + c*A(j,:);
   #+END_SRC
 - To emulate
   \[
   G_j A = (I + \sum_{i=j+1}^n c_{i,j} \be_i \be_j\tp)A
   \]
   in MATLAB:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 for i = j+1:n
     c = ....
     A(i,:) = A(i,:) + c*A(j,:);
 end
   #+END_SRC
   This can be done without using a loop.

**** Analytical Properties of GTM
 - GTMs are /unit/ lower triangular matrices.
 - The product of GTMs is another unit lower triangular matrix.
 - The inverse of a GTM is also a unit lower triangular matrix.

* DONE Chapter 3: Overdetermined Linear Systems
** DONE Introduction to Overdetermined Linear Systems
:PROPERTIES:
:EXPORT_FILE_NAME: 15-overdetermined-linear-systems
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

%\begin{frame}{Contents}
%  \tableofcontents
%\end{frame}
#+END_EXPORT

*** Notes                                                                           :noexport:
 - FNC 3.1, 3.2
 - LM 12.5

*** Opening Example: Polynomial Approximation
**** Introduction
***** Problem: Fitting Functions to Data
 Given data points $\{ (x_i, y_i) \mid i \in \NN[1,m]\}$, pick a form for the ``fitting'' function $f(x)$ and minimize its total error in representing the data.
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - With real-world data, interpolation is often not the best method.
 - Instead of finding functions lying exactly on given data points, we look for ones which are ``close'' to them.
 - In the most general terms, the fitting function takes the form
   \[
   f(x) = c_1 f_1(x) + \cdots + c_n f_n(x),
   \]
   where $f_1, \ldots, f_n$ are known functions while $c_1, \ldots, c_n$ are to be determined.

**** Linear Least Squares Approximation
In this discussion:
\vs
 - use a polynomial fitting function $p(x) = c_1 + c_2 x + \cdots + c_n x^{n-1}$ with $n < m$;
 - minimize the 2-norm of the error $r_i = y_i - p(x_i)$:
   \[
     \Norm{\br}{2} = \sqrt{\sum_{i=1}^m r_i^2} = \sqrt{\sum_{i=1}^{m} \left( y_i - p(x_i) \right)^2}.
   \]

\vs
Since the fitting function is linear in unknown coefficients and the 2-norm is minimized, this method of approximation is called the *linear least squares (LLS) approximation*.

**** Example: Temperature Anomaly
Below are 5-year averages of the worldwide temperature anomaly as compared to the 1951-1980 average (source: NASA).

#+ATTR_LATEX: :align c|r
| Year | Anomaly ($^{\circ}C$) |
|------+-----------------------|
| 1955 |               -0.0480 |
| 1960 |               -0.0180 |
| 1965 |               -0.0360 |
| 1970 |               -0.0120 |
| 1975 |               -0.0040 |
| 1980 |                0.1180 |
| 1985 |                0.2100 |
| 1990 |                0.3320 |
| 1995 |                0.3340 |
| 2000 |                0.4560 |

**** Example: Temperature Anomaly {{{cont}}}
*Import and Plot Data*
***** Columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
t = (1955:5:2000)';
y = [-0.0480; -0.0180;
     -0.0360; -0.0120;
     -0.0040;  0.1180;
      0.2100;  0.3320;
      0.3340;  0.4560];
plot(t, y, '.')
#+END_SRC

#+RESULTS:
#+begin_example
t = (1955:5:2000)';
y = [-0.0480; -0.0180;
     -0.0360; -0.0120;
     -0.0040;  0.1180;
      0.2100;  0.3320;
      0.3340;  0.4560];
plot(t, y, '.', 'MarkerSize', 30)
grid on
xlabel('year')
ylabel('anomaly ({\circ}C)')
print -depsc 'temp_anomaly'
'org_babel_eoe'
ans =
    'org_babel_eoe'
#+end_example

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
#+ATTR_LATEX: :width 0.9\linewidth
[[../img/temp_anomaly.eps]]

**** Example: Temperature Anomaly {{{cont}}}
*Interpolation*
***** Columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
t = (t-1950)/10;
n = length(t);
V = t.^(0:n-1);
c = V\y;
p = @(x) polyval(flip(c), (x-1950)/10);
hold on
fplot(p, [1955 2000])
#+END_SRC

#+RESULTS:
: t = (t-1950)/10;
: n = length(t);
: V = t.^(0:n-1);
: c = V\y;
: p = @(x) polyval(flip(c), (x-1950)/10);
: hold on, fplot(p, [1955 2000])
: 'org_babel_eoe'
: ans =
:     'org_babel_eoe'

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
#+ATTR_LATEX: :width 0.9\linewidth
[[../img/temp_anomaly_interp.eps]]

**** Fitting by a Straight Line
Suppose that we are fitting data to a linear polynomial: $p(x) = c_1 + c_2 x$.
\vs

 - If it were to pass through all data points:
   \begin{equation*}
     \left\{\;
     \begin{alignedat}{4}
       y_1  & {}={} & p(x_1) & {}={} & c_1  & {}+{} & c_2 x_1 \\
       y_2  & {}={} & p(x_2) & {}={} & c_1  & {}+{} & c_2 x_2 \\
       \vdots  &  & \vdots &  & & \vdots &  \\
       y_m  & {}={} & p(x_m) & {}={} & c_1  & {}+{} & c_2 x_m
     \end{alignedat}
     \right.
     \;\xrightarrow{\text{ matrix equation }}\;
     \underbrace{
       \begin{bmatrix}
         y_1 \\ y_2 \\ \vdots \\ y_m
       \end{bmatrix}
     }_{\by}
     =
     \underbrace{
       \begin{bmatrix}
         1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_m
       \end{bmatrix}
     }_{V}
     \underbrace{
       \begin{bmatrix}
         c_1 \\ c_2
       \end{bmatrix}
     }_{\bc}
   \end{equation*}
 - The above is unsolvable; instead, find $\bc$ which makes the /residual/ $\br = \by - V\bc$ ``as small as possible'' in the sense of vector 2-norm.
 - *Notation:* $\by \, \text{``=''} \, V\bc$

**** Fitting by a Striaght Line: MATLAB Implementation
Revisiting the temperature anomaly example again:
***** Columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
year = (1955:5:2000)';
t = year - 1955;
V = t.^(0:1);
c = V\y;
p = @(x) polyval(flip(c), x-1955);
plot(year, y, '.')
hold on
fplot(p, [1955, 2000])
#+END_SRC

#+RESULTS:
#+begin_example
year = (1955:5:2000)';
t = year - 1955;
V = t.^(0:1);
c = V\y;
p = @(x) polyval(flip(c), x-1955);
plot(year, y, '.', 'MarkerSize', 30)
hold on
fplot(p, [1955 2000])
grid on
xlabel('year')
ylabel('anomaly ({\circ}C)')
print -depsc 'temp_anomaly_lls1'
'org_babel_eoe'
ans =
    'org_babel_eoe'
#+end_example

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
#+ATTR_LATEX: :width 0.9\linewidth
[[../img/temp_anomaly_lls1.eps]]

**** Fitting by a General Polynomial
In general, when fitting data to a polynomial
\[
  p(x) = c_1 + c_2 x + c_3 x^2 + \cdots + c_n x^{n-1},
\]
we need to solve
\begin{equation*}
  \underbrace{
    \begin{bmatrix}
      y_1 \\ y_2 \\ \vdots \\ y_m
    \end{bmatrix}
  }_{\by}
  \text{``=''}
  \underbrace{
    \begin{bmatrix}
      1 & x_1 & x_1^2 & \cdots & x_1^{n-1} \\
      1 & x_2 & x_2^2 & \cdots & x_2^{n-1} \\
      \vdots & \vdots & \vdots & \ddots & \vdots \\
      1 & x_m & x_m^2 & \cdots & x_m^{n-1}
    \end{bmatrix}
  }_{V}
  \underbrace{
    \begin{bmatrix}
      c_1 \\ c_2 \\ \vdots \\ c_n
    \end{bmatrix}
  }_{\bc} \,.
\end{equation*}


 - The solution $\bc$ of $\by\,\text{``=''}\,V\bc$ turns out to be the solution of the \underline{\it normal equation\/}
   \[
     V^{\rm T} V \bc = V^{\rm T} \by \,.
   \]

**** Fitting by a General Polynomial: MATLAB Implementation
Revisiting the temperature anomaly example again:
***** Columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
V = t.^(0:3);
c = V\y;
q = @(x) polyval(flip(c), x-1955);
hold on
fplot(q, [1955 2000])
#+END_SRC

#+RESULTS:
: V = t.^(0:3);
: c = V\y;
: q = @(x) polyval(flip(c), x-1955);
: hold on
: fplot(q, [1955 2000])
: 'org_babel_eoe'
: ans =
:     'org_babel_eoe'

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
#+ATTR_LATEX: :width 0.9\linewidth
[[../img/temp_anomaly_lls3.eps]]

**** Backslash Again (old)                                                         :noexport:
# Write it more clearly/cleanly

***** The Versatile Backslash
In MATLAB, the generic linear equation $A\bx = \bb$ is solved by ~x = A\b~.
 - When $A$ is a square matrix, Gaussian elimination is used.
 - When $A$ is NOT a square matrix, the normal equation $A^{\rm T} A \bx = A^{\rm T} \bb$ is solved instead.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - As long as $A \in \RR^{m \times n}$ where $m \ge n$ has rank $n$, the square matrix $A^{\rm T} A$ is nonsingular. (unique solution)
 - Though $A\tp A$ is a square matrix, MATLAB does not use Gaussian elimination to solve the normal equation.
 - Rather, a faster and more accurate algorithm is used.

**** Backslash Again
***** The Versatile Backslash
In MATLAB, ~x = A\b~ computes
 - the solution of $A\bx = \bb$ when $A$ is a square matrix;
 - the solution of $A\tp A \bx = A\tp \bb$ when $A$ is not a square matrix.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - As long as $A \in \RR^{m \times n}$ where $m \ge n$ has rank $n$, the square matrix $A^{\rm T} A$ is nonsingular. (unique solution)
   # QUESTION: What does MATLAB do when $A$ is not a full-rank matrix?

 - Though $A\tp A$ is a square matrix, MATLAB does not use Gaussian elimination to solve the normal equation.
 - Rather, a faster and more accurate algorithm is used.

\vfill
To learn more about the algorithms behind the backslash operator, check out =doc mldivide=.

** DONE Normal Equations
:PROPERTIES:
:EXPORT_FILE_NAME: 16-normal-equations
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** The Normal Equations
**** LLS and Normal Equation
*Big Question:* How is the least square solution $\bx$ to $A\bx \, \text{``=''} \, \bb$ equivalent to the solution of the normal equation $A^{\rm T}A \bx = A^{\rm T} \bb$?
\vs

***** Theorem (Normal Equation and LLS)                                            :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
Let $A \in \RR^{m \times n}$ with $m \ge n$. Then $\bx \in \RR^{n}$ satisfies $A\tp A \bx = A\tp \bb$ if and only if $\bx$ solves the LLS problems, /i.e./, $\bx$ minimizes $\Norm{\bb - A\bx}{2}$.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - *Idea of Proof for $(\Rightarrow)$*. Enough show to that $\Norm{\bb - A(\bx+\by)}{2} \ge \Norm{\bb - A\bx}{2}$ for any $\by \in \RR^n$.
 - *Useful identity*.
   \[
   \Norm{\bu \pm \bv}{2}^2 = \Norm{\bu}{2}^2 + \Norm{\bv}{2}^2 \pm 2 \bu\tp\bv, \tag{$\star$}
   \]

**** Proof of the Forward Implication
/Proof/. ($\Rightarrow$) Let $\by \in \RR^{m}$ be arbitrary. Using the identity $(\star)$, we can write
\[
\Norm{\bb - A(\bx+\by)}{2}^2 = \Norm{\bb - A\bx}{2}^2 + \Norm{A\by}{2}^2 - 2\by\tp A\tp (\bb-A\bx).
\]
But $\bx$ solves the normal equation $A\tp A\bx = A\tp \bb$ and $\Norm{A\by}{2}\ge0$, so it follows that
\[
\Norm{\bb - A(\bx+\by)}{2}^2 \ge \Norm{\bb - A\bx}{2}^2.
\]
Since $\by$ was chosen arbitrarily, this shows that $\bx$ minimizes $\norm{\bb - A\bx}$.
\qed


# #+ATTR_LATEX: :width \linewidth
# -----
# #+LATEX: {\footnotesize
# Alternately, one can derive the normal equation using calculus. See Appendix to this deck of lecture slides.
# #+LATEX: }

**** Proof of the Backward Implication
# Here we show that the minimizer $\bx$ of $\norm{\bb - A\bx}{2}$ is the solution of the normal equation $A\tp A \bx = A\tp \bb$.

/Proof/. ($\Leftarrow$) Suppose that $\bx$ satisfies $A\bx \, \text{``=''} \, \bb$, that is, $\bx$ minimize the 2-norm of the residual $\br = \bb - A\bx$:
\[
  g(x_1, x_2, \dots, x_n)
  := \Norm{\br}{2}^2
  = \sum_{i=1}^{m} \Bigl( b_i - \sum_{j=1}^{n} a_{ij} x_j \Bigr)^2 \,.
\]
By calculus, it follows that
\[
  {\bf 0} = \grad g(x_1, x_2, \dots, x_n) \,
\]
which yields $n$ equations in $n$ unknowns $x_1, x_2, \dots, x_n$.

#+LATEX: \vfill
#+LATEX: \hfill {\color{gray}{(continued on then next page)}}

**** Proof of the Backward Implication {{{cont}}}

Note that $\del x_j / \del x_k = \delta_{j,k}$, so the $n$ equations $\del g /\del x_k = 0$ are written out as
\[
  0 = \sum_{i=1}^{m} 2 \bigl( b_i - \sum_{j=1}^{n} a_{ij} x_j \bigr)
  (-a_{ik}),
  \qquad \text{for $k \in \NN[1,n]$} \,,
\]
which can be rearranged into
\[
  \sum_{i=1}^{m} a_{ik} b_i
  = \sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij} a_{ik} x_j,
  \qquad \text{for $k \in \NN[1,n]$} \,.
\]
One can see that the two sides correspond to the $k^{\rm th}$ elements of $A^{\rm T} \bb$ and $A^{\rm T} A \bx$ respectively:
\[
  A^{\rm T} A \bx = A^{\rm T} \bb \,,
\]
which completes the proof. \qed

**** Pseudoinverse
*****                                                                         :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
Let $A \in \RR^{m \times n}$. Then the *pseudoinverse* of $A$, denoted $A^{+}$, is the $n \times m$ matrix given by
\[
A^{+} = \left( A\tp A \right)^{-1} A\tp.
\]
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs

#+ATTR_LATEX: :align c|c|c
| Problem                       | Mathematical Solution | MATLAB    |
|-------------------------------+-----------------------+-----------|
| $A\bx = \bb$                  | $\bx = A^{-1} \bb$    | ~x = A\b~ |
| $A\bx \, \text{``=''} \, \bb$ | $\bx = A^{+} \bb$     | ~x = A\b~ |

#+BEGIN_QUOTE
In MATLAB, backslash is equivalent mathematically to left-multiplication by the inverse (square case) or pseudoinverse (rectangular case) of a matrix.
#+END_QUOTE

*** TODO Pseudoinverse
**** Pseudoinverse
*****                                                                         :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
Let $A \in \RR^{m \times n}$. Then the *pseudoinverse* of $A$, denoted $A^{+}$, is the $n \times m$ matrix given by
\[
A^{+} = \left( A\tp A \right)^{-1} A\tp.
\]
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs

#+ATTR_LATEX: :align c|c|c
| Problem                       | Mathematical Solution | MATLAB    |
|-------------------------------+-----------------------+-----------|
| $A\bx = \bb$                  | $\bx = A^{-1} \bb$    | ~x = A\b~ |
| $A\bx \, \text{``=''} \, \bb$ | $\bx = A^{+} \bb$     | ~x = A\b~ |

#+BEGIN_QUOTE
In MATLAB, backslash is equivalent mathematically to left-multiplication by the inverse (square case) or pseudoinverse (rectangular case) of a matrix.
#+END_QUOTE

**** Analytical Properties of $A\tpA$
The matrix $A\tp A$ appearing in the definition of $A^+$ satisfies the following properties.
***** @@latex:  @@                                                               :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
For any $A \in \RR^{m \times n}$ with $m \ge n$, the following are true:
 1. $A\tp A$ is symmetric.
 2. $A\tp A$ is singular if and only if $\text{rank}(A) < n$.
 3. If $A\tp A$ is nonsingular, then it is positive definite.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
A symmetric positive definite (SPD) matrix $S$ such as $A\tp A$ permits so-called the *Cholesky factorization*
\[
S = R\tp R
\]
where $R$ is an upper triangular matrix.

**** Cholesky Factorization
\begin{frame}
  \frametitle{Symmetric Positive Definite Matrices -- Cholesky Factorization}
  Let $A \in \RR^{n \times n}$. \vs
  \begin{itemize}
  \item We say that $A$ is \textbf{positive definite} if the \textit{quadratic form} is positive, \textit{i.e.}, $\bx\tp A \bx > 0$ for all $\bx \neq \bzero$, \textit{i.e.},
    \[
      \bx\tp A \bx = \sum_{i=1}^{n} \sum_{j=1}^{n} A_{ij}x_i x_j > 0
      \quad\text{for $\bx \neq \bzero$.}
    \]
    \vs
  \item We say that $A$ is \textbf{symmetric positive definite} (SPD) if $A$ is symmetric and $A$ is positive definite.
    \vs
  \item \textbf{Useful.} A symmetric matrix is positive definite if and only if all its eigenvalues are real positive number\footnote{It follows that any SPD matrix is invertible.}.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Cholesky Factorization -- Connection to LDLT}
  Let $A \in \RR^{n \times n}$ be a SPD matrix.
  \vs
  \begin{itemize}
  \item Symmetry implies $A = LDL\tp$. \vshalf
  \item Positive definiteness implies $\bx\tp A \bx = \bx\tp L D L\tp \bx > 0$ for any $\bx \neq \bzero$. \vshalf
  \end{itemize}

  \vs
  Consequently, the diagonal element $d_{kk}$ of $D$ is positive for all $k \in \NN[1,n]$, which allows
  \[
    A = LDL\tp = (L D^{1/2}) (D^{1/2} L\tp) \equiv R\tp R,
  \]
  where $R = D^{1/2}L\tp$ is an upper triangular matrix whose diagonal entries are positive.


  % \begingroup
  % \tiny
  % \begin{equation*}
  %   \begin{bmatrix}
  %     a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
  %     a_{12} & a_{22} & a_{23} & \cdots & a_{2n} \\
  %     a_{13} & a_{23} & a_{33} & \cdots & a_{3n} \\
  %     \vdots & \vdots & \vdots & \ddots & \vdots \\
  %     a_{14} & a_{2n} & a_{3n} & \cdots & a_{nn}
  %   \end{bmatrix}
  %   =
  %   \begin{bmatrix}
  %     r_{11} &&&&\\
  %     r_{12} & r_{22} &&&\\
  %     r_{13} & r_{23} & r_{33} &&\\
  %     \vdots & \vdots & & \ddots &\\
  %     r_{1n} & r_{2n} & r_{3n} & \cdots & r_{nn}
  %   \end{bmatrix}
  %   \begin{bmatrix}
  %     r_{11} & r_{12} & r_{13} & \cdots & r_{1n} \\
  %     & r_{22} & r_{23} & \cdots & r_{2n} \\
  %     && r_{33} & \cdots & r_{3n} \\
  %     &&& \ddots & \vdots \\
  %     &&&& r_{nn}
  %   \end{bmatrix}
  % \end{equation*}
  % \endgroup
\end{frame}

\begin{frame}[fragile]
  \frametitle{Cholesky Factorization -- Implementation}
  The decomposition of a SPD matrix $A = R\tp R$ is called the \textbf{Cholesky factorization}.
  \vs
  \begin{itemize}
  \item The calculation of $R$ takes $\sim \frac{1}{3} n^3$ \flops.\vs
  \item Once $R$ is obtained, $R\tp R \bx = \bb$ can be solved by forward elimination and backward substitution in $\sim 2n^2$ \flops. \vs
  \item \textbf{General Formula for $R = [r_{jk}]$}: For derivation, see Section 10.3.
    \begingroup
    \footnotesize
    \begin{align*}
      r_{jj} & = \left( a_{jj} - \sum_{i=1}^{j-1} r_{ij}^2 \right)^{1/2} \\
      r_{jk} & = \left. \left( a_{jk} - \sum_{i=1}^{j-1} r_{ij}r_{ik} \right) \middle/ r_{jj} \right.
               \quad \text{for $k=j+1, \, j+2, \, \ldots, \, n$.}
    \end{align*}
    \endgroup
  \item In \matlab, $R$ is computed by
\begin{verbatim}
>> R = chol(A)
\end{verbatim}
  \end{itemize}
\end{frame}

**** Least Squares Using Cholesky Factorization
One can solve the LLS problem $A \bx  \, \text{``=''} \, \bb$ by solving the normal equation $A\tp A \bx = A\tp \bb$ directly as below.

 1. Compute $N = A\tp A$.
 2. Compute $\bz = A\tp \bb$.
 3. Solve the square linear system $N\bx = \bz$ for $\bx$.

\vs
Step 3 is done using =chol= which implements the Cholesky factorization.
\vfill

*MATLAB Implementarion.*
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
N = A'*A;
z = A'*b;
R = chol(N);
w = forelim(R',z);    % solve R'w = z
x = backsub(R,w);     % solve R x = w
#+END_SRC

**** Conditioning of Normal Equations

 - Recall that the condition number of solving a square linear system $A\bx = \bb$ is $\kappa(A) = \norm{A}\norm{A^{-1}}$.
 - Provided that the residual norm at the least square solution is relatively small, the conditioning of LLS problem is similar:
   \[
   \kappa(A) = \norm{A} \norm{A^+}.
   \]
 - If $A$ is rank-deficient (columns are linearly dependent), then $\kappa(A) = \infty$.
 - If an LLS problem is solved solving the normal equation, it can be shown that the condition number is
   \[
   \kappa(A\tp A) = \kappa(A)^2.
   \]

** DONE QR Factorization
:PROPERTIES:
:EXPORT_FILE_NAME: 18-qr-fact
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

% \begin{frame}{Contents}
%   \tableofcontents
% \end{frame}
#+END_EXPORT

*** Orthogonality
**** Normal Equation Revisited                                                     :noexport:
Alternate perspective on the ``normal equation'':
\[
  A^\rT (\bb - A\bx) = \bzero
  \quad \Longleftrightarrow \quad
  \bz^\rT (\underbrace{\bb - A\bx}_{\text{\rm residual $= \br$}})
  = 0
  \quad \text{for all $\bz \in \cR(A)$} \,,
\]
***** Columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
i.e., $\bx$ solves the normal equation if and only if the residual is orthogonal to the range of $A$.

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+LATEX: \vspace{-2em}
#+ATTR_LATEX: :width 0.95\linewidth
[[../img/orthog.pdf]]

**** Orthogonal Vectors
Recall that the angle $\theta$ between two vectors $\bu, \bv \in \RR^n$ satisfies
\[
  \cos(\theta) = \frac{\bu\tp \bv}{\Norm{\bu}{2}\Norm{\bv}{2}}.
\]
***** @@latex:  @@                                                            :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
 - Two vectors $\bu, \bv \in \RR^n$ are *orthogonal* if $\bu^\rT \bv = 0$.
 - Vectors $\bq_1, \bq_2, \ldots, \bq_k \in \RR^n$ are *orthogonal* if $\bq_i^\rT\bq_j = 0$ for all $i\neq j$.
 - Vectors $\bq_1, \bq_2, \ldots, \bq_k \in \RR^n$ are *orthonormal* if $\bq_i\tp \bq_j = \delta_{i,j}$.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vfill
#+LATEX: \begingroup\footnotesize
*Notation.* (Kronecker delta function)
\begin{equation*}
\delta_{i,j} =
\begin{dcases}
  1, & i = j \\
  0, & i\neq j
\end{dcases}
\end{equation*}
#+LATEX: \endgroup

**** Matrices with Orthogonal Columns
Let $Q = \left[\; \bq_1\mid \bq_2 \mid \dots \mid \bq_k \;\right] \in \RR^{n \times k}$. Note that
\begin{equation*}
  Q^\rT Q =
  \visible<1|handout:0>{
    \begin{bmatrix}
      &\rule[-0.5em]{0pt}{1.3em}\bq_1\tp &\\
      \hline
      &\rule[-0.5em]{0pt}{1.5em}\bq_2\tp &\\
      \hline
      &\vdots &\\
      \hline
      &\rule[-0.3em]{0pt}{1.3em}\bq_k\tp &
    \end{bmatrix}
    \begin{bmatrix}[c|c|c|c]
      &&& \\
      \bq_1 & \bq_2 & \cdots & \bq_k \\
      &&&
    \end{bmatrix}
    =
    \begin{bmatrix}
      \bq_1^\rT \bq_1 & \bq_1\tp \bq_2 & \cdots & \bq_1^\rT \bq_k \\
      \bq_2^\rT \bq_1 & \bq_2\tp \bq_2 & \cdots & \bq_2^\rT \bq_k \\
      \vdots   & \vdots   &  \ddots & \vdots \\
      \bq_k^\rT \bq_1 & \bq_k\tp \bq_2 & \cdots & \bq_k^\rT \bq_k
    \end{bmatrix}.
  }
\end{equation*}

\vs
Therefore,
 - $\bq_1, \ldots, \bq_k$ are orthogonal. \quad $\Longleftrightarrow$ \quad $Q\tp Q$ is a $k \times k$ diagonal matrix.
 - $\bq_1, \ldots, \bq_k$ are orthonormal. \quad $\Longleftrightarrow$ \quad $Q\tp Q$ is the $k \times k$ identity matrix.

**** Matrices with Orthonormal Columns
***** @@latex:  @@                                                               :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Let $Q = \left[\; \bq_1\mid \bq_2 \mid \dots \mid \bq_k \;\right] \in \RR^{n \times k}$ and suppose that $\bq_1, \ldots, \bq_k$ are orthonormal. Then
 1. $Q\tp Q = I \in \RR^{k \times k}$;
 2. $\Norm{Q\bx}{2} = \Norm{\bx}{2}$ for all $\bx \in \RR^k$;
 3. $\Norm{Q}{2} = 1$.

**** Orthogonal Matrices
***** @@latex:  @@                                                            :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
We say that $Q \in \RR^{n \times n}$ is an \textbf{orthogonal matrix} if $Q^\rT Q = I \in \RR^{n \times n}$.\vshalf
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - A square matrix with orthogonal columns is not, in general, an orthogonal matrix!

**** Properties of Orthogonal Matrices
***** @@latex:  @@                                                               :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Let $Q \in \RR^{n \times n}$ be orthogonal. Then
 1. $Q^{-1} = Q\tp$;
 2. $Q\tp$ is also an orthogonal matrix;
 3. $\kappa_2(Q)=1$;
 4. For any $A \in \RR^{n \times n}$, $\Norm{AQ}{2} = \Norm{A}{2}$;
 5. if $P \in \RR^{n \times n}$ is another orthogonal matrix, then $PQ$ is also orthogonal.

***** ...                                                                           :B_note:
:PROPERTIES:
:BEAMER_env: note
:END:
\vs
#+LATEX: \begingroup\footnotesize
/Proof of 4/. Let $\bx \in \RR^n$ with $\Norm{\bx}{2} = 1$. Then $\Norm{Q\bx}{2} = 1$ and so by the definition of the induced norm,
\[
\Norm{AQ\bx}{2} \le \Norm{A}{2}.
\]
Taking the maximum over all $\Norm{\bx}{2} = 1$, $\Norm{AQ}{2} \le \Norm{A}{2}$. Conversely, for any $\by \in \RR^n$ with $\Norm{\by}{2} = 1$, there exists $\bx \in \RR^n$ with $\Norm{\bx}{2} = 1$ such that $\by = Q\bx$ since $Q^{-1}$ exists. It follows that
\[
\Norm{A\by}{2} = \Norm{AQ\bx}{2} \le \Norm{AQ}{2}
\]
Taking the maximum over all $\Norm{\by}{2} = 1$, $\Norm{A}{2} \le \Norm{AQ}{2}$.
#+LATEX: \endgroup

**** Why Do We Like Orthogonal Vectors?
 - If $\bu$ and $\bv$ are orthogonal, then
   \[
     \Norm{\bu \pm \bv}{2}^2 =
     \visible<0|handout:1>{\Norm{\bu}{2}^2 \pm 2 \bu\tp\bv + \Norm{\bv}{2}^2 = \Norm{\bu}{2}^2 + \Norm{\bv}{2}^2.}
   \]
 - Without orthogonality, it is possible that $\Norm{\bu - \bv}{2}$ is much smaller than $\Norm{\bu}{2}$ and $\Norm{\bv}{2}$.
 - The addition and subtraction of orthogonal vectors are guaranteed to be well-conditioned.

#+LATEX: \begin{figure}[ht] \centering \footnotesize
\incfig{../img/orthog_vec_subtraction}{0.8\linewidth}
#+LATEX: \end{figure}

*** QR Factorization
**** The QR Factorization
The following matrix factorization plays a role in solving linear least squares problems similar to that of LU factorization in solving linear systems.
***** @@latex:  @@                                                               :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Let $A \in \RR^{m \times n}$ where $m \ge n$. Then $A = QR$ where
$Q \in \RR^{m\times m}$ is orthogonal and $R \in \RR^{m \times n}$
is upper triangular.
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+LATEX: \visible<1|handout:0>
#+LATEX: {\begingroup \small
\begin{equation*}
  \underbrace{
    \begin{bmatrix}[c|c|c|c]
      &&& \\
      &&& \\
      \ba_1 & \ba_2 & \cdots & \ba_n \\
      &&& \\
      &&&
    \end{bmatrix}
  }_{A}
  =
  \underbrace{
    \begin{bmatrix}[c|c|c|c]
      &&& \\
      &&& \\
      \bq_1 & \bq_2 & \cdots & \bq_m \\
      &&& \\
      &&&
    \end{bmatrix}
  }_{Q}
  \underbrace{
    \begin{bmatrix}
      r_{11} & r_{12} & \cdots & r_{1n} \\
      0 & r_{22} & \cdots & r_{2n} \\
      \vdots & & \ddots & \vdots \\
      0 & 0 & \cdots & r_{nn} \\
      0 & 0 & \cdots & 0 \\
      \vdots & \vdots & & \vdots \\
      0 & 0 & \cdots & 0
    \end{bmatrix}
  }_{R}
\end{equation*}
#+LATEX: \endgroup}

**** Thick VS Thin QR Factorization
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:
 - Here is the QR factorization again.
#+LATEX: \begingroup \small
\begin{equation*}
  A =
  \underbrace{
    \begin{bmatrix}[c|c|c|c]
      &&& \\
      &&& \\
      \bq_1 & \bq_2 & \cdots & \bq_m \\
      &&& \\
      &&&
    \end{bmatrix}
  }_{Q}
  \underbrace{
    \begin{bmatrix}
      r_{11} & r_{12} & \cdots & r_{1n} \\
      0 & r_{22} & \cdots & r_{2n} \\
      \vdots & & \ddots & \vdots \\
      0 & 0 & \cdots & r_{nn} \\
      0 & 0 & \cdots & 0 \\
      \vdots & \vdots & & \vdots \\
      0 & 0 & \cdots & 0
    \end{bmatrix}
  }_{R} \tag{thick}
\end{equation*}
#+LATEX: \endgroup
 - When $m$ is much larger than $n$, it is much more efficient to use the /thin/ or /compressed/ QR factorization.
#+LATEX: \begingroup \small
\begin{equation*}
  A =
  \underbrace{
    \begin{bmatrix}[c|c|c|c]
      &&& \\
      &&& \\
      \bq_1 & \bq_2 & \cdots & \bq_n \\
      &&& \\
      &&&
    \end{bmatrix}
  }_{\widehat{Q}}
  \underbrace{
    \begin{bmatrix}
      r_{11} & r_{12} & \cdots & r_{1n} \\
      0 & r_{22} & \cdots & r_{2n} \\
      \vdots & & \ddots & \vdots \\
      0 & 0 & \cdots & r_{nn}
    \end{bmatrix}
  }_{\widehat{R}} \tag{thin}
\end{equation*}
#+LATEX: \endgroup

**** QR Factorization in MATLAB
Either type of QR factorization is computed by =qr= command.
 - Thick/Full QR factorization
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 [Q, R] = qr(A)
   #+END_SRC

 - Thin/Compressed QR factorization
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 [Q, R] = qr(A, 0)
   #+END_SRC

Test the orthogonality of =Q= by calculating the norm of $Q\tp Q - I$ where $I$ is the identity matrix with /suitable/ dimensions.

#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth, xleftmargin=.06\linewidth
#+BEGIN_SRC matlab
 norm(Q'*Q - eye(m))       % full QR
 norm(Q'*Q - eye(n))       % thin QR
#+END_SRC

*** Least Squares and QR Factorization
**** Recap: Least Squares and Pseudoinverse
Let $A \in \RR^{m \times n}$ with $m \ge n$ and suppose that columns of $A$ are linearly independent. Recall:

 - The least square problem $A\bx \,\text{``=''}\, \bb$ is equivalent to the normal equation $A\tp A \bx = A\tp \bb$, which is a square matrix equation.
 - The solution can be written as
   \[
     \bx = \left( A\tp A \right)^{-1} A\tp \bb = A^+ \bb,
   \]
   where $A^+ \in \RR^{n \times m}$ is the *pseudoinverse*.
 - MATLAB's backslash is mathematically equivalent to left-multiplication by the inverse or pseudoinverse of a matrix.
 - MATLAB's =pinv= calculates the pseudoinverse, but it is rarely used in practice, just as =inv=.

**** Pseudoinverse and QR
The pseudoinverse $A^{+}$ can be written using the thin QR factorization of $A = \widehat{Q} \widehat{R}$ as
\[
  A^{+} = \widehat{R}^{-1} \widehat{Q}\tp.
\]

# One can derive a similar result using the thick QR factorization as seen in *LM* on p. 1624.

***** NOTE: Inverse of transpose is transpose of inverse                          :noexport:
Lemma: Let $A$ be invertible. Then $A\tp$ is invertible and $(A\tp)^{-1} = (A^{-1})\tp$.
\vs

/Proof/. Since $A$ is invertible, $\rk(A) = n = \rk( A\tp )$, so $A\tp$ is also invertible. It follows that
\[
\left(A^{-1}\right)\tp
= \left(A^{-1}\right)\tp A\tp \left(A\tp\right)^{-1}
= \underbrace{ \left(A A^{-1} \right)\tp }_{=I} \left(A\tp\right)^{-1}
= \left(A\tp\right)^{-1}.
\]
This completes the proof. \qed

**** Least Squares Using QR Factorization
By substituting the thin factorization $A = \widehat{Q} \widehat{R}$ into the normal equation $A\tp A \bx = A\tp \bb$ and simplifying, we reveal the connection between QR factorization and the LLS approximation.

**** Least Squares Using QR Factorization {{{cont}}}
***** Summary: Algorithm for LLS Approximation
If $A$ has rank $n$, the normal equation  $A\tp A\bx = A\tp \bb$ is consistent and is equivalent to $\widehat{R}\bx = \widehat{Q}\tp \bb$.
 1. Factor $A = \wh{Q}\wh{R}$.
 2. Let $\bz = \wh{Q}^\rT \bb$.
 3. Solve $\wh{R}\bx = \bz$ for $\bx$ using backward substitution.

**** Least Squares Using QR Factorization {{{cont}}}
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
function x = lsqrfact(A,b)
% LSQRFACT x = lsqrfact(A,b)
% Solve linear least squares by QR factorization
% Input:
%   A    coefficient matrix (m-by-n, m>n)
%   b    right-hand side (m-by-1)
% Output:
%   x    minimizer of || b - Ax || (2-norm)
    [Q,R] = qr(A,0);            % thin QR fact.
    z = Q'*b;
    x = backsub(R,z);
end
#+END_SRC

** DONE Computing QR Factorization
:PROPERTIES:
:EXPORT_FILE_NAME: 19-computing-qr
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Householder Transformation
**** Motivation
***** Problem
Given $\bz \in \RR^m$, find an orthogonal matrix $H \in \RR^{m \times m}$ such that $H\bz$ is nonzero only in the first element.
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - Since orthogonal matrices preserve the 2-norm, $H$ must satisfy
   \begin{equation*}
     H\bz =
     \begin{bmatrix}
       \pm \Norm{\bz}{2} \\ 0 \\ \vdots \\ 0
     \end{bmatrix}
     = \pm \Norm{\bz}{2} \be_1.
   \end{equation*}
 - The *Householder transformation matrix* $H$ defined by
   \[
     H = I - 2 \frac{\bv \bv\tp}{\bv\tp \bv},
     \quad\text{where $\bv = \pm \Norm{\bz}{2}\be_1 - \bz$,}
   \]
   solves the problem. See Theorem [[thm:householder]] on the next slide.

**** Properties of Householder Transformation
***** @@latex:  @@                                                               :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
# #+LATEX: \label{thm:householder}
<<thm:householder>>
Let $\bv = \Norm{\bz}{2}\be_1 - \bz$ and let $H$ be the Householder transformation defined by
\[
  H = I - 2 \frac{\bv \bv\tp}{\bv\tp \bv}.
\]
Then
 1. $H$ is symmetric;
 2. $H$ is orthogonal;
 3. $H\bz = \Norm{\bz}{2}\be_1$.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - $H$ is invariant under scaling of $\bv$.
 - If $\Norm{\bv}{2} = 1$, then $H = I - 2\bv \bv\tp$.

**** Geometry Behind Householder Transformation
The Householder transformation matrix $H$ is the reflector across $\langle \bv \rangle^{\perp}$.
\vs

***** col1                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
From any $\bz$ to the ``mirror'':
\[
  \mathbf{w} = - \frac{\bz^\rT \bv}{\sqrt{\bv^\rT \bv}} \cdot
  \frac{\bv}{\sqrt{\bv^\rT\bv}} = - \bv
  \frac{\bz^\rT\bv}{\bv^\rT\bv} \,.
\]
From any $\bz$ to its reflection:
\[
  H\bz - \bz = -2 \bv \frac{\bz^\rT\bv}{\bv^\rT\bv} \,.
\]
***** col2                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+ATTR_LATEX: :width 0.85\linewidth
[[../img/householder.pdf]]

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs

Thus, for any $\bz$,
\[
  H\bz = \bz -2 \bv \frac{\bz^\rT\bv}{\bv^\rT\bv} = \left(I - 2\frac{\bv\bv^\rT}{\bv^\rT\bv}\right) \bz
  \quad \Longrightarrow \quad
  H = I - 2 \frac{\bv\bv^\rT}{\bv^\rT\bv} \,.
\]

*** QR Factorization Algorithm
**** QR Factorization Algorithm via Triangularization
 - The Gram-Schmidt orthogonalization (thin QR factorization) is unstable in floating-point calculations.
 - *Stable alternative:* Find orthogonal matrices $H_1, H_2, \dots, H_n$ so that
   \[
     \underbrace{H_n H_{n-1} \cdots H_2 H_1}_{=: Q^\rT} A = R \,.
   \]
   introducing zeros one column at a time below diagonal terms. Householder matrices will do.
 - As a product of orthogonal matrices, $Q^\rT$ is also orthogonal and so $(Q^\rT)^{-1} = Q$. Therefore,
    \[
      A = QR \,.
    \]

**** MATLAB Implementation: =MYQR=
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
function [Q, R] = myqr(A)
  [m, n] = size(A);
  A0 = A;
  Q = eye(m);
  for j = 1:min(m,n)
      Aj = A(j:m, j:n);
      z = Aj(:, 1);
      v = z + sign0(z(1))*norm(z)*eye(length(z), 1);
      Hj = eye(length(v)) - 2/(v'*v) * v*v';
      Aj = Hj*Aj;
      H = eye(m);
      H(j:m, j:m) = Hj;
      Q = Q*H;
      A(j:m, j:n) = Aj;
  end
  R = A;
end
#+END_SRC

**** MATLAB Implementation: =MYQR= {{{cont}}}
(continued from the previous page)
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
% local function
function y = sign0(x)
  y = ones(size(x));
  y(x < 0) = -1;
end
#+END_SRC

 - The MATLAB command =qr= works similar to, but more efficiently than, this.
 - The function finds the factorization in $\sim (2mn^2 - n^3/3)$ flops asymptotically.

**** Which Reflector Is Better?
Recall:
\[
  H = I - 2 \frac{\bv \bv\tp}{\bv\tp \bv},
  \quad\text{where $\bv = \pm \Norm{\bz}{2} \be_1 - \bz$,}
\]
\vs

In =myqr.m=, the statement
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
 v = z + sign0(z(1))*norm(z)*eye(length(z), 1);
#+END_SRC
defines $\bv$ slightly differently, namely,
\[
  \bv = \bz \pm \Norm{\bz}{2} \be_1.
\]
This does not cause any difference since $H$ is invariant under scaling of $\bv$; see p. \pageref{thm:householder}.
**** Which Reflector Is Better? {{{cont}}}
The sign of $\pm \Norm{\bz}{2}$ is chosen so as to avoid possible catastrophic cancellation in forming $\bv$:
\begin{equation*}
  \bv =
  \begin{bmatrix}
    z_1 \\ z_2 \\ \vdots \\ z_m
  \end{bmatrix}
  +
  \begin{bmatrix}
    \pm \Norm{\bz}{2} \\ 0 \\ \vdots \\ 0
  \end{bmatrix}
  =
  \begin{bmatrix}
    z_1 \pm \Norm{\bz}{2} \\ z_2 \\ \vdots \\ z_m
  \end{bmatrix}
\end{equation*}
Subtractive cancellation may arise when $z_1 \approx \pm \Norm{\bz}{2}$.
 - if $z_1 > 0$, use $z_1 + \Norm{\bz}{2}$;
 - if $z_1 < 0$, use $z_1 - \Norm{\bz}{2}$;
 - if $z_1 = 0$, either works.

\vfill
#+BEGIN_QUOTE
For numerical stability, it is desirable to reflect $\bz$ to the vector $s \Norm{\bz}{2} \be_1$ that is not too close to $\bz$ itself. (Trefethen & Bau)
#+END_QUOTE

*** Appendix: Projection and Reflection
**** Projection and Reflection Operators
Let $\bu, \bv \in \RR^m$ be nonzero vectors.
 - Projection of $\bu$ onto $\langle \bv \rangle = \text{span}(\bv)$:
   \[
   \frac{\bv\tp\bu}{\bv\tp\bv} \bv = \underbrace{\left( \frac{\bv\bv\tp}{\bv\tp\bv} \right)}_{=:P} \bu =: P\bu.
   \]
 - Projection of $\bu$ onto $\langle \bv \rangle^{\perp}$, the orthogonal complement of $\langle \bv \rangle$:
   \[
   \bu - \frac{\bv\tp\bu}{\bv\tp\bv} \bv  = \left( I - \frac{\bv\bv\tp}{\bv\tp\bv} \right) \bu =: (I - P)\bu.
   \]

# *Question.* How about the reflection of $\bu$ across $\langle \bv \rangle^\perp$?
 - Reflection of $\bu$ across $\langle \bv \rangle^\perp$:
   \[
   \bu - 2\frac{\bv\tp\bu}{\bv\tp\bv} \bv  = \left( I - 2\frac{\bv\bv\tp}{\bv\tp\bv} \right) \bu =: (I - 2P)\bu.
   \]

**** Projection and Reflection Operators {{{cont}}}
*Summary:* for given $\bv \in \RR^m$, a nonzero vector, let
\[
P = \frac{\bv\bv\tp}{\bv\tp\bv} \in \RR^{m \times m}.
\]
Then the following matrices carry out geometric transformations
 - Projection onto $\langle \bv \rangle$: $P$
 - Projection onto $\langle \bv \rangle^\perp$: $I - P$
 - Reflection across $\langle \bv \rangle^\perp$: $I - 2P$

*Note.* If $\bv$ were a unit vector, the definition of $P$ simplifies to $P = \bv \bv\tp$.

*** Appendix: Gram-Schmidt Orthogonalization
**** The Gram--Schmidt Procedure
***** Problem: Orthogonalization                                                   :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
Given $\ba_1, \ldots, \ba_n \in \RR^m$, construct orthonormal vectors $\bq_1, \ldots, \bq_n \in \RR^m$ such that
\[
  \mathrm{span}\left\{ \ba_1, \ldots, \ba_k \right\}
  = \mathrm{span} \left\{ \bq_1, \ldots, \bq_k \right\},
  \quad \text{for any $k \in \NN[1,n]$.\vs}
\]
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
#+LATEX: \begingroup \small
 - *Strategy.* At the $j\text{th}$ step, find a unit vector $\bq_j \in \mathrm{span}\{\ba_1, \ldots, \ba_j\}$ that is orthogonal to $\bq_1, \ldots, \bq_{j-1}$.
 - *Key Observation.* The vector $\bv_j$ defined by
   \[
     \bv_j = \ba_j - \left( \bq_1\tp \ba_j \right) \bq_1 - \left( \bq_2\tp \ba_j \right) \bq_2 - \cdots - \left( \bq_{j-1}\tp \ba_j \right) \bq_{j-1}
   \]
  satisfies the required properties.
#+LATEX: \endgroup

**** GS Algorithm
***** col1                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.7
:END:
#+LATEX: \begingroup \small
The Gram--Schmidt iteration is outlined below:
\begin{equation*}
  \begin{aligned}
    \bq_1 & = \frac{\ba_1}{r_{11}}, \\
    \bq_2 & = \frac{\ba_2 - r_{12}\bq_1}{r_{22}}, \\
    \bq_3 & = \frac{\ba_3 - r_{13}\bq_1 - r_{23}\bq_2}{r_{33}}, \\
    & \vdots \\
    \bq_n & = \frac{\ba_n - \sum_{i=1}^{n-1}r_{in}\bq_i}{r_{nn}},
  \end{aligned}
\end{equation*}
where
\[
  r_{ij} =
  \begin{dcases*}
    \bq_i\tp \ba_j, & if $i \neq j$ \\
    \pm \Norm[3]{\ba_j - \sum_{k=1}^{j-1}r_{kj} \bq_k}{2}, & if $i =j$
  \end{dcases*}.
\]
#+LATEX: \endgroup

**** GS Procedure and Thin QR Factorization
 - The GS algorithm, written as a matrix equation, yields a *thin QR factorization*:
   #+LATEX: \begingroup \small
   \begin{equation*}
     A =
     \underbrace{
       \begin{bmatrix}[c|c|c]
         && \\
         && \\
         && \\
         && \\
         \ba_1 & \cdots & \ba_n \\
         && \\
         && \\
         && \\
         &&
       \end{bmatrix}
     }_{A}
     =
     \underbrace{
       \begin{bmatrix}[c|c|c]
         && \\
         && \\
         && \\
         && \\
         \bq_1 & \cdots & \bq_n \\
         && \\
         && \\
         && \\
         &&
       \end{bmatrix}
     }_{\widehat{Q}}
     \underbrace{
       \begin{bmatrix}
         r_{11} & r_{12} & \cdots & r_{1n} \\
         0 & r_{22} & \cdots & r_{2n} \\
         \vdots & & \ddots & \vdots \\
         0 & 0 & \cdots & r_{nn}
       \end{bmatrix}
     }_{\widehat{R}}
     = \widehat{Q} \widehat{R}
   \end{equation*}
   #+LATEX: \endgroup
 - While it is an important tool for theoretical work, the GS algorithm is numerically unstable.

** Orthogonal Triangularization by Hand: A Complete Worked Example
Consider
\begin{equation*}
A =
\begin{bmatrix}[rrr]
  1 & -2 & 3.5 \\
  1 & 3 & -0.5 \\
  1 & 3 & 2.5 \\
  1 & -2 & 0.5
\end{bmatrix}
\end{equation*}

Let $\bz_1$ be the first column of $A$ and set
\begin{equation*}
  \bv_1 = \bz_1 - \sign(z_{11}) \norm{\bz_1} \be_1
  =
  \begin{bmatrix}
    1 \\ 1 \\ 1 \\ 1
  \end{bmatrix}
  -2
  \begin{bmatrix}
    1 \\ 0 \\ 0 \\ 0
  \end{bmatrix}
  =
  \begin{bmatrix}[r]
    -1 \\ 1 \\ 1 \\ 1
  \end{bmatrix}.
\end{equation*}
This gives the following Householder matrix
\begin{equation*}
  H_1 = I - 2 \frac{\bv_1 \bv_1\tp}{\bv_1\tp \bv_1}
  = \frac{1}{2}
  \begin{bmatrix}[rrrr]
    1 & 1 & 1 & 1 \\
    1 & 1 & -1 & -1 \\
    1 & -1 & 1 & -1 \\
    1 & -1 & -1 & 1
  \end{bmatrix},
\end{equation*}
and left-multiplying $A$ by $H_1$ yields
\begin{equation*}
  H_1 A =
  \begin{bmatrix}[rrr]
    2 & 1 & 3 \\
    0 & 0 & 0 \\
    0 & 0 & 3 \\
    0 & -5 & 1
  \end{bmatrix}.
\end{equation*}

Now let $\bz_2$ be the first column of the 3-by-2 submatrix of $H_1 A$ obtained by removing its first row and first column, that is,
\begin{equation*}
  \bz_2 =
  \begin{bmatrix}[r]
    0 \\ 0 \\ -5
  \end{bmatrix}.
\end{equation*}
Now set
\begin{equation*}
  \bv_2 = \bz_2 - \sign(\bz_{21}) \norm{\bz_2} \be_1
  =
  \begin{bmatrix}[r]
    -5 \\ 0 \\ -5
  \end{bmatrix}.
\end{equation*}
(Note the slight abuse of notation above where $\be_1$ now denotes $\be_1 = [1, 0, 0]\tp \in \RR^3$.) This gives
\begin{equation*}
  \widetilde{H}_2 = I - 2 \frac{\bv_2 \bv_2\tp}{\bv_2\tp \bv_2} =
  \begin{bmatrix}[rrr]
    0 & 0 & -1 \\ 0 & 1 & 0 \\ -1 & 0 & 0
  \end{bmatrix},
\end{equation*}
and, using this, we construct the second Householder transformation
\begin{equation*}
  H_2 =
  \begin{bmatrix}
    1 & 0 \\ 0 & \widetilde{H}_2
  \end{bmatrix}
  =
  \begin{bmatrix}[rrrr]
    1 & 0 & 0 & 0 \\ 0 & 0 & 0 & -1 \\ 0 & 0 & 1 & 0 \\ 0 & -1 & 0 & 0
  \end{bmatrix},
\end{equation*}
which yields
\begin{equation*}
  H_2 H_1 A =
  \begin{bmatrix}[rrr]
    2 & 1 & 3 \\ 0 & 5 & -1 \\ 0 & 0 & 3 \\ 0 & 0 & 0
  \end{bmatrix},
\end{equation*}
an upper triangular matrix; call it $R$. Then $A = (H_2 H_1)^{-1} R =: QR$. Note that
\[
  Q = (H_2 H_1)^{-1} = H_1^{-1} H_2^{-1} = H_1\tp H_2\tp = H_1 H_2,
\]
where the third equality is because Householder transformations are orthogonal matrices, and the last equality is because Householder transformations are symmetric. In conclusion,
\begin{equation*}
  Q = \frac{1}{2}
  \begin{bmatrix}[rrrr]
    1 & -1 & 1 & -1 \\
    1 & 1 & -1 & -1 \\
    1 & 1 & 1 & 1 \\
    1 & -1 & -1 & 1
  \end{bmatrix}
  \quad\text{and}\quad
  R =
  \begin{bmatrix}[rrr]
    2 & 1 & 3 \\ 0 & 5 & -1 \\ 0 & 0 & 3 \\ 0 & 0 & 0
  \end{bmatrix}.
\end{equation*}

* DONE Chapter 4: Roots of Nonlinear Equations
** DONE Rootfinding Problem
:PROPERTIES:
:EXPORT_FILE_NAME: 20-rootfinding
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

% \begin{frame}{Contents}
%   \tableofcontents
% \end{frame}
#+END_EXPORT
CLOSED: [2021-03-08 Mon 21:29]
*** Notes                                                                           :noexport:
 - http://ranger.uta.edu/~huber/cse4345/Notes/
 - FNC 4.1
 - LM 13.1

*** Nonlinear Rootfinding
**** Problem Statement
***** Rootfinding Problem
Given a continuous scalar function of a scalar variable, find a real number $r$ such that $f(r) = 0$.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - $r$ is a *root* of the function $f$.
 - The formulation $f(x) = 0$ is general enough; /e.g./, to solve $g(x) = h(x)$, set $f = g- h$ and find a root of $f$.

**** Iterative Methods
 - Unlike the earlier linear problems, the root cannot be produced in a finite number of operations.
 - Rather, a sequence of approximations that formally converge to the root is pursued.

\vs

*Iteration Strategy for Rootfinding.* To find the root of $f$:
 1. Start with an initial iterate, say $x_0$.
 2. Generate a sequence of iterates $x_1, x_2, \ldots$ using an /iteration algorithm/ of the form
    \[
    x_{k+1} = g(x_k), \quad k = 0, 1, \ldots
    \]
 3. Continue the iteration process until you find an $x_i$ such that $f(x_i) = 0$. (In practice, continue until some member of the sequence seems to be ``good enough''.)

**** MATLAB's =FZERO=
=fzero= is MATLAB's general purpose rootfinding tool.

\vs

*Syntax:*
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
 x_zero = fzero( <function>, <initial iterate> )
 x_zero = fzero( <function>, <initial interval> )
 [x_zero, fx_zero] = ....
#+END_SRC

**** Example
The roots of $J_m$, a Bessel function of the first kind, is found by
\vs

***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.48
:END:

#+LATEX: \setlength{\leftmargini}{0.5em}
 - Plot the function.
 - Find approximate locations of roots.
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
 J3 = @(x) besselj(3,x);
 fplot(J3,[0 20])
 grid on
 guess = [6,10,13,16,19];
#+END_SRC

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.52
:END:
#+LATEX: \vspace{0pt}
#+ATTR_LATEX: :width 0.8\linewidth
[[../img/besselfunc.eps]]

**** Example {{{cont}}}
***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.48
:END:

#+LATEX: \setlength{\leftmargini}{0.5em}
 - Then use =fzero= to locate the roots:
#+ATTR_LATEX: :options style=matlab, basicstyle=\footnotesize\ttfamily
#+BEGIN_SRC matlab
 omega = zeros(size(guess));
 for j = 1:length(guess)
   omega(j) = fzero(J3,guess(j));
 end
 hold on
 plot(omega,J3(omega),'ro')
#+END_SRC

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.52
:END:
#+ATTR_LATEX: :width 0.8\linewidth
[[../img/besselroots.eps]]

**** Conditioning
 - Sensitivity of the rootfinding problem can be measured in terms of the condition number:
   \[
     \text{(absolute condition number)}
     = \frac{\abs{\text{abs. error in output}}}{\abs{\text{abs. error in input}}},
   \]
   where, in the context of finding roots of $f$,
   #+LATEX: \begin{multicols}{2}
   - input: $f$ (function)
   - output: $r$ (root)
   #+LATEX: \end{multicols}

 - Denote the changes by:
   - error/change in input: $\epsilon g$, where $\epsilon > 0$ is small @@latex: \hfill @@ ($f \mapsto f + \epsilon g$)
   - error/change in output: $\Delta r$ @@latex: \hfill @@ ($r \mapsto r + \Delta r$)

**** Conditioning {{{cont}}}
 - The /perturbed equation/
   \[
   f(r + \Delta r) + \epsilon g(r + \Delta r) = 0
   \]
   is linearized to (Taylor expansion)
   \[
   f(r) + f'(r) \Delta r + g(r) \epsilon  +  g'(r) \epsilon \Delta r \approx 0,
   \]
   ignoring $O((\Delta r)^2)$ terms[fn::That is, terms involving $(\Delta r)^2$ and higher powers of $\Delta r$].

 - Since $f(r) = 0$, we solve for $\Delta r$ to get
   \[
   \Delta r
   \approx -\epsilon \frac{g(r)}{f'(r) + \epsilon g'(r)}
   \approx -\epsilon \frac{g(r)}{f'(r)},
   \]
   for small $\epsilon$ compared with $f'(r)$.

**** Conditioning {{{cont}}}
 - Therefore, the absolute condition number of the rootfinding problem is
   \[
   \kappa_{f \mapsto r} = \frac{1}{\abs{f'(r)}},
   \]
   which implies that the problem is highly sensitive whenever $f'(r) \approx 0$.

 - In other words, if $|f'|$ is small at the root, a computed /root estimate/ may involve large errors.

**** Residual and Backward Error
 - Without knowing the exact root, we cannot compute the error.
 - But the *residual* of a root estimate $\tilde{r}$ can be computed:
   \[
   \text{(residual)} = f(\tilde{r}).
   \]
 - Small residual /might/ be associated with a small error.
 - The residual $f(\tilde{r})$ is the /backward error/ of the estimate.

**** Multiple Roots
***** Multiplicity of Roots                                                   :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
Assume that $r$ is a root of the differentiable function $f$. Then if
\[
0 = f(r) = f'(r) = \cdots = f^{(m-1)}(r)
\quad\text{but}\quad
f^{(m)}(r) \neq 0,
\]
we say that $f$ has a root of *multiplicity* $m$ at $r$.
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - We say that $f$ has a *multiple root* at $r$ if the multiplicity is greater than 1.
 - A root is called *simple* if its multiplicity is 1.
 - If $r$ is a multiple root, the condition number is infinite.
 - Even if $r$ is a simple root, we expect difficulty in numerical computation if $f'(r) \approx 0$.

** DONE Fixed Point Iteration
:PROPERTIES:
:EXPORT_FILE_NAME: 21-fpi
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

% \begin{frame}{Contents}
%   \tableofcontents
% \end{frame}
#+END_EXPORT

*** Notes                                                                           :noexport:
 - FNC 4.2-4.4
 - LM 13.1

*** Fixed Point Iteration
**** Fixed Point
***** Fixed Point                                                             :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
The real number $r$ is a *fixed point* of the function $g$ if $g(r) = r$.
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
 - The rootfinding problem $f(x) = 0$ can always be written as a fixed point problem $g(x)=x$ by, /e.g./, setting[fn::This is not the only way to transform the rootfinding problem. More on this later.]
   \[
   g(x) = x - f(x).
   \]
 - The fixed point problem is true at, and only at, a root of $f$.

**** Fixed Point Iteration
A fixed point problem $g(x) = x$ naturally provides an iteration scheme:
\begin{equation*}
\left\{\;
\begin{aligned}
x_0 & = \text{initial guess} \\
x_{k+1} & = g(x_k), \quad k = 0, 1, 2, \ldots.
\end{aligned}
\right.
\tag{fixed point iteration}
\end{equation*}

\vs
 - The sequence $\{x_k\}$ may or may not converge as $k \to \infty$.
 - If $g$ is continuous and $\{x_k\}$ converges to a number $r$, then $r$ is a fixed point of $g$.
   \[
   g(r)
   = g\left( \lim_{k \to \infty} x_k \right)
   = \lim_{k \to \infty} g(x_k)
   = \lim_{k \to \infty} x_{k+1} = r.
   \]

**** Fixed Point Iteration Algorithm
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
 function x = fpi(g, x0, n)
 % FPI x = fpi(g, x0, n)
 % Computes approximate solution of g(x)=x
 % Input:
 %   g    function handle
 %   x0   initial guess
 %   n    number of iteration steps
     x = x0;
     for k = 1:n
         x = g(x);
     end
 end
#+END_SRC

**** Examples
 - To find a fixed point of $g(x) = 0.3 \cos(2x)$ near $0.5$ using =fpi=:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 g = @(x) 0.3*cos(2*x);
 xc = fpi(g,0.5,20)
   #+END_SRC

   #+BEGIN_EXAMPLE
 xc = 0.260266319627758
   #+END_EXAMPLE

**** Not All Fixed Point Problems Are The Same
The rootfinding problem $f(x) = x^3 + x - 1 = 0$ can be transformed to various fixed point problems:
 - $g_1(x) = x - f(x) = 1 - x^3$
 - $g_2(x) = \sqrt[3]{1 - x}$
 - $\ds g_3(x) = \frac{1+2x^3}{1+3x^2}$

\vs

Note that all $g_j(x) = x$ are equivalent to $f(x)=0$. However, not all these find a fixed point of $g$, that is, a root of $f$ on the computer.

\vs
*Exercise.* Run =fpi= with $g_j$ and $x_0 = 0.5$. Which fixed point iterations converge?

**** Geometry of Fixed Point Iteration
The following script[fn::Modified from FNC.] finds a root of $f(x) = x^2 - 4x + 3.5$ via FPI.

***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :options style=matlab, basicstyle=\scriptsize\ttfamily
#+BEGIN_SRC matlab
 f = @(x) x.^2 - 4*x + 3.5;
 g = @(x) x - f(x);
 fplot(g, [2 3], 'r');
 hold on
 plot([2 3], [2 3], 'k--')
 x = 2.1;
 y = g(x);
 for k = 1:5
     arrow([x y], [y y], 'b');
     x = y; y = g(x);
     arrow([x x], [x y], 'b');
 end
#+END_SRC

#+RESULTS:
#+begin_example

clf
f = @(x) x.^2 - 4*x + 3.5;
g = @(x) x - f(x);
fplot(g, [2 3], 'r');
hold on
plot([2 3], [2 3], 'k--')
x = 2.1;
y = g(x);
for k = 1:5
     arrow([x y], [y y], 'b');
     x = y; y = g(x);
     arrow([x x], [x y], 'b');
 end

axis equal, grid on
xlabel('x', 'fontsize', 25)
ylabel('y', 'fontsize', 25)
print -depsc 'fpi_conv'
'org_babel_eoe'
ans =
    'org_babel_eoe'
#+end_example

\vs

#+LATEX: \begingroup\small
Note the line segments spiral in towards the fixed point.
#+LATEX: \endgroup

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
#+ATTR_LATEX: :width 0.8\linewidth
[[../img/fpi_conv.eps]]

**** Geometry of Fixed Point Iteration {{{cont}}}
However, with a different starting point, the process does not converge.

***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+ATTR_LATEX: :options style=matlab, basicstyle=\scriptsize\ttfamily
#+BEGIN_SRC matlab
 clf
 fplot(g, [1 2], 'r');
 hold on
 plot([1 2], [1 2], 'k--'),
 ylim([1 2])
 x = 1.3; y = g(x);
 for k = 1:5
     arrow([x y], [y y], 'b');
     x = y; y = g(x);
     arrow([x x], [x y], 'b');
 end
#+END_SRC

#+RESULTS:
#+begin_example
clf
fplot(g, [1 2], 'r');
hold on
plot([1 2], [1 2], 'k--'),
ylim([1 2])
x = 1.3; y = g(x);
for k = 1:5
     arrow([x y], [y y], 'b');
     x = y; y = g(x);
     arrow([x x], [x y], 'b');
 end

axis equal, grid on
xlabel('x', 'fontsize', 25)
ylabel('y', 'fontsize', 25)
print -depsc 'fpi_div'
'org_babel_eoe'
ans =
    'org_babel_eoe'
#+end_example

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
#+ATTR_LATEX: :width 0.8\linewidth
[[../img/fpi_div.eps]]

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+LATEX: \vfill
#+LATEX: \begingroup\tiny
Custom function: =arrow = @(p1, p2, varargin) quiver(p1(1), p1(2), p2(1)-p1(1), p2(2)-p1(2), 0, varargin{:})=
#+LATEX: \endgroup

**** Series Analysis
#+LATEX: \begingroup\small
Let $\epsilon_k = x_k - r$ be the sequence of errors.
\vs
 - The iteration formula $x_{k+1} = g(x_k)$ can be written as
   \begin{align*}
     \epsilon_{k+1} + r
     & = g(\epsilon_k + r) \\
     & = g(r) + g'(r) \epsilon_k + \frac{1}{2} g''(r) \epsilon_k^2 + \cdots, \tag{Taylor series}
   \end{align*}
   implying
   \[
     \epsilon_{k+1} = g'(r)\epsilon_k + O(\epsilon_k^2)
   \]
   assuming sufficient regularity of $g$.
 - Neglecting the second-order term, we have $\epsilon_{k+1} \approx g'(r) \epsilon_k$, which is satisfied if $\epsilon_k \approx C \left[ g'(r) \right]^k$ for sufficiently large $k$.
 - Therefore, the iteration converges if $\abs{g'(r)} < 1$ and diverges if $\abs{g'(r)}> 1$.
#+LATEX: \endgroup

**** Note: Rate of Convergence
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:
<<rate_of_conv>>
***** Linear Convergnece                                                      :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
Suppose $\lim_{k \to \infty} x_k = r$ and let $\epsilon_k = x_k - r$, the error at step $k$ of an iteration method. If
\[
\lim_{k \to \infty} \frac{\abs{\epsilon_{k+1}}}{\abs{\epsilon_k}} = \sigma < 1,
\]
the method is said to obey *linear convergence* with rate $\sigma$.
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
***** col1                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
*Note.* In general, say
\[
\lim_{k \to \infty} \frac{\abs{\epsilon_{k+1}}}{\abs{\epsilon_k}^p} = \sigma
\]
for some $p \ge 1$ and $\sigma > 0$.
***** col2                                                                           :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
#+LATEX: \vspace{-1em}
 - If $p = 1$ and
   - $\sigma = 1$, the convergence is \textit{sublinear};
   - $0 < \sigma < 1$, the convergence is \textit{linear};
   - $\sigma = 0$, the convergence is \textit{superlinear}.
 - If $p = 2$, the convergence is \textit{quadratic};
 - If $p = 3$, the convergence is \textit{cubic}, \ldots
**** COMMENT Terminology: Local Convergence
***** @@latex:  @@                                                            :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
An iterative method is called *locally convergent* to $r$ if the method converges to $r$ for initial guess sufficiently close to $r$.
**** Convergence of Fixed Point Iteration
***** Convergence of FPI                                                         :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Assume that $g$ is continuously differentiable, that $g(r) = r$, and that $\sigma = \abs{g'(r)} < 1$. Then the fixed point iterates $x_k$ generated by
\[
x_{k+1} = g(x_k), \quad k = 1, 2, \ldots,
\]
converge linearly with rate $\sigma$ to the fixed point $r$ for $x_0$ sufficiently close to $r$.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
 \vs
 In the previous example with $g(x) = x - f(x) = -x^2 + 5x - 3.5$:
  - For the first fixed point, near 2.71, we get $g'(r) \approx -0.42$ (convergence);
  - For the second fixed point, near 1.29, we get $g'(r) \approx 2.42$ (divergence).

\vfill
#+LATEX: \begingroup\footnotesize
#+LATEX: \hrule
#+LATEX: \vspace{0.5em}
*Note.* An iterative method is called *locally convergent* to $r$ if the method converges to $r$ for initial guess sufficiently close to $r$.
#+LATEX: \endgroup

**** Contraction Maps

***** Lipschitz Condition
A function $g$ is said to satisfy a *Lipschitz condition* with constant $L$ on the interval $S \subset \RR$ if
\[
\abs{g(s) - g(t)} \le L \abs{s - t}
\quad\text{for all $s, t \in S$.}
\]

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
 - A function satisfying the Lipschitz condition is continuous on $S$.
 - If $L<1$, $g$ is called a *contraction map*.

**** When Does FPI Succeed?

***** Contraction Mapping Theorem
Suppose that $g$ satisfies Lipschitz condition on $S$ with $L < 1$, /i.e./, $g$ is a contraction map on $S$. Then $S$ contains exactly one fixed point $r$ of $g$. If $x_1, x_2, \ldots$ are generated by the fixed point iteration $x_{k+1} = g(x_k)$, and $x_1, x_2, \ldots$ all lie in $S$, then
\[
\abs{x_k - r} \le L^{k-1} \abs{x_1 - r}, \quad k > 1.
\]

** DONE Newton's Method
:PROPERTIES:
:EXPORT_FILE_NAME: 22-newton-iteration
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

% \begin{frame}{Contents}
%   \tableofcontents
% \end{frame}
#+END_EXPORT
*** Newton's Method
**** Newton's Method
To find the root of $f$:
#+LATEX: \begingroup\small
***** Newton's Method (Algorithm)
 - Begin at the point $(x_0, f(x_0))$ on the curve and draw the tangent line at the point using the slope $f'(x_0)$:
   \[
   y = f(x_0) + f'(x_0) (x - x_0).
   %\frac{y- f(x_0)}{x-x_0} = f'(x_0) \,.
   \]
 - Find the $x\text{-intercept}$ of the line and call it $x_1$:
   \[
     x_1 = x_0 - \frac{f(x_0)}{f'(x_0)} \,.
   \]
 - Continue this procedure to find $x_2, x_3, \dots$ until the sequence converges to the root.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
*General iterative formula:*
\[
  x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}
  \quad \text{for $k = 0, 1, 2, \dots$}
  \tag{$\star$}
\]

#+LATEX: \endgroup

**** Newton's Method: Illustration
#+ATTR_LATEX: :width 0.8\linewidth
[[../img/newton_method.pdf]]

**** Series Analysis
:PROPERTIES:
:BEAMER_opt: shrink=0
:END:
#+LATEX: \begingroup\small
Let $\epsilon_k = x_k - r$, $k = 1, 2, \ldots$, where $r$ is the limit of the sequence and $f(r)=0$.

\vs

Substituting $x_k = r + \epsilon_k$ into the iterative formula ($\star$):
\[
\epsilon_{k+1} = \epsilon_k - \frac{f(r+\epsilon_k)}{f'(r+\epsilon_k)}.
\]

\vs

Taylor-expand $f$ about $x=r$ and simplify (assuming $f'(r) \neq 0$):
 \begin{align*}
   \epsilon_{k+1}
   & = \epsilon_k -
     \frac%
     {f(r) + \epsilon_k f'(r) + \frac{1}{2} \epsilon_k^2 f''(r) + O(\epsilon_k^3)}%
     {f'(r) + \epsilon_k f''(r) + O(\epsilon_k^2)} \\
   & = \epsilon_k - \epsilon_k \left[ 1 + \frac{1}{2} \frac{f''(r)}{f'(r)} \epsilon_k + O(\epsilon_k^2) \right]
     \left[ 1 + \frac{f''(r)}{f'(r)} \epsilon_k + O(\epsilon_k^2) \right]^{-1} \\
   & = \frac{1}{2} \frac{f''(r)}{f'(r)} \epsilon_k^2 + O(\epsilon_k^3).
 \end{align*}
#+LATEX: \endgroup

**** Series Analysis {{{cont}}}
 - Previous calculation shows that $\epsilon_{k+1} \approx C \epsilon_k^2$, eventually. Written differently,
   \[
   \abs{\epsilon_{k+1}}/\abs{\epsilon_k}^2 \to \text{(some positive number)},\;  \text{as $k \to \infty$}.
   \]
   that is, each Newton iteration roughly squares the previous error. This is *quadratic convergence*.
 - Alternately, note that
   \[
   \log \abs{\epsilon_{k+1}} \approx 2 \log \abs{\epsilon_k} + \text{(constant)},
   \]
   ignoring high-order terms. This means that the number of accurate digits[fn::We say that an iterate is *correct within $p$ decimal places* if the error is less than $0.5 \times 10^{-p}$.] approximately doubles at each iteration.

**** Convergence of Newton's Method
***** Quadratic Convergence of Newton's Method                                   :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Let $f$ be twice continuously differentiable and $f(r)=0$. If $f'(r) \neq 0$, then Newton's method is locally and quadratically convergent to $r$. The error $\epsilon_k = x_k - r$ at step $k$ satisfies
\[
\lim_{k \to \infty} \frac{ \abs{ \epsilon_{k+1} } }{ \abs{ \epsilon_k }^2 } = \abs{ \frac{f''(r)}{2f'(r)} }.
\]

**** Implementation
:PROPERTIES:
:BEAMER_opt: shrink=5
:END:
#+ATTR_LATEX: :options style=matlab, basicstyle=\footnotesize\ttfamily
#+BEGIN_SRC matlab
function x = newton(f,dfdx,x1)
% NEWTON   Newton's method for a scalar equation.
% Input:
%   f        objective function
%   dfdx     derivative function
%   x1       initial root approximation
% Output
%   x        vector of root approximations (last one is best)

% Operating parameters.
    funtol = 100*eps;  xtol = 100*eps;  maxiter = 40;

    x = x1;
    y = f(x1);
    dx = Inf;   % for initial pass below
    k = 1;

    while (abs(dx) > xtol) && (abs(y) > funtol) && (k < maxiter)
        dydx = dfdx(x(k));
        dx = -y/dydx;           % Newton step
        x(k+1) = x(k) + dx;

        k = k+1;
        y = f(x(k));
    end

    if k==maxiter, warning('Maximum number of iterations reached.'), end
end
#+END_SRC

**** Note: Stopping Criteria
#+LATEX: \begingroup\small
For a set tolerance, =TOL=, some example stopping criteria are:
 - Absolute error:
     \[
     \abs{x_{k+1} - x_k} < \mathtt{TOL}.
     \]
 - Relative error: (useful when the solution is not too close to zero)
   \[
   \frac{ \abs{x_{k+1} - x_k} }{ \abs{x_{k+1}} } < \mathtt{TOL}.
   \]
 - Hybrid:
   \[
   \frac{ \abs{x_{k+1} - x_k} }{ \max( \abs{x_{k+1}}, \theta ) } < \mathtt{TOL},
   \]
   for some $\theta > 0$.
 - Residual:
   \[
   \abs{f(x_k)} < \mathtt{TOL}.
   \]
Also useful to set a limit on the maximum number of iterations in case convergence fails.
#+LATEX: \endgroup

** DONE Interpolation-Based Methods
:PROPERTIES:
:EXPORT_FILE_NAME: 23-secant-method
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT
*** Secant Method
**** Secant Method
 - Newton's method requires calculation and evaluation of $f'(x)$, which may be challenging at times.

 - The most common alternative to such situations is the *secant method*.

 - The secant method replaces the instanteneous slope in Newton's method by the average slope using the last two iterates.

**** Secant Method {{{cont}}}
#+LATEX: \begingroup \small
***** Secant Method (Algorithm)
 - Begin with two initial iterates $x_{-1}$ and $x_0$; draw the secant line connecting $(x_{-1}, f(x_{-1}))$ and $(x_0, f(x_0))$:
   \[
   y = f(x_0) + \frac{f(x_0)-f(x_{-1})}{x_0 - x_{-1}} (x - x_0).
   \]
 - Find the $x\text{-intercept}$ of the line and call it $x_1$:
   \[
     x_1 = x_0 - f(x_0) \frac{x_0 - x_{-1}}{f(x_0)-f(x_{-1})} \,.
   \]
 - Continue this procedure to find $x_2, x_3, \dots$ until convergence is obtained.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
*General iterative formula:*
\[
  x_{k+1} = x_k - f(x_k)
  \frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})}
  \quad \text{for $k = 0, 1, 2, \dots$}
\]
#+LATEX: \endgroup

**** Secant Method: Illustration
#+ATTR_LATEX: :width 0.8\linewidth
[[../img/secant_method.pdf]]

**** Series Analysis
#+LATEX: \begingroup\small
Assume that the secant method converges to $r$ and $f'(r) \neq 0$. Let $\epsilon_k = x_k - r$ as before.

\vs

It can be shown that
\[
\abs{ \epsilon_{k+1} } \approx \abs{ \frac{f''(r)}{2f'(r)} } \abs{ \epsilon_{k} } \abs{ \epsilon_{k-1} },
\]
which implies that
\[
\abs{ \epsilon_{k+1} } \approx \abs{ \frac{f''(r)}{2f'(r)} }^{\alpha-1} \abs{ \epsilon_{k} }^\alpha,
\]
where
\[
\alpha = \frac{1+\sqrt{5}}{2} \approx 1.618,
\]
the /golden ratio/.

\vs

Therefore, the convergence of the secant method is *superlinear*; it lies between linearly and quadratically convergent methods.
#+LATEX: \endgroup

**** Series Analysis {{{cont}}}
*Exercise.* Confirm the statements in the previous page. Namely, show that
\vs
 a) The error $\epsilon_k$ satisfies the approximate equation
    \[
    \abs{ \epsilon_{k+1} } \approx \abs{ \frac{f''(r)}{2f'(r)} } \abs{ \epsilon_{k} } \abs{ \epsilon_{k-1} }.
    \]
 b) If in addition $\lim_{k \to \infty} \abs{ \epsilon_{k+1} } / \abs{ \epsilon_k }^\alpha$ exists and is nonzero for some $\alpha > 0$, then
    \[
    \abs{ \epsilon_{k+1} } \approx \abs{ \frac{f''(r)}{2f'(r)} }^{\alpha-1} \abs{ \epsilon_{k} }^\alpha, \quad \text{where }
    \alpha = \frac{1+\sqrt{5}}{2}.
    \]

**** Implementation
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:

#+ATTR_LATEX: :options style=matlab, basicstyle=\footnotesize\ttfamily
#+BEGIN_SRC matlab
function x = secant(f,x1,x2)
% SECANT   Secant method for a scalar equation.
% Input:
%   f        objective function
%   x1,x2    initial root approximations
% Output
%   x        vector of root approximations (last is best)

% Operating parameters.
    funtol = 100*eps;  xtol = 100*eps;  maxiter = 40;

    x = [x1 x2];
    dx = Inf;  y1 = f(x1);
    k = 2;  y2 = 100;

    while (abs(dx) > xtol) && (abs(y2) > funtol) && (k < maxiter)
        y2 = f(x(k));
        dx = -y2 * (x(k)-x(k-1)) / (y2-y1);   % secant step
        x(k+1) = x(k) + dx;

        k = k+1;
        y1 = y2;    % current f-value becomes the old one next time
    end

    if k==maxiter, warning('Maximum number of iterations reached.'), end
end
#+END_SRC

*** Appendix: Other Methods
**** Inverse Interpolation
The *inverse quadratic interpolation* (IQI) is a generalization of the secant method to parabolas.
 - Instead of using two most recent points (to determine a straight line), use three and obtain an quadratic interpolant.
 - The parabola of the form $y = p(x)$ may have zero, one, or two $x\text{-intercept(s)}$. So use the form $x = p(y)$, a parabola open sideways.
\vs
*Algorithm.*
 * Begin with three initial iterates $x_{-2}, x_{-1}, x_0$; find the parabola of the form $x=p(y)$ passing through the three points $(x_{-2}, f(x_{-2}))$, $(x_{-1}, f(x_{-1}))$, and $(x_{0}, f(x_{0}))$.
 * Find the $x\text{-intercept}$ of the parabola and call it $x_1$.
 * Continue the procedure to find $x_2, x_3, \ldots$ until convergence is obtained.
**** Inverse Interpolation {{{cont}}}
*General iterative formula:*
\[
x_{k+1} = x_{k} - \frac{ r(r-q)(x_k - x_{k-1}) + (1-r)s(x_k - x_{k-2}) }{(q-1)(r-1)(s-1)}, \quad\text{for $k=0, 1, 2, \ldots$},
\]
where
\[
q = \frac{ f(x_{k-2}) }{ f(x_{k-1}) },\quad
r = \frac{ f(x_{k}) }{ f(x_{k-1}) },\quad
s = \frac{ f(x_{k}) }{ f(x_{k-2}) }.
\]

\vs
Rather than deriving and implementing the formula, try using =polyfit= to perform the interpolation step.

**** Bisection Method: Bracketing a Root
The following is a corollary to the intermediate value theorem.
***** Existence of a Root                                                        :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Let $f$ be a continuous function on $[a,b]$, satisfying $f(a) f(b) < 0$. Then $f$ has a root between $a$ and $b$, that is, there exists a number $r \in (a,b)$ such that $f(r) = 0$.

**** Bisection Method {{{cont}}}
*Algorithm.*
 - Start with an interval $[a,b]$ where $f(a)f(b) \le 0$.
 - Bisect the interval into $[a,m] \cup [m,b]$ where $m = \ (a+b)/2$ is the midpoint.
 - Select the subinterval in which $f(x)$ changes signs, /i.e./, calculate $f(a)f(m)$ and $f(m)f(b)$, choose the nonpositive
  one, and update the values of $a$ and $b$.
 - Repeat the process until you get close enough to the solution.

**** Notes
Let $[a, b]$ be the initial interval and let $[a_k, b_k]$ be the interval after $k$ bisection steps.

 - The length of $[a_k, b_k]$ is $(b-a)/2^k$.
 - Using the midpoint $x_k = (a_k + b_k)/2$ as an estimate of the root $r$, note that
   \[
   \abs{\epsilon_k} = \abs{x_k - r} < \frac{b-a}{2^{k+1}}.
   \]
 - This accuracy is obtained by $k+2$ function evaluations.

**** Bisection Method: Pseudocode
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
 while <a NOT CLOSE ENOUGH TO b>
   m = (a + b)/2;
   fm = f(m);
   if sign(fa) ~= sign(fm)
     b = m;
     fb = fm;
   else
     a = m;
     fa = fm;
   end
 end
 x_zero = .5*(a + b);
#+END_SRC

** DONE Multidimensional Rootfinding
:PROPERTIES:
:EXPORT_FILE_NAME: 24-multidim-rootfinding
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

% \begin{frame}{Contents}
%   \tableofcontents
% \end{frame}
#+END_EXPORT
*** Notes                                                                           :noexport:
 - FNC 4.5
 - LM 13.1

*** Newton's Method for Nonlinear Systems
**** Multidimensional Rootfinding Problem
***** Rootfinding Problem: Vector Version
Given a continuous vector-valued function $\bff:\RR^n \to \RR^n$, find a vector $\br \in \RR^n$ such that $\bff(\br) = \bzero$.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:

\vs
The rootfinding problem $\mathbf{f}(\bx) = \bzero$ is equivalent to solving the /nonlinear/ system of $n$ scalar equations in $n$ unknowns:
\begin{align*}
f_1(x_1, \ldots, x_n) & = 0, \\
f_2(x_1, \ldots, x_n) & = 0, \\
& \vdots \\
f_n(x_1, \ldots, x_n) & = 0.
\end{align*}

**** Multidimensional Taylor Series
If $\bff$ is differentiable, we can write
\[
\bff(\bx + \bh) = \bff(\bx) + \bJ(\bx) \bh + O( \norm{\bh}^2 ),
\]
where $\bJ$ is the *Jacobian matrix* of $\bff$
\begin{equation*}
  \bJ(\bx) =
  \begin{bmatrix}
    \frac{\del f_1}{\del x_1} & \frac{\del f_1}{\del x_2} & \cdots & \frac{\del f_1}{\del x_n} \\
    \frac{\del f_2}{\del x_1} & \frac{\del f_2}{\del x_2} & \cdots & \frac{\del f_2}{\del x_n} \\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\del f_n}{\del x_1} & \frac{\del f_n}{\del x_2} & \cdots & \frac{\del f_n}{\del x_n} \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    \frac{\del f_i}{\del x_j}
  \end{bmatrix}_{i,j = 1, \ldots, n}.
\end{equation*}

\vs

 - The first two terms $\bff(\bx) + \bJ(\bx) \bh$ is the ``linear approximation'' of $\bff$ near $\bx$.
 - If $\bff$ is actually linear, /i.e./, $\bff(\bx) = A\bx - \bb$, then the Jacobian matrix is the coefficient matrix $A$ and the rootfinding problem $\bff(\bx) = \bzero$ is simply $A\bx = \bb$.

**** Example
<<nl_sys_example>>

#+LATEX: \begingroup\small
Let
#+LATEX: \vspace{-1.5em}
\begin{align*}
f_1 (x_1, x_2, x_3) & = -x_1 \cos(x_2) - 1,\\
f_2 (x_1, x_2, x_3) & = x_1 x_2 + x_3,\\
f_3 (x_1, x_2, x_3) & = e^{-x_3} \sin(x_1+x_2) + x_1^2 - x_2^2.
\end{align*}

Then
\begin{equation*}
  \bJ(\bx) =
  \begin{bmatrix}
    -\cos(x_2) & x_1 \sin(x_2) & 0 \\
    x_2 & x_1 & 1 \\
    e^{-x_3} \cos(x_1 + x_2) + 2x_1 & e^{-x_3} \cos(x_1 + x_2) - 2x_2 & -e^{-x_3} \sin(x_1+x_2)
  \end{bmatrix}.
\end{equation*}

*Exercise.* Write out the linear part of the Taylor expansion of
\[
f_1(x_1+h_1, x_2+h_2, x_3+h_3), \quad\text{near $(x_1, x_2, x_3)$.}
\]
#+LATEX: \endgroup
**** The Multidimensional Newton's Method
Recall the idea of Newton's method:
\vs

#+BEGIN_QUOTE
If finding a zero of a function is difficult, replace the function with a simpler approximation (/linear/) whose zeros are easier to find.
#+END_QUOTE

\vs
Applying the principle:
\vs
 - Linearize $\bff$ at the $k\text{th}$ iterate $\bx_k$:
   \[
   \bff(\bx) \approx L(\bx) = \bff(\bx_k) + \bJ(\bx_k)(\bx - \bx_k).
   \]
 - Define the next iterate $\bx_{k+1}$ by solving $L(\bx_{k+1}) = \bzero$:
   \[
   \bzero = \bff(\bx_k) + \bJ(\bx_k)(\bx - \bx_k)
   \quad\Longrightarrow\quad
   \bx_{k+1} = \bx_k - \left[ \bJ(\bx_k) \right]^{-1} \bff(\bx_k).
   \]
   Note that $\bJ^{-1} \bff$ plays the same role as $f/f'$ in the scalar Newton.

**** The Multidimensional Newton's Method {{{cont}}}
 #+LATEX: \begingroup\small
 - In practice, we do not compute $\bJ^{-1}$. Rather, the $k\text{th}$ Newton step $\bs_k = x_{k+1} - x_k$ is found by solving the square linear system
   \[
   \bJ(\bx_k) \bs_k = - \bff(\bx_k),
   \]
   which is solved using the backslash in MATLAB.
 - Suppose =f= and =J= are MATLAB functions calculating $\bff$ and $\bJ$, respectively. Then the Newton iteration is done simply by
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 % x is a Newton iterate (a column vector).
 % The following is the key fragment
 % inside Newton iteration loop.
 fx = f(x)
 s = -J(x)\fx;
 x = x + s;
   #+END_SRC
 - Since $\bff(x_k)$ is the residual and $\bs_k$ is the gap between two consecutive iterates at the $k\text{th}$ step, monitor their norms to determine when to stop iteration.
#+LATEX: \endgroup

**** Computer Illustration

1. Define $\bff$ and $\bJ$, either as anonymous functions or as function m-files.
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 f = @(x) [exp(x(2)-x(1)) - 2;
           x(1)*x(2) + x(3);
           x(2)*x(3) + x(1)^2 - x(2)];
 J = @(x) [-exp(x(2)-x(1)),exp(x(2)-x(1)), 0;
           x(2), x(1), 1;
           2*x(1), x(3)-1, x(2)];
   #+END_SRC

***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
2. [@2] Define an initial iterate =x=, say $\bx_0 = (0, 0, 0)\tp$.
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 x = [0 0 0]';
   #+END_SRC

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
3. [@3] Iterate.
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 for k = 1:7
     s = -J(x)\f(x);
     x = x + s;
 end
   #+END_SRC

**** Implementation
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:
#+ATTR_LATEX: :options style=matlab, basicstyle=\footnotesize\ttfamily
#+BEGIN_SRC matlab
function x = newtonsys(f,x1)
% NEWTONSYS   Newton's method for a system of equations.
% Input:
%   f        function that computes residual and Jacobian matrix
%   x1       initial root approximation (n-vector)
% Output
%   x        array of approximations (one per column, last is best)

% Operating parameters.
    funtol = 1000*eps;  xtol = 1000*eps;  maxiter = 40;

    x = x1(:);
    [y,J] = f(x1);
    dx = Inf;
    k = 1;

    while (norm(dx) > xtol) && (norm(y) > funtol) && (k < maxiter)
        dx = -(J\y);   % Newton step
        x(:,k+1) = x(:,k) + dx;

        k = k+1;
        [y,J] = f(x(:,k));
    end

    if k==maxiter, warning('Maximum number of iterations reached.'), end
end
#+END_SRC

** Closing Notes                                                                     :noexport:
CLOSED: [2021-03-12 Fri 11:14]
*** Notes                                                                           :noexport:
 - FNC 4.6, 4.7

*** Revisiting =FZERO=
**** How Does =FZERO= Work?
*** Quasi-Newton Methods
**** Jacobian by Finite Differences
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
function J = fdjac(f,x0,y0)
% FDJAC   Finite-difference approximation of a Jacobian.
% Input:
%   f        function to be differentiated
%   x0       evaluation point (n-vector)
%   y0       value of f at x0 (m-vector)
% Output
%   J        approximate Jacobian (m-by-n)

    delta = sqrt(eps);   % FD step size
    m = length(y0);  n = length(x0);
    J = zeros(m,n);
    I = eye(n);
    for j = 1:n
        J(:,j) = ( f(x0+delta*I(:,j)) - y0) / delta;
    end

end
#+END_SRC

**** Broyden's Update
**** Levenberg's Method
*** Basin of Attraction
*** Nonlinear Least Squares (NLS)
**** Gauss-Newton Method
**** Convergence
**** Nonlinear Data Fitting

** Problem Solving Sessions                                                          :noexport:
:PROPERTIES:
:EXPORT_TITLE: Module 5 Problem Solving Sessions
:EXPORT_FILE_NAME: module5-rootfinding-prob-solving
:EXPORT_OPTIONS: H:3 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Rootfinding
***** =FZERO= to Solve Complex Problem
 - *FNC* 4.1.5 (Kepler's Law)
***** Lambert W-Function
 - *FNC* 4.1.6
***** More With Lambert W-Function
*Question.* Show that
\[
r = - \frac{ W \left( -\log (2) / 5 \right) }{ \log 2 }.
\]
is a solution of the equation $2^x = 5x$. (Here, as usual in this class, $\log(\;\cdot\;) = \ln (\;\cdot\;)$ is the natural logarithmic function.) Then numerically verify the result using =fzero=[fn::Two real-valued solutions, $r_1 \approx 0.2355$ and $r_2 \approx 4.488$.]
***** FPI: When Convergence Is Faster Than Expected
 - *FNC* 4.2.6
***** FPI: Conditions for Convergence
 - *FNC* 4.2.7
***** Stopping Criteria
 - *FNC* 4.3.8

*** Exercise with Series Analysis
***** Linear Convergence of Newton's Method
****** Newton's Method for Multiple Roots
Assume that $f \in C^{m+1}[a,b]$ has a root $r$ of multiplicity $m$. Then Newton's method is locally convergent to $r$, and the error $\epsilon_k$ at step $k$ satisfies
\[
\lim_{k \to \infty} \frac{\epsilon_{k+1}}{\epsilon_k} = \frac{m-1}{m}
\tag{linear convergence}
\]
****** ...                                                                :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - See Problem 4 of HW07 (*FNC* 4.3.7)
 - Remedy: Modify the iteration formula
   \[
   x_{k+1} = x_k - \frac{m f(x_k)}{f'(x_k)}
   \]
***** Calculating $n\text{th}$ Roots
*Question.* Let $n$ be a positive integer. Use Newton's method to produce a quadratically convergent method for calculating the $n\text{th}$ root of a positive number $a$. Prove quadratic convergence.
***** Predicting Next Error
*Question.* Let $f(x) = x^3 - 4x$.
 - The function $f(x)$ has a root at $r = 2$. If the error $\epsilon_k = x_k - r$ after four steps of Newton's method is $\epsilon_4 = 10^{-6}$, estimate $\epsilon_5$.
 - Do the same to the root $r = 0$.
***** Secant Method
Assume that iterates $x_1, x_2, \ldots$ generated by the secant method converge to a root $r$ and $f'(r) \neq 0$. Let $\epsilon_k = x_k - r$.

\vs

*Exercise.* Show that

\vs

 a) The error $\epsilon_k$ satisfies the approximate equation
    \[
    \abs{ \epsilon_{k+1} } \approx \abs{ \frac{f''(r)}{2f'(r)} } \abs{ \epsilon_{k} } \abs{ \epsilon_{k-1} }.
    \]
 b) If in addition $\lim_{k \to \infty} \abs{ \epsilon_{k+1} } / \abs{ \epsilon_k }^\alpha$ exists and is nonzero for some $\alpha > 0$, then
    \[
    \abs{ \epsilon_{k+1} } \approx \abs{ \frac{f''(r)}{2f'(r)} }^{\alpha-1} \abs{ \epsilon_{k} }^\alpha, \quad \text{where }
    \alpha = \frac{1+\sqrt{5}}{2}.
    \]

* DONE Chapter 5: Piecewise Interpolation
** DONE Piecewise Interpolation (Introduction)
:PROPERTIES:
:EXPORT_FILE_NAME: 25-ppinterp
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

% \begin{frame}{Contents}
%   \tableofcontents
% \end{frame}
#+END_EXPORT
*** Notes                                                                           :noexport:
 - FNC 5.1, 5.2
 - LM 12.2

*** Interpolation Problem                                                    :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
**** Problem Statement
***** General Interpolation Problem
#+LATEX:  \begingroup
Given a set of $n$ data points $\{(x_j,y_j) \mid j \in \NN[1,n]\}$ with $x_1 < x_2 < \ldots < x_n$, find a function $p(x)$, called the *interpolant*, such that
\[
  p(x_j) = y_j, \quad \text{for } j = 1, 2, \ldots, n \,.
\]
#+LATEX: \endgroup
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
The ordered pair $(x_j,y_j)$ is called the *data point*.
 - $x_j$ is called the *abscissa* or the *node*.
 - $y_j$ is called the *ordinate*.
**** Polynomials
One approach is to find an interpolating /polynomial/ of degree (at most) $n-1$,
\[
  p(x) = c_1 + c_2 x + c_3 x^2 + \cdots + c_n x^{n-1}.
\]
 - The unknown coefficients $c_1, \ldots, c_n$ are determined by solving the square linear system $V \bc = \by$ where
   \begin{equation*}
     V =
     \begin{bmatrix}
       1      & x_1    & \cdots & x_1^{n-2} & x_1^{n-1} \\
       1      & x_2    & \cdots & x_2^{n-2} & x_2^{n-1} \\
       \vdots & \vdots &        & \vdots   & \vdots \\
       1      & x_n    & \cdots & x_n^{n-2} & x_n^{n-1}
     \end{bmatrix},
     \quad
     \bc =
     \begin{bmatrix}
       c_1 \\ c_2 \\ \vdots \\ c_n
     \end{bmatrix},
     \quad\text{and}\quad
     \by =
     \begin{bmatrix}
       y_1 \\ y_2 \\ \vdots \\ y_n
     \end{bmatrix} \,.
   \end{equation*}
   the matrix $V$ is called the *Vandermonde matrix*.
 - A polynomial interpolant has severe oscillations as $n$ grows large, unless nodes are special; see illustration in the next slide.

**** Illustration of Runge's Phenomenon
#+LATEX: \begingroup\small
Polynomial Interpolation of 15 data points collected from the same function
\[
f(x) = \frac{1}{1 + 25x^2}.
\]
#+LATEX: \endgroup

***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+ATTR_LATEX: :width 0.9\linewidth
[[../img/interp_unif.eps]]
****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+ATTR_LATEX: :width 0.9\linewidth
[[../img/interp_cheb.eps]]

**** Piecewise Polynomials
To handle real-life data sets with large $n$ and unrestricted node distribution:
\vs
 - An alternate approach is to use a low-degree polynomial between each pair of data points; it is called the *piecewise polynomial interpolation*.
 - The simplest case is *piecewise linear interpolation* (degree 1) in which the interpolant is linear between each pair of consecutive nodes.
 - The most commonly used method is *cubic spline interplation* (degree 3).
 - The endpoints of the low-degree polynomials are called *breakpoints* or *knots*.
 - The breakpoints and the data points almost always coincide.

**** MATLAB Function: =INTERP1=
In MATLAB, piecewise polynomials are constructed using =interp1= function. Suppose the $x$ and $y$ data are stored in vectors =xdp= and =ydp=. To evaluate the piecewise interpolant at =x= (an array):
\vs
 - By default, it finds a piecewise linear interpolant.
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
  y = interp1(xdp, ydp, x);
   #+END_SRC

 - To obtain a smoother interpolant that is piecewise cubic, use ='spline'= option.
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
  y = interp1(xdp, ydp, x, 'spline');
   #+END_SRC

**** Demonstration: Piecewise Polynomial Interplation
#+LATEX: \begingroup\small
To interpolate data obtained from [fn::This function is often called the Runge's function.]
\[
f(x) = \frac{1}{1 + 25x^2}.
\]
#+LATEX: \vspace{-1em}
#+ATTR_LATEX: :options style=matlab, basicstyle=\footnotesize\ttfamily
#+BEGIN_SRC matlab
 % Generate data and eval pts
 n = 15;
 xdp = linspace(-1,1,n)';
 ydp = 1./(1+25*xdp.^2);
 x = linspace(-1,1,400)';

 % PL
 plot(xdp,ydp,'o'), hold on
 plot(x, interp1(xdp,ydp,x))

 % Cubic spline
 plot(xdp,ydp,'o'), hold on
 plot(x, interp1(xdp,ydp,x,'spline'));
#+END_SRC
#+LATEX: \endgroup

**** Demonstration: Piecewise Polynomial Interplation {{{cont}}}
***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+ATTR_LATEX: :width 0.9\linewidth
[[./plinterp.eps]]
****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+ATTR_LATEX: :width 0.9\linewidth
[[./splinterp.eps]]

**** Conditioning
*Set-up for analysis.*

 - Let $(x_j, y_j)$ for $j = 1, \ldots, n$ denote the data points. Assume that the nodes $x_j$ are fixed and let $a = x_1$, $b = x_n$.

 - View the interpolation problem as a mathematical function $\mathcal{I}$ with
   - Input: a vector $\by$ (ordinates, or $y\text{-data points}$)
   - Output: a function $p(x)$ such that $p(x_j) = y_j$ for all $j$.
   (That is, $\cI$ is a /black box/ that produces the interpolant from a data vector.)

 - For the interpolation methods under consideration (polynomial or piecewise polynomial), $\cI$ is /linear/:
   \[
   \cI(\alpha \by + \beta \bz) = \alpha \cI(\by) + \beta \cI(\bz),
   \]
   for all vectors $\by, \bz$ and scalars $\alpha, \beta$.

**** Conditioning: Main Theorem
***** Conditioning of General Interpolation                                      :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Suppose that $\cI$ is a linear interpolation method. Then the absolute condition number of $\cI$ satisfies
\[
\max_{1 \le j \le n} \Norm{ \cI(\be_j) }{\infty} \le \kappa(\by) \le \sum_{j=1}^{n} \Norm{ \cI(\be_j) }{\infty},
\]
where all vectors and functions are measured in the infinity norm.
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:

**** Conditioning: Notes
 - The functional infinity norm is defined by
   \[
   \Norm{f}{\infty} = \max_{x \in [a,b]} \abs{f(x)},
   \]
   in a manner similar to vector infinity norm.
 - The expression $\cI(\be_j)$ represents the interpolant $p(x)$ which is /on/ at $x_j$ and /off/ elsewhere, /i.e./,
   \begin{equation*}
   p(x_k) = \delta_{k,j} =
   \begin{dcases}
     1, & k = j \\
     0, & k \neq j
   \end{dcases}.
   \end{equation*}
   Such interpolants are known as *cardinal functions*.
 - The theorem says that the (absolute) condition number is larger than the largest of $\Norm{\cI(\be_j)}{\infty}$, but smaller than the sum of these.
** DONE Piecewise Linear Interpolation
:PROPERTIES:
:EXPORT_FILE_NAME: 26-plinterp
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT
*** Piecewise Linear Interpolation
**** Piecewise Linear Interpolation
Assume that $x_1 < x_2 < \cdots < x_n$ are fixed. The function $p(x)$ defined piecewise[fn::Note the formula changes depending on which interval $x$ lies in.] by
\[
  p(x) = y_j + \frac{y_{j+1} - y_j}{x_{j+1} - x_j} (x-x_j) \,,
  \quad \text{for $x \in [x_j, x_{j+1}]$, $1 \le j \le n-1$}
\]
 - is linear on each interval $[x_j, x_{j+1}]$;
 - connects any two consecutive data points $(x_j, y_j)$ and $(x_{j+1}, y_{j+1})$ by a straight line.

**** Hat Functions
Denote by $H_j(x)$ the $j\text{th}$ /piecewise linear/ cardinal function:
\begin{equation*}
  H_j(x) =
  \begin{dcases}
    \frac{x - x_{j-1}}{x_j - x_{j-1}}, & x \in [x_{j-1}, x_j], \\
    \frac{x_{j+1} - x}{x_{j+1} - x_j}, & x \in [x_{j}, x_{j+1}], \\
    0, & \text{otherwise},
  \end{dcases}
  \quad j = 1, 2, \ldots, n.
\end{equation*}

 - The functions $H_1, \ldots, H_n$ are called *hat functions* or *tent functions*.
 - Each $H_j$ is globally continuous and is linear inside each interval $[x_j, x_{j+1}]$

#+LATEX: \vspace{0.8in}
#+LATEX: \hrule
#+LATEX: \begingroup\footnotesize \vspace{0.5em}
*Note:* The definitions of $H_1(x)$ and $H_n(x)$ require additional nodes $x_0$ and $x_{n+1}$ for $x$ outside of $[x_1, x_n]$, which is not relevant in the discussion of interpolation.
#+LATEX: \endgroup

**** Hat Functions {{{cont}}}
#+ATTR_LATEX: :width 0.7\linewidth
[[./hatfun.eps]]

**** Hat Functions As Basis
 - Any linear combination of hat functions is continuous and is linear inside each interval $[x_j, x_{j+1}]$.
 - Conversely, /any/ such function is expressible as a unique linear combination of hat functions, /i.e./,
   \[
   \sum_{j=1}^n c_j H_j(x), \quad\text{for some choice of $c_1, \ldots, c_n$.}
   \]
 - No smaller set of functions has the same properties.

\vfill
#+BEGIN_CENTER

#+ATTR_LATEX: :options [width=0.9\linewidth,arc=0mm,boxrule=1pt,colback=white,nobeforeafter]
#+BEGIN_tcolorbox
The hat functions form a *basis* of the set of functions that are continuous and piecewise linear relative to $\bx$ (the vector of nodes).
#+END_tcolorbox

#+END_CENTER

**** Cardinality Conditions
 - By construction, the hat functions are cardinal functions for piecewise linear (PL) interpolation, /i.e./, they satisfy
   \[
   H_j(x_k) = \delta_{j,k}. \tag{cardinality condition}
   \]
 - Key consequence of this property is that the piecewise linear interpolant $p(x)$ for the data values in $\by$ is trivially expressed by
   \[
   p(x) = \sum_{j=1}^{n} y_j H_j(x).
   \]

**** Recipe for PL Interpolant
#+LATEX: \vspace{-0.5em}
***** Piecewise Linear Interpolant
#+LATEX: \begingroup\small
The piecewise linear polynomial
\[
  p(x) = \sum_{j=1}^{n} y_j H_j(x)
\]
is the unique such function which passes through all the data points.
#+LATEX: \endgroup

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vspace{0.5em}
#+LATEX: \begingroup\footnotesize
/Proof:/ It is easy to check the interpolating property:
\[
  p(x_k)
  = \sum_{j=1}^{n} y_j H_j(x_k)
  = \sum_{j=1}^{n} y_j \delta_{j,k} = y_k
  \quad \text{for every $k \in \NN[1,n]$.}
\]
To show uniqueness, suppose $\widetilde{p}$ is another such function in the form
\[
  \widetilde{p}(x) = \sum_{j=1}^{n} c_j H_j(x) \,.
\]
Then $p(x_k) - \widetilde{p}(x_k) = 0$ for all $k \in \NN[1,n]$. This implies that $c_k = y_k$ for all $k$
\qed{}
#+LATEX: \endgroup

**** Implementation                                                                :noexport:
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
function H = hatfun(x,xdp,j)
% HATFUN   Hat function/piecewise linear basis function.
% Input:
%   x      evaluation points (vector)
%   xdp    interpolation nodes (vector, length n)
%   j      node index (integer, in 1,...,n)
% Output:
%   H      values of the jth hat function
% Modified from FNC.

    n = length(xdp);

    % Fictitious nodes to deal with first, last funcs.
    xdp = [ 2*xpd(1)-xdp(2); xdp(:); 2*xdp(n)-xpd(n) ];
    j = j+1;  % adjust index for the fictitious first node
    H1 = (x-xdp(j-1))/(xdp(j)-xdp(j-1));   % upward slope
    H2 = (xdp(j+1)-x)/(xdp(j+1)-xdp(j));   % downward slope

    H = min(H1,H2);
    H = max(0,H);

end
#+END_SRC

*** Analysis
**** Conditioning
***** Lemma
Let $\cI$ is the piecewise linear interpolation operator and $\bz \in \RR^n$. Then
\[
\Norm{\cI(\bz)}{\infty} = \Norm{\bz}{\infty}.
\]
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - It follows from the lemma that _the absolute condition number of piecewise linear interpolation in the infinity norm equals one_.

**** Conditioning {{{cont}}}
#+LATEX: \begingroup\small
/Proof of lemma/.
Let
\[
p(x) = \cI(\bz) = \sum_{j=1}^n z_j H_j(x).
\]
# We need to show that $\Norm{p}{\infty} = \Norm{\bz}{\infty}$.
# \vs
Let $k$ be the index corresponding to the element of $\bz$ with the largest absolute value, that is, $z_k = \Norm{\bz}{\infty}$. Since $z_k = p(x_k)$, it follows that $\abs{ p(x_k) } = \Norm{\bz}{\infty}$ and so $\Norm{p}{\infty} \ge \Norm{\bz}{\infty}$.

\vs

To show the other inequality, note that
\[
\abs{ p(x) } = \abs{ \sum_{j=1}^n z_j H_j(x) } \le  \sum_{j=1}^n \abs{z_j} H_j(x) \le \Norm{\bz}{\infty} \sum_{j=1}^n H_j(x) = \Norm{\bz}{\infty},
\]
where the final step uses the fact[fn::This property is called the /partition of unity/. Confirm it!] that $\sum_{j=1}^n H_j(x) = 1$. It implies that $\Norm{p}{\infty} \le \Norm{\bz}{\infty}$. Therefore, $\Norm{p}{\infty} = \Norm{\bz}{\infty}$. \qed
#+LATEX: \endgroup

**** Convergence: Error Analysis
*Set-up for analysis.*
\vs
 - Generate a set of data points using a ``nice'' function $f$ on an interval containing all nodes, /i.e./, $y_j = f(x_j)$. (The /niceness/ of a function is described in precise terms below.)
 - Then perform PL interpolation of the data to obtain the interpolant $p$.
 - *Question.* How close is $p$ to $f$?

\vfill

***** Notation (Space of Differentiable Functions)
Let $C^n[a,b]$ denote the set of all functions that are $n\text{-times}$ continuously differentiable on $[a,b]$. That is, if $f \in C^n[a,b]$, then $f^{(n)}$ exists and is continuous on $[a,b]$, where derivatives at the end points are taken to be one-sided derivatives.

**** Convergence: Error Analysis {{{cont}}}
***** Error Theorem for PL Interpolation                                         :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Suppose that $f \in C^2[a,b]$. Let $p_n$ be the piecewise linear interpolant of $(x_j, f(x_j))$ for $j = 1, \ldots, n$, where
\begin{equation*}
x_j = a + (j-1) h \quad\text{and}\quad h = \frac{b-a}{n-1}.
\end{equation*}
Then
\[
\Norm{f - p_n}{\infty} \le \Norm{f''}{\infty} h^2.
\]
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
#+LATEX: \begingroup\small
 - The theorem pertains to the interpolation on equispaced nodes.
 - The significance of the theorem is that the error in the interpolant is $O(h^2)$ as $h \to 0$. (We say that PL interpolation is /second-order accurate/.)
 - *Practical implication:* If $n$ is doubled, the PL interpolant becomes about four times more accurate. A log-log graph (=loglog=) of error against $n$ is a straight line.
#+LATEX: \endgroup

**** Convergence: Error Analysis {{{cont}}}
#+ATTR_LATEX: :width 0.75\linewidth
[[./pl_convg.eps]]

** DONE Piecewise Cubic Interpolation
:PROPERTIES:
:EXPORT_FILE_NAME: 27-pcinterp
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT
*** Notes                                                                           :noexport:
 - FNC 5.3
 - LM 12.2, 12.8

*** Hermite Cubic Interpolation
**** Problem Set-Up: General Piecewise Cubic Interpolation
We now seek a piecewise cubic polynomial $\pp$ which interpolates the data $(x_i, y_i)$ for $i = 1, \ldots, n$, with $x_1 < x_2 < \cdots < x_n$, defined as
\begin{equation*}
\pp(x) =
\begin{dcases}
  p_1(x), & x \in [x_1, x_2) \\
  p_2(x), & x \in [x_2, x_3) \\
  \hphantom{p_1}\vdots & \hphantom{x \in}\vdots \\
  p_{n-1}(x), & x \in [x_{n-1}, x_n]
\end{dcases},
\end{equation*}
where the $i\text{th}$ /local/ cubic polynomial $p_i$ is written in shifted power form as
\[
  p_i(x) = c_{i,1} + c_{i,2} (x-x_i) + c_{i,3} (x-x_i)^2 + c_{i,4} (x-x_i)^3 .
\]

**** Hermite Cubic Interpolation
If the slopes at the breakpoints are prescribed, /i.e./, for each $i = 1, \ldots, n-1$,
\[
  p_i(x_i) = y_i \,,\quad
  p_i'(x_i) = \sigma_i \,,\quad
  p_i(x_{i+1}) = y_{i+1} \,,\quad
  p_i'(x_{i+1}) = \sigma_{i+1} \,,
\]
then we can solve for the four unknown coefficients $c_{i,j}$, $j = 1, \ldots, 4$:
\begin{equation*}
  \begin{alignedat}{2}
    c_{i,1} &= y_i \,,\qquad
    &  c_{i,3} &= \frac{3y[x_i, x_{i+1}] -
      2\sigma_i-\sigma_{i+1}}{\Delta x_i} \,,\\
    c_{i,2} &= \sigma_i \,,\qquad
    &  c_{i,4} &= \frac{\sigma_i+\sigma_{i+1}-2y[x_i,
      x_{i+1}]}{(\Delta x_i)^2} \,.
  \end{alignedat}
\end{equation*}
where $\Delta x_i = x_{i+1} - x_i$ and
\[
y[x_i, x_{i+1}] = \frac{y_{i+1}-y_i}{x_{i+1}-x_i}.
\tag{Newton's divided difference}
\]
This is called *Hermite cubic interpolation*.

**** Implementation
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
 function c = hermiteCoeff(x,y,s)
 % Input:
 %   x,y,s   data points and slopes
 % Ouput:
 %   c       coefficients in matrix form
     n = length(x);
     c = zeros(n-1, 4);
     dx = diff(x);
     dy = diff(y);
     dydx = dy./dx;
     c(:,1) = y;
     c(:,2) = s;
     c(:,3) = (3*dydx - 2*s(1:n-1) - s(2:n))./dx;
     c(:,4) = (s(1:n-1) + s(2:n-1) - 2*dydx))./(dx.^2);
 end
#+END_SRC

**** Convergence: Error Analysis
***** Error Theorem for Hermite Cubic Interpolation                              :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Let $f \in C^4[a,b]$ and let $\pp(x)$ be the Hermite cubic interpolant of
\[
\left(x_i, f(x_i), f'(x_i)\right),\quad \text{for $i = 1, \ldots, n$,}
\]
where
\begin{equation*}
x_j = a + (j-1) h \quad\text{and}\quad h = \frac{b-a}{n-1}.
\end{equation*}
Then
\[
  \Norm{f - \pp}{\infty} \le \frac{1}{384} \Norm[2]{f^{(4)}}{\infty} h^4.
\]

**** Drawbacks of Hermite Cubic Interpolation
 - The interpolant $\pp(x)$ is in $C^1$ and so its display may be too crude in graphical applications.
 - In other applications, there may be difficulties if $\pp''(x)$ is discontinuous.
 - In experimental settings where $y_i$ are measurements of some sort, we may not have the first derivative information required for the cubic Hermite process.

*** Cubic Splines
**** COMMENT Definitions
***** Spline                                                                  :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
$S(x)$ is called a \textbf{spline of degree $k$} if
 - $S_i(x)$ is a polynomial of degree $k$;
 - $S(x)$ is $(k-1)\text{-times}$ continuously differentiable.
**** Cubic Splines
#+LATEX: \begingroup \small
*Idea:* Put together cubic polynomials to make the result as smooth as possible.
 - At interior breakpoints: for $j = 2, 3, \cdots, n-1$
   - matching values: $p_{j-1}(x_j) = p_j(x_j)$ \hfill  [$(n-2)$ eqns]
   - matching first derivatives: $p_{j-1}'(x_j) = p_j'(x_j)$ \hfill  [$(n-2)$ eqns]
   - matching second derivative: $p_{j-1}''(x_j) = p_j''(x_j)$ \hfill  [$(n-2)$ eqns]
 - So, together with the $n$ interpolating conditions, we have total of $(4n-6)$ equations.
 - To match up with the number of unknowns $(4n-4)$, we need to impose two more conditions on the boundary:
   1. slopes at each end (clamped cubic spline)
   2. second derivatives at the endpoints (natural cubic spline)
   3. periodic boundary condition
   4. /not-a-knot/ boundary condition: $p_1(x) \equiv p_2(x)$ and $p_{n-2}(x) \equiv p_{n-1}(x)$.
#+LATEX: \endgroup

**** Convergence: Error Analysis
***** Error Theorem for Clamped Cubic Splines                                    :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Let $f \in C^4[a,b]$ and let $\pp(x)$ be the cubic spline interpolant of
\[
\left(x_i, f(x_i)\right),\quad \text{for $i = 1, \ldots, n$,}
\]
with the exact boundary conditions
\[
  \sigma_1 = f'(x_1) \quad \text{and} \quad \sigma_n = f'(x_n),
\]
in which
\begin{equation*}
x_j = a + (j-1) h \quad\text{and}\quad h = \frac{b-a}{n-1}.
\end{equation*}
Then
\[
  \Norm{f - \pp}{\infty} \le \frac{5}{384} \Norm[2]{f^{(4)}}{\infty} h^4.
\]

**** Remarks
 - Hermite cubic interpolation is about five times as accurate as cubic spline interpolation, yet both have /fourth-order accuracy/.
 - Unlike the former, the latter does not require first derivatives.

*** Appendix: Derivation of Cubic Spline Algorithm
**** Cubic Spline: Problem Set-Up
Given $\{(x_i, y_i) \,|\, i = 1, 2, \ldots, n\}$, find a piecewise polynomial $\pp(x) = p_i(x)$ on $[x_i, x_{i+1}]$ with
\[
  p_i(x) = c_{i,1} + c_{i,2}(x-x_i) + c_{i,3}(x-x_i)^2 + c_{i,4}(x-x_i)^3,
\]
satisfying[fn::Let us not worry about the boundary conditions yet.]

\vs

 1. $\pp(x_i) = y_i$ for $i=1, \ldots, n$;
 2. $p_i(x_{i+1}) = p_{i+1}(x_{i+1})$ for $i=1, \ldots, n-2$;
 3. $p_i'(x_{i+1}) = p_{i+1}'(x_{i+1})$ for $i=1, \ldots, n-2$;
 4. $p_i''(x_{i+1}) = p_{i+1}''(x_{i+1})$ for $i=1, \ldots, n-2$.

**** Connection to Hermite Cubic Interpolation
*Key Observation.* If $c_{i,j}$'s are set to be
\begin{equation*}
  \label{eq:hermite_coeffs}
  \begin{alignedat}{2}
    c_{i,1} &= y_i \,,\qquad
    &  c_{i,3} &= \frac{3y[x_i, x_{i+1}] -
      2\sigma_i-\sigma_{i+1}}{\Delta x_i} \,,\\
    c_{i,2} &= \sigma_i \,,\qquad
    &  c_{i,4} &= \frac{\sigma_i+\sigma_{i+1}-2y[x_i,
      x_{i+1}]}{(\Delta x_i)^2} \,,
  \end{alignedat}
  \tag{$\star$}
\end{equation*}
as in the Hermite cubic interpolation for some constants $\sigma_1, \sigma_2, \ldots, \sigma_n$ to be determined, then $\pp(x)$ satisfies the first three requirements from the previous slide.

\vs

*Reduction.* Determine $\sigma_1, \sigma_2, \ldots, \sigma_n$ so that the fourth requirement is satisfied.

**** Derivation of a Linear System for Cubic Splines
*Exercise.* Using \eqref{eq:hermite_coeffs}, write out the fourth requirement $p_{i-1}''(x_i) = p_{i}''(x_i)$ in terms of $\sigma_{i-1}, \sigma_i, \sigma_{i+1}$, where $i \in \NN[2,n-1]$.

**** Derivation of a Linear System for Cubic Splines {{{cont}}}
*Exercise.* Express the system of $n-2$ equations for $\sigma_1, \ldots, \sigma_n$ as a matrix equation $X\bsigma = \br$, where $\bsigma = (\sigma_1, \ldots, \sigma_n)\tp$ and $X \in \RR^{n \times n}$ and $\br \in \RR^n$ are to be found[fn::Since two equations are still missing, leave the first and last rows of $X$ and $\br$ empty for now. The answer is provided in the next slide.].

**** Tridiagonal System for Cubic Splines
#+LATEX: \begingroup \small
*Notation.* $\Delta x_i = x_{i+1} - x_i$ and $\grad x_i = \Delta x_{i-1} + \Delta x_{i} = x_{i+1} - x_{i-1}$.
\vs
# The requirements for cubic splines give rise to the tridiagonal matrix equation $X\bsigma = \br$, where

#+LATEX: \begingroup \footnotesize
\begin{align*}
  X = &
  \begin{bmatrix}
    * & * & * & \cdots & * & * & * \\
    \Delta x_2 & 2 \grad x_2 & \Delta x_1 &&&& \\
    & \Delta x_3 & 2 \grad x_3 & \Delta x_2 &&& \bigzero \\
    && \ddots & \ddots & \ddots && \\
    \bigzero&&& \Delta x_{n-2} & 2 \grad x_{n-2} & \Delta x_{n-3} & \\
    &&&& \Delta x_{n-1} & 2 \grad x_{n-1} & \Delta x_{n-2} \\
    \vphantom{1.2em}* & * & * & \cdots & * & * & *
  \end{bmatrix}, \\\\
  \bsigma = &
  \begin{bmatrix}
    \sigma_1 \\ \sigma_2 \\ \sigma_3 \\ \vdots \\ \sigma_{n-2} \\ \sigma_{n-1} \\ \sigma_n
  \end{bmatrix},
  \quad\text{and}\quad
  \br =
  \begin{bmatrix}
    * \\
    3 \left( y[x_1,x_2]\Delta x_2 + y[x_2,x_3] \Delta x_1 \right) \\
    3 \left( y[x_2,x_3]\Delta x_3 + y[x_3,x_4] \Delta x_2 \right) \\
    \vdots \\
    3 \left( y[x_{n-3},x_{n-2}]\Delta x_{n-2} + y[x_{n-2},x_{n-1}] \Delta x_{n-3} \right) \\
    3 \left( y[x_{n-2},x_{n-1}]\Delta x_{n-1} + y[x_{n-1},x_{n}] \Delta x_{n-2} \right) \\
    *
  \end{bmatrix}.
\end{align*}
#+LATEX: \endgroup
#+LATEX: \endgroup

**** Remarks
An $n \times n$ matrix $A$ is said to be *diagonally dominant* if
\[
\abs{a_{ii}} > \sum_{j=1, j\neq i}^{n} \abs{a_{ij}} \quad\text{for each $i = 1, \ldots, n$.}
\]

 - It is known that a diagonally dominant matrix is nonsingular.
 - Note that $X$ is a tridiagonal matrix which is diagonally dominant, and thus invertible.

**** Implementation of Boundary Conditions
 - (*clamped cubic spline*) If slopes at each end are known, fill in the first and the last equation of $X\bsigma = \br$ with
   \[
     \sigma_1 = y_1', \quad \sigma_n = y_n'.
   \]
 - (*natural cubic spline*) If the second derivatives at the endpoints are known, then use
   \begin{align*}
     2 \sigma_1 + \sigma_2 & = 3y[x_1, x_2] - \frac{1}{2} \Delta x_1 y_1'' \\
     \sigma_{n-1} + 2\sigma_n & = 3y[x_{n-1},x_n] + \frac{1}{2} \Delta x_{n-1} y_n''.
   \end{align*}

**** Implementation of Boundary Conditions {{{cont}}}
 - (*periodic boundary condition*) If the data points come from a periodic function with period $P = x_n - x_1$ so that $\sigma_1 = \sigma_n$, then use
   \begin{align*}
     \Delta x_1 \sigma_{n-1} + 2 \grad x_1 \sigma_1 + \Delta x_{n-1} \sigma_2 & = 3 \left( y[x_{n-1}, x_n] \Delta x_1 + y[x_1, x_2] \Delta x_{n-1} \right) \\
     \sigma_1 - \sigma_n & = 0.
   \end{align*}
   Here, take $\grad x_1 = x_2 - x_0 = x_2 - (x_{n-1} - P)$.

**** Implementation of Boundary Conditions {{{cont}}}
 - (*/not-a-knot/ boundary condition*) If nothing is known about the endpoints, require $p_1(x) \equiv p_2(x)$ and $p_{n-2}(x) = p_{n-1}(x)$:
   \begin{multline*}
     \left( \Delta x_2 \right)^2 \sigma_1 + \left( \left( \Delta x_2 \right)^2 - \left( \Delta x_1 \right)^2 \right) \sigma_2 - \left( \Delta x_1 \right)^2 \sigma_3 \\
     = 2 \left( y[x_1, x_2] \left( \Delta x_2 \right)^2 - y[x_2, x_3] \left( \Delta x_1 \right)^2 \right),
   \end{multline*}
   \begin{multline*}
     -\left( \Delta x_{n-1} \right)^2 \sigma_{n-2} + \left( \left( \Delta x_{n-2} \right)^2 - \left( \Delta x_{n-1} \right)^2 \right) \sigma_{n-1} + \left( \Delta x_{n-2} \right)^2 \sigma_n \\
     = 2 \left( y[x_{n-1}, x_n] \left( \Delta x_{n-2} \right)^2 - y[x_{n-2}, x_{n-1}] \left( \Delta x_{n-1} \right)^2 \right).
   \end{multline*}
\vfill
*Exercise.* Derive the equations shown above.

** DONE Numerical Differentiation
:PROPERTIES:
:EXPORT_FILE_NAME: 28-num-diff
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Introduction
**** Difference Quotients to Approximate Slopes
:PROPERTIES:
:BEAMER_opt: c
:END:

#+ATTR_LATEX: :options {0.49\linewidth}
#+BEGIN_minipage
\vspace{0pt}
\begin{image}[0.75\linewidth]
  \begin{tikzpicture}
    \begin{axis}[
      width=0.99\textwidth,
      height=0.8\textheight,
      domain=-1:1,
      range=0.1:1.2,
      ymax=1.2,
      ymin=0.1,
      axis lines = left,
      axis y line = none,
      every axis x label/.style={at=(current axis.right of origin),anchor=west},
      xtick={-0.5, 0, 0.5},
      xticklabels={$$, $x$,$$},
      axis on top,
      ]
      \addplot [penColor, thick, dashed,  domain=(-0.8:0.8)] {0.5 + 0.25*x};
      \addplot [very thick,textColor, smooth,domain=(-0.8:0.8)] {-1/(x-2)};
      \addplot [color=textColor,fill=textColor,only marks,mark=*] coordinates{(0, 0.5)};  %% closed hole
    \end{axis}
  \end{tikzpicture}
\end{image}
#+END_minipage
\hfill
#+ATTR_LATEX: :options {0.49\linewidth}
#+BEGIN_minipage
\begingroup\small
Let $f$ be a smooth function. Analytically, the derivative is calculated by
\[
  D\{f\}(x) = f'(x) = \lim_{h \to 0} \frac{f(x+h)-f(x)}{h},
\]
which represents the slope of the line tangent to the graph of $f$ at $x$.
\endgroup
#+END_minipage

**** Difference Quotients to Approximate Slopes {{{cont}}}
#+ATTR_LATEX: :options {0.49\linewidth}
#+BEGIN_minipage
\begin{image}[0.75\linewidth]
  \begin{tikzpicture}
    \begin{axis}[
      width=0.99\textwidth,
      height=0.8\textheight,
      domain=-1:1,
      range=0.1:1.2,
      ymax=1.2,
      ymin=0.1,
      axis lines = left,
      axis y line = none,
      every axis x label/.style={at=(current axis.right of origin),anchor=west},
      xtick={-0.5, 0, 0.5},
      xticklabels={$x-h$, $x$,$x+h$},
      axis on top,
      ]
      \addplot [penColor, dashed,  domain=(-0.8:0.8)] {0.5 + 0.25*x}; \addlegendentry{slope};
      \only<1>{\addplot [penColor2,thick,dashed,domain=(-0.8:0.8)] {0.5 + 0.33*x}; \addlegendentry{FD};}
      \only<2>{\addplot [penColor3,thick,dashed,domain=(-0.8:0.8)] {0.5 + 0.2*x}; \addlegendentry{BD};}
      \only<3>{\addplot [penColor4,thick,dashed,domain=(-0.8:0.8)] {0.533 + 0.267*x}; \addlegendentry{CD};}
      \addplot [very thick,textColor, smooth,domain=(-0.8:0.8)] {-1/(x-2)};
      \addplot [color=textColor,fill=textColor,only marks,mark=*] coordinates{(0, 0.5)};  %% closed hole
      \only<1,3>{\addplot [color=textColor,fill=textColor,only marks,mark=*] coordinates{(0.5, 0.667)};}
      \only<2-3>{\addplot [color=textColor,fill=textColor,only marks,mark=*] coordinates{(-0.5, 0.4)};}
    \end{axis}
  \end{tikzpicture}
\end{image}
#+END_minipage
\hfill
#+ATTR_LATEX: :options {0.49\linewidth}
#+BEGIN_minipage
\begingroup\small
Since the definition relies on $h$ approaching $0$, choosing a small, fixed value for $h$ approximates $f'(x)$.
\vs
\begin{itemize}[<+->]
\item 1st-order forward difference
  \[
    D^{\rm [1f]}_h \{f\} (x) = \frac{f(x+h)-f(x)}{h}
  \]
  \vfill
\item 1st-order backward difference
  \[
    D^{\rm [1b]}_h \{f\} (x) = \frac{f(x) - f(x-h)}{h}
  \]
  \vfill
\item 2nd-order centered difference
  \[
    D^{\rm [2c]}_h \{f\} (x) = \frac{f(x+h) - f(x-h)}{2h}
  \]
  \vfill
\end{itemize}
\endgroup
#+END_minipage

**** Difference Quotients to Approximate Slopes {{{cont}}}
The three approximation formulas presented above are examples of so-called *finite difference formulas*.

***** Note                                                                         :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
The terms /first-order/ and /second-order/ refer to how quickly the approximation converges to the actual value of $f'(x)$ as $h$ approaches $0$, not to the order of differentiation. More on this later.

**** Interpolation and Difference Formulas
For simplicity of notation, let's set $x=0$.

\vfill

Observe that
 - The forward difference formula is simply the slope of the (secant) line through the two points $(0, f(0))$ and $(h, f(h))$.
 - Similarly, the backward difference formula is simply the slope of the (secant) line through the two points $(0, f(0))$ and $(h, f(h))$.

\vfill

Think:
 - slope $\Leftrightarrow$ derivative
 - line through two points $\Leftrightarrow$ interpolant

\vfill

A natural extension of this perspective is to think of the centered difference formula as the derivative of the quadratic interpolant of the three points $(-h, f(-h))$, $(0, f(0))$, and $(h, f(h))$.

**** Interpolation and Difference Formulas {{{cont}}}
*Exercise 1.* Show that the quadratic function
\[
q(x) = \frac{x(x-h)}{2h^2} f(-h) - \frac{x^2-h^2}{h^2} f(0) + \frac{x(x+h)}{2h^2} f(h).
\]
interpolates $(-h, f(-h))$, $(0, f(0))$, and $(h, f(h))$.

\vfill

*Exercise 2.* Show that $q'(0) = D_h^{\rm [2c]}\{f\}(0)$.

\vfill

**** Interpolation and Difference Formulas {{{cont}}}
#+ATTR_LATEX: :options [width=\linewidth,arc=0mm,boxrule=0.7pt,colback=white,nobeforeafter]
#+BEGIN_tcolorbox
In principle, once nodes are determined, a finite difference formula can be derived by:

\vs

#+BEGIN_QUOTE
Interpolate the given function values, then differentiate the interpolant exactly.
#+END_QUOTE
#+END_tcolorbox

\vs
Some commonly used difference formulas are provided, without derivation, in the next slide.

**** Common Difference Formulas
#+LATEX: \begingroup\footnotesize
#+ATTR_LATEX: :align c|c|c|c
| Type     | Order | Notation                     | Formula                                                                 |
|----------+-------+------------------------------+-------------------------------------------------------------------------|
| Forward  |     1 | $\ds D_h^{\rm [1f]}\{f\}(x)$ | $\ds \frac{f(x+h)-f(x)}{h}$ \rule[-1em]{0pt}{3em}                       |
|          |     2 | $\ds D_h^{\rm [2f]}\{f\}(x)$ | $\ds \frac{-3f(x) + 4f(x+h)-f(x+2h)}{2h}$ \rule[-1.3em]{0pt}{3.3em}     |
|----------+-------+------------------------------+-------------------------------------------------------------------------|
| Backward |     1 | $\ds D_h^{\rm [1b]}\{f\}(x)$ | $\ds \frac{f(x)-f(x-h)}{h}$ \rule[-1em]{0pt}{3em}                       |
|          |     2 | $\ds D_h^{\rm [2b]}\{f\}(x)$ | $\ds \frac{3f(x) - 4f(x-h)+f(x-2h)}{2h}$ \rule[-1.3em]{0pt}{3.3em}      |
|----------+-------+------------------------------+-------------------------------------------------------------------------|
| Centered |     2 | $\ds D_h^{\rm [2c]}\{f\}(x)$ | $\ds \frac{f(x+h)-f(x-h)}{2h}$ \rule[-1em]{0pt}{3em}                    |
|          |     4 | $\ds D_h^{\rm [4c]}\{f\}(x)$ | $\ds \frac{f(x-2h)-8f(x-h)+8f(x+h)-f(x+2h)}{12h}$ \rule[-1em]{0pt}{3em} |
#+LATEX: \endgroup

**** Higher Derivatives

*** Convergence of Difference Formulas
**** Convergence of Difference Formulas
 - All finite difference formulas introduced converge as $h \to 0$.
 - But there are difficulties in implementing this limiting calculation numerically, so we need to work with $h > 0$ which would yield acceptable accuracy.
 - To address this kind of issues, we need to understand how the accuracy of difference formulas increases as $h \to 0$.
 - In other words, we need to study how /quickly/ the error $D^{\rm [\cdot]}\{f\}(x) - f'(x)$ diminishes as $h \to 0$.
 - The main tool for the analysis is the Taylor series.

**** First-Order Difference Formulas
The formula $D^{\rm [1f]}_h\{f\}$ is said to be a \textbf{first-order} method because
\[
D^{\rm [1f]}_h \{f\}(x) - f'(x) = \underbrace{\frac{1}{2}f''(x) h}_{\text{leading error}} + O(h^2).
\]

#+ATTR_LATEX: :width .6\textwidth
-----
\vs

#+LATEX: \begingroup\small
*Derivation.* Use the Taylor series of $D^{\rm [1f]}_h\{f\}$ at $x$:
\begin{align*}
  D^{\rm [1f]}_h\{f\}(x) - f'(x)
  & = \frac{f(x+h)-f(x)}{h} - f'(x) \\
  & = \frac{1}{h} \left[ \cancel{f(x)} + f'(x) h + \frac{f''(x)}{2} h^2 + O(h^3) - \cancel{f(x)} \right] - f'(x) \\
  & = \left[ \cancel{f'(x)} + \frac{f''(x)}{2} h + O(h^2) \right] - \cancel{f'(x)} \\
  & = \frac{f''(x)}{2} h + O(h^2).
\end{align*}
#+LATEX: \endgroup

**** Second-Order Difference Formulas
The formula $D^{\rm [2c]}_h\{f\}$ is said to be a \textbf{second-order} method because
\[
D^{\rm [2c]}_h \{f\}(x) - f'(x) = \underbrace{\frac{1}{6}f'''(x) h^2}_{\text{leading error}} + O(h^4).
\]

#+ATTR_LATEX: :width .6\textwidth
-----
\vs

#+LATEX: \begingroup\small
*Derivation.* Exercise.
#+LATEX: \endgroup

**** Remarks
 - The difference $D_h^{\rm [\,\cdot\,]}\{f\}(x) - f'(x)$ is called the *truncation error*. This term is derived from the idea that the finite difference formula has to truncate the series representation and thus cannot be exactly correct for all functions.
 - When using a first-order formula, cutting $h$ in half should reduce the error in the approximation of $f'(x)$ by about half, when $h$ is sufficiently small.
 - In general, when using an $n\text{th-order}$ formula, reducing $h$ by a factor of two should cut the error in $f'(x)$ estimate by a factor of about $2^n$, when $h$ is sufficiently small.
 - The notation $O(f(n))$ is commonly used to describe the temporal or spatial complexity of an algorithm. In that context, a $O(n^2)$ algorithm is much worse than a $O(n)$ algorithm. (Remember flop counts for LU factoriztion.) However, when referring to error, a $O(h^2)$ algorithm is *better* than a $O(h)$ algorithm because it means that the accuracy improves faster as $h$ decreases.

**** MATLAB Demo
Comparison of convergence.

**** Determining Optimal $h$

#+LATEX: \begingroup\small
The difference formulas are inherently ill-conditioned as $h \to 0$ due to catastrophic cancellation when implemented in floating point arithmetic. As an example, consider the numerical evaluation[fn::For simplicity, we only consider round-off errors arising in the evaluation of $f$.] of the centered difference formula:

#+LATEX: \begingroup\footnotesize
\begin{align*}
  \widehat{D}_h^{\rm [2c]}\{f\}(x)
  & = \frac{\widehat{f(x+h)} - \widehat{f(x-h)}}{2h} \\
  & = \frac{f(x+h)(1+\epsilon_{+}) - f(x-h)(1+\epsilon_{-})}{2h}, \quad \text{for some $\abs{\epsilon_\pm} \le \frac{1}{2} \meps$} \\
  & = \frac{f(x+h) - f(x-h)}{2h} + \frac{f(x+h)\epsilon_+ - f(x-h)\epsilon_-}{2h} \\
  & = \left[  f'(x) + \frac{1}{6} f'''(x) h^2 + O(h^4) \right]
    + \frac{\left( f(x) + O(h) \right)\epsilon_+ - \left( f(x) + O(h) \right) \epsilon_-}{2h} \\
  & = f'(x) + \frac{1}{6}f'''(x)h^2 + \frac{\epsilon_+ - \epsilon_-}{2h}f(x) + O(\meps) + O(h^4).
\end{align*}
#+LATEX: \endgroup
So the error is bounded from above by:
#+LATEX: \begingroup\footnotesize
\[
\abs{\widehat{D}_h^{\rm [2c]}\{f\}(x) - f'(x)} \le \frac{1}{6} \abs{f'''(x)} h^2 + \frac{\meps}{2h} \abs{f(x)} + O(\meps) + O(h^4).
\]
\vs
#+LATEX: \endgroup
#+LATEX: \endgroup

**** Determining Optimal $h$ {{{cont}}}

#+LATEX: \begingroup\small
Ignoring the last two terms, the error bound consists of two parts:

\[
\abs{\text{error}} \lessapprox \underbracket{\frac{1}{6} \abs{f'''(x)} h^2}_{\text{truncation error}} +
\underbracket{\frac{\meps}{2h} \abs{f(x)}}_{\text{round-off error}} = \alpha h^2 + \frac{\beta}{h}\meps =: g(h).
\]

Using calculus, one can find that $g(h)$ is minimized when
\[
h = \left(\frac{\beta}{2\alpha}\meps\right)^{1/3} = \left(\frac{3}{2} \abs{\frac{f(x)}{f'''(x)}} \right)^{1/3} \meps^{1/3} = O(\meps^{1/3}),
\]
in which case
\[
\abs{\widehat{D}_h^{\rm [2c]}\{f\}(x) - f'(x)} \lessapprox \left(\frac{9}{32} \abs{f^2(x) f'''(x)} \right)^{1/3} \meps^{2/3} = O(\meps^{2/3}).
\]

*Exercise.* Repeat the analysis above to determine the optimal $h$ for a first-order accurate difference formula. How about a general $n\text{th-order}$ method?
#+LATEX: \endgroup

***** figure                                                                      :noexport:
#+BEGIN_SRC matlab
alpha = 2; beta = 1;
IP = 'Interpreter'; ip = 'LaTeX';
FS = 'FontSize'; fs = 18;
hc = ( beta/(2*alpha) )^(1/3);
g = @(h) alpha*h.^2 + beta./h;
clf
fplot(g, [0 2]), grid on
hold on
plot(hc, g(hc), 'r.', 'MarkerSize', 30)
ylim([0 10])
xlabel('$h$', IP, ip, FS, fs)
ylabel('$g(h)$', IP, ip, FS, fs)
hc
g(hc)
#+END_SRC

#+RESULTS:
#+begin_example
alpha = 2; beta = 1;
IP = 'Interpreter'; ip = 'LaTeX';
FS = 'FontSize'; fs = 18;
hc = ( beta/(2*alpha) )^(1/3);
g = @(h) alpha*h.^2 + beta./h;
clf
fplot(g, [0 2]), grid on
hold on
plot(hc, g(hc), 'r.', 'MarkerSize', 30)
ylim([0 10])
xlabel('$h$', IP, ip, FS, fs)
ylabel('$g(h)$', IP, ip, FS, fs)
hc
hc =
    0.6300
g(hc)
ans =
    2.3811
'org_babel_eoe'
ans =
    'org_babel_eoe'
#+end_example

The effect of round-off error on numerical differentiation. For sufficiently small $h$, the error is dominated by rounding error.

#+RESULTS:
#+begin_example
alpha = 2; beta = 1;
IP = 'Interpreter'; ip = 'LaTeX';
FS = 'FontSize'; fs = 18;
hc = ( beta/(2*alpha) )^(1/3);
g = @(h) alpha*h.^2 + beta./h;
clf
fplot(g, [0 2]), grid on
hold on
plot(hc, g(hc), 'r.', 'MarkerSize', 30)
ylim([0 10])
xlabel('$h$', IP, ip, FS, fs)
ylabel('$g(h)$', IP, ip, FS, fs)
'org_babel_eoe'
ans =
    'org_babel_eoe'
#+end_example

**** Determining Optimal $h$ {{{cont}}}
#+ATTR_LATEX: :options {0.6\linewidth}
#+BEGIN_minipage
\begin{image}[0.9\linewidth]
  \begin{tikzpicture}
    \begin{axis}[
      xmin=0,xmax=2,ymin=0,ymax=10,
      unit vector ratio*=1 0.4 1,samples=500,
      axis lines =middle, xlabel=$h$, ylabel=$g(h)$,
      every axis y label/.style={at=(current axis.above origin),anchor=south},
      every axis x label/.style={at=(current axis.right of origin),anchor=west},
      xtick={0.63},
      xticklabels={{\footnotesize $\ds \left( \frac{\beta}{2\alpha}\meps \right)^{1/3}$}},
      ytick={0},
      ]
      \addplot[thick, color=penColor, smooth, domain=(0.1:2)] {2*x^2 + 1/x};
      \addplot[mark=*,red] coordinates {(0.63,2.38)};
      \addplot[penColor,thin,dashed] plot coordinates {(0.63,0) (0.63,2.38)};
      % \draw[draw=red,fill=lightgray] (axis cs:4,0) rectangle (axis cs:5,0);
      % \draw[draw=red,fill=lightgray] (axis cs:5,0) rectangle (axis cs:6,1);
      % \draw[draw=red,fill=lightgray] (axis cs:6,0) rectangle (axis cs:7,1.41421);
      % \filldraw[draw=red,fill=lightgray] (axis cs:7,0) rectangle (axis cs:8,1.7321);
    \end{axis}
  \end{tikzpicture}
\end{image}
#+END_minipage
\hfill
#+ATTR_LATEX: :options {0.39\linewidth}
#+BEGIN_minipage
The effect of round-off error on numerical differentiation. For sufficiently small $h$, the error is dominated by rounding error.
#+END_minipage

**** Richardson Extrapolation
#+LATEX: \begingroup
*Set-up.* Let $V_h$ be a numerical approximation to the analytical value $V$ such that
\[
V_h = V + \underbrace{c_1 h^{p_1} + c_2 h^{p_2} + c_3 h^{p_3} + \cdots}_{\text{discretization error}},
\]
where $p_1 < p_2 < p_3 < \cdots$ are positive integers and $c_1, c_2, c_3,\ldots$ are constant. Here $h$ is some discretization of the analytical calculation, and as $h \to 0$, $V_h \to V$.
\vs

*Goal.* Construct a higher-order accurate method approximating $V$ out of a lower-order accurate method $V_h$.
\vs

*Procedure.* (/Richardson extrapolation/) Form a suitable linear combination of $V_h$ and $V_{h/2}$ approximating $V$ which removes the leading error term $h^{p_1}$.
\[
\text{Result: }\qquad \frac{2^{p_1}}{2^{p_1}-1} V_{h/2} - \frac{1}{2^{p_1}-1} V_h = V + O(h^{p_2})
\]
#+LATEX: \endgroup

**** Richardson Extrapolation: Illustration of Method
Write down
\begin{align}
V_h & = V + c_1 h^{p_1} + c_2 h^{p_2} + c_3 h^{p_3} + \cdots \label{eq:Vh}\\
V_{h/2} & = V + c_1 \left(\frac{h}{2}\right)^{p_1} + c_2 \left(\frac{h}{2}\right)^{p_2} + c_3 \left(\frac{h}{2}\right)^{p_3} + \cdots \label{eq:Vh2}
\end{align}
Multiplying \eqref{eq:Vh} by $a$ and \eqref{eq:Vh2} by $b$ and summing up, we obtain
\begin{multline*}
aV_h + bV_{h/2} = \underbrace{(a+b)}_{=1} V + c_1 \underbrace{\left[ a + \frac{b}{2^{p_1}} \right]}_{=0} h^{p_1}
+ c_2 \left[ a + \frac{b}{2^{p_2}} \right] h^{p_2} + \cdots
\end{multline*}
Find $a$ and $b$ satisfying
\begin{equation*}
\left\{\;
\begin{aligned}
& a + b = 1 \\
& a + \frac{b}{2^{p_1}} = 0
\end{aligned}
\right.
\quad\Longrightarrow\quad
\left\{\;
\begin{aligned}
& a =  -\frac{1}{2^{p_1}-1} \\
& b = \frac{2^{p_1}}{2^{p_1}-1}
\right.
\end{aligned}
\end{equation*}

**** Another Derivation of 2nd-Order Forward Difference
*Exercise.* Derive $D_h^{\rm [2f]}\{f\}(x)$ using Richardson extrapolation on $V_h = D_h^{\rm [1f]}\{f\}(x)$.
** DONE Numerical Integration
:PROPERTIES:
:EXPORT_FILE_NAME: 29-num-integ
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT
*** Introduction
**** Quadrature Problem
#+ATTR_LATEX: :options {0.55\linewidth}
#+BEGIN_minipage
Consider the definite integral
\[
  I \{f\} = \int_a^b f(x) \; dx
\]
which represents the /net area/ of the region between the curve $y = f(x)$ and the $x\text{-axis}$ on $[a, b]$.
#+END_minipage
\hfill
#+ATTR_LATEX: :options {0.4\linewidth}
#+BEGIN_minipage
#+BEGIN_EXPORT latex
\begin{image}[\linewidth]
  \begin{tikzpicture}[font=\footnotesize,
    declare function = {f(\x) = -sin(deg(\x)) + 3;}]
    \begin{axis}[
      domain=-.2:7, xmin =-.2,xmax=7,ymax=5,ymin=-.2,
      width=2.2in,
      height=1.4in,
      xtick={1,6},
      xticklabels={$a$,$b$},
      ytick style={draw=none},
      yticklabels={},
      axis lines=center, xlabel=$x$, ylabel=$y$,
      every axis y label/.style={at=(current axis.above origin),anchor=south},
      every axis x label/.style={at=(current axis.right of origin),anchor=west},
      axis on top,
      ]
      \addplot [draw=none,fill=fillp,domain=1:6, smooth] {f(x)} \closedcycle;
      \addplot [thick,penColor, smooth] {f(x)};
    \end{axis}
  \end{tikzpicture}
\end{image}
#+END_EXPORT
#+END_minipage

\vfill
We seek to approximate it numerically by a weighted sum
\[
  I \{f\} \approx \sum_{i=1}^{n} \omega_i f(x_i) .
\]
 - $\omega_i$'s are called the \textit{weights};
 - $x_i$'s are called the \textit{nodes} for the particular numerical method used.

**** Some Questions
#+LATEX: \begingroup \small
*Q1. Why do we care?*
 - An exact antiderivative of $f$ is not accessible
 - $f$ may be known at limited points
\vfill

*Q2. How do we do?*
 - Replace $f(x)$ by an approximate function $p(x)$ and integrate it instead.
\vfill

*Q3. What are good candidates for $p(x)$?*
 - In choosing $p(x)$, we require that (a) $f(x) - p(x)$ is not too large and (b) $p(x)$ can be exactly integrated (by hand).
   - (piecewise) constant $\longrightarrow$ (composite) midpoint rule
   - (piecewise) linear $\longrightarrow$ (composite) trapezoidal rule
   - (piecewise) quadratic $\longrightarrow$ (composite) Simpson's rule
#+LATEX: \endgroup

**** Newton-Cotes Formulas
/Newton-Cotes methods/ are a collection of numerical integration methods in which nodes are equally spaced in $[a,b]$. Let $m = (a+b)/2$.
\vs

***** left column                                                          :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+ATTR_LATEX: :options {0.54\linewidth}
#+BEGIN_minipage
#+LATEX: \begingroup\small
#+ATTR_BEAMER: :overlay <+->
 - *Midpoint Method:*
   \[
     I^{[\rm m]}\{f\} = f(m) (b-a)
   \]
 - *Trapezoidal Method:*
   \[
     I^{[\rm t]} \{f\} = \frac{1}{2} \left( f(a) + f(b) \right) (b-a)
   \]
 - *Simpson's Method:*
   \[
     I^{[\rm s]} \{f\} = \frac{1}{6} \left( f(a) + 4f(m) + f(b) \right) (b-a)
   \]
#+LATEX: \endgroup
#+END_minipage
\hfill
***** right column (images)                                                :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+ATTR_LATEX: :options {0.45\linewidth}
#+BEGIN_minipage
#+BEGIN_EXPORT latex
\begin{image}[\linewidth]
  \begin{tikzpicture}[font=\footnotesize,
    declare function =
    {f(\x) = -sin(deg(\x)) + 3;
      p(\x) = 0.22*\x + 1.93;
      q(\x) = -0.10*\x^2 + 0.93*\x + 1.33;}]
    \begin{axis}[
      domain=-.2:7, xmin =-.2,xmax=7,ymax=5,ymin=-.2,
      width=2.2in,
      height=1.4in,
      xtick={1,3.5,6},
      xticklabels={$a$,$m$,$b$},
      ytick style={draw=none},
      yticklabels={},
      axis lines=center, xlabel=$x$, ylabel=$y$,
      every axis y label/.style={at=(current axis.above origin),anchor=south},
      every axis x label/.style={at=(current axis.right of origin),anchor=west},
      axis on top,
      ]
      % midpoint
      \only<1>{
        \addplot [draw=none,fill=fillp] plot coordinates { (1, {f(3.5)}) (6, {f(3.5)}) } \closedcycle;
        \addplot [draw=penColor4,dashed] plot coordinates {(-0.2,{f(3.5)}) (7,{f(3.5)})};
        \addplot [penColor,thin,dotted] plot coordinates { (3.5,0) (3.5, {f(3.5)})};
        \addplot [mark=*,red] coordinates {(3.5,{f(3.5)})};
      }
      % trapezoid
      \only<2>{
        \addplot [draw=none,fill=fillp] plot coordinates { (1, {f(1)}) (6, {f(6)}) } \closedcycle;
        \addplot [draw=penColor4,dashed] plot coordinates {(-0.2,{p(-0.2)}) (7,{p(7)})};
        \addplot [mark=*,red] coordinates {(1,{f(1)})};
        \addplot [mark=*,red] coordinates {(6,{f(6)})};
      }
      % simpson
      \only<3>{
        \addplot [draw=none,fill=fillp,domain=1:6,smooth] {q(x)} \closedcycle;
        \addplot [draw=penColor4,dashed,smooth] {q(x)};
        \addplot [penColor,thin,dotted] plot coordinates { (3.5,0) (3.5, {f(3.5)})};
        \addplot [mark=*,red] coordinates {(1,{f(1)})};
        \addplot [mark=*,red] coordinates {(3.5,{f(3.5)})};
        \addplot [mark=*,red] coordinates {(6,{f(6)})};
      }
      % function f
      \addplot [thick,penColor, smooth] {f(x)};
    \end{axis}
  \end{tikzpicture}
\end{image}
#+END_EXPORT
\vfill
#+LATEX: \begingroup\small
#+BEGIN_CENTER
@@beamer:\only<1>{Midpoint method: one node}@@
@@beamer:\only<2>{Trapezoid method: two nodes}@@
@@beamer:\only<3>{Simpson's method: three node}@@
#+END_CENTER
#+LATEX: \endgroup
#+END_minipage

***** quadratic interpolant                                                       :noexport:
-0.1010897554*x^2 + 0.9318055840*x + 1.327813187

#+BEGIN_SRC maple
with(CurveFitting):
f := unapply(-sin(x)+3, x):
PolynomialInterpolation([[1,f(1)],[3.5,f(7/2)],[6,f(6)]], x):
evalf(%);
#+END_SRC

#+RESULTS:
: PolynomialInterpolation([[1, -sin(1)+3], [3.5, -sin(7/2)+3], [6, -sin(6)+3]],x)

**** Convergence of Midpoint and Trapezoidal Methods
Both of $I^{\rm [m]} \{f\}$ and $I^{\rm [t]} \{f\}$ are \textbf{third-order} accurate:
#+LATEX: \begingroup\small
\begin{align*}
I^{\rm [m]} \{f\} - I\{f\} & = \underbrace{ -\frac{1}{24} f''(m) (b-a)^3 }_{\text{leading error}} - \frac{1}{1920} f^{(4)}(m) (b-a)^5 + O\left((b-a)^7\right), \\
I^{\rm [t]} \{f\} - I\{f\} & = \underbrace{ \frac{1}{12} f''(m) (b-a)^3 }_{\text{leading error}} + \frac{1}{480} f^{(4)}(m) (b-a)^5 + O\left((b-a)^7\right).
\end{align*}
#+LATEX: \endgroup

**** Convergence of Simpson's Methods
The Simpson's method is \textbf{fifth-order} accurate:
#+LATEX: \begingroup\small
\begin{align*}
I^{\rm [s]} \{f\} - I\{f\} & = \underbrace{ \frac{1}{2880} f^{(4)}(m) (b-a)^5 }_{\text{leading error}} + O\left((b-a)^7\right), \\
\end{align*}
#+LATEX: \endgroup

**** Derivation of Simpson's Method via Extrapolation
The Simpson's method can be derived by forming a suitable linear combination of two 3rd-order accurate methods, $I^{\rm [m]}\{f\}$ and $I^{\rm [t]}\{f\}$:
\vs


We know that
#+LATEX: \begingroup\small
\begin{align*}
I^{\rm [m]} \{f\} & = I\{f\} - \frac{1}{24} f''(m) (b-a)^3 - \frac{1}{1920} f^{(4)}(m) (b-a)^5 + O\left((b-a)^7\right), \\
I^{\rm [t]} \{f\} & = I\{f\} + \frac{1}{12} f''(m) (b-a)^3 + \frac{1}{480} f^{(4)}(m) (b-a)^5 + O\left((b-a)^7\right).
\end{align*}
#+LATEX: \endgroup

It follows that
\[
\underbrace{\frac{2}{3} I^{\rm [m]}\{f\} + \frac{1}{3} I^{\rm [t]}\{f\}} = I\{f\} + \frac{1}{2880} f^{(4)}(m) (b-a)^5 + O \left( (b-a)^7 \right).
\]

The underbraced left-hand side is the Simpson's method. (Confirm it.)
*** Composite Methods
**** Composite Trapezoidal and Midpoint Methods
For better accuracy, we can subdivide the interval $[a, b]$ into equispaced subintervals
\begin{equation*}
  a = x_1 < x_2 < \cdots < x_n = b
  \quad\text{with }
  x_i = a + (i-1)h
  \text{ and }
  h = \frac{b-a}{n-1}.
\end{equation*}

 - *Composite Midpoint Method*:
   \begin{equation*}
   I_h^{\rm [m]}\{f\} = \sum_{i=1}^{n-1} f(x_{i+1/2}) h,
   \end{equation*}
   where $x_{i+1/2} = (x_i + x_{i+1})/2$.
 - *Composite Trapezoidal Method*:
   \begin{equation*}
   I_h^{\rm [t]}\{f\} =  \sum_{i=1}^{n-1} \frac{1}{2} \left( f(x_i) + f(x_{i+1}) \right) h
     = \frac{1}{2} \left( f(x_1) + f(x_n) \right) h
   + \sum_{i=2}^{n-1} f(x_i) h.
   \end{equation*}

**** Composite Simpson's Methods
 - *Composite Simpson's Method*:
   \begin{equation*}
   I_h^{\rm [s]}\{f\} =  \sum_{i=1}^{n-1} \frac{1}{6} \left( f(x_i) + 4 f(x_{i+1/2}) + f(x_{i+1}) \right) h.
   \end{equation*}

**** Convergence of Composite Methods
The composite midpoint and trapezoidal methods are *second-order* accurate while the composite Simpson's method is *fourth-order* accurate:
\begin{align*}
I_h^{\rm [m]} \{f\} - I\{f\} & = - \frac{ f''(\xi_m) }{24} (b-a) h^2, \\
I_h^{\rm [t]} \{f\} - I\{f\} & = \frac{ f''(\xi_t) }{12} (b-a) h^2, \\
I_h^{\rm [s]} \{f\} - I\{f\} & = - \frac{ f^{(4)}(\xi_s) }{2880} (b-a) h^4.
\end{align*}

*** Misc                                                                            :noexport:
**** MATLAB Implementation
MATLAB has four functions that calculate 1-D definite integrals:
\vs
 - =quad=: adaptive Simpson's method
 - =quadl=: Gauss-Lobatto quadrature
 - =quadgk=: Gauss-Kronrod quadrature
 - =integral=
\vfill

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
  a = 2; b = 5;
  f = @(x) x.*exp(x);
  f_int = @(x) (x - 1).*exp(x);
  exact_area = f_int(b) - f_int(a);
  int1 = quad(f, a, b);
  int2 = quadl(f, a, b);
  int3 = quadgk(f, a, b);
  int4 = integral(f, a, b);
#+END_SRC

** TODO Review: Numerical Calculus
*** TODO Problem Solving Session (1/2)
CLOSED: [2021-04-12 Mon 13:18]
**** Notes                                                                         :noexport:

**** Key Tools and Techniques
***** Lagrange Polynomials
***** Summary of Key Formulas (Differentiation)
***** Summary of Key Formulas (Integration)
***** Formula for Second Derivatives
***** Richardson Extrapolation
***** Extrapolation Exercise (Difference Formula)
 - Done 2nd-order forward difference in lecture.
 - Need to derive 3rd-order forward difference for homework.
***** Extrapolation Exercise (Quadrature Formula)
 - Derived Simpson from midpoint and trapezoidal.
 - How about composite case?
***** Optimal $h$
 - Done 2nd-order centered difference.
 - How about in general?
***** Quadrature Exercises
 - Euler Spiral
 - Area of an ellipse

*** TODO Problem Solving Session (2/2)
CLOSED: [2021-04-13 Tue 16:24]
**** Exercise Problems
:PROPERTIES:
:EXPORT_TITLE: Numerical Calculus: Problem Solving Session
:EXPORT_FILE_NAME: numcalc-prob-solving
:EXPORT_OPTIONS: H:1 date:nil author:nil
:END:
***** Frontmatters                                                         :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}
#+END_EXPORT

***** Optimal $h$
*Question.* Suppose that a function $f(x)$ is numerically calculated by the following procedure.
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
function y = f(x)
    a = 1; b = cos(x);
    for i = 1:5
        c = b;
        b = sqrt(a.*b);
        a = (a + c)/2;
    end
    y = (pi/2)./a;
end
#+END_SRC

Compute $f'(\pi/4)$ as accurately as possible using a method of numerical differentiation.

***** Logarithmic Integral
The *logarithmic integral* is a special mathematical function defined by the equation
\[
{\rm li}(x) = \int_2^x \frac{dt}{\ln t}.
\]
Find ${\rm li}(200)$ by means of the composite trapezoid method.

***** Quadrature Exercise
Compute
\[
\int_0^{\infty} e^{-x^2} \; dx = \frac{\sqrt{\pi}}{2}
\]
by using small and large values for the limits of integration and applying a numerical method. Then compute it by making the change of variable
\[
x = -\ln t.
\]

***** Quadrature Exercise
Find the area of the ellipse $y^2 + 4x^2 = 1$.

***** Airplane Velocity
The radar stations $A$ and $B$, separated by the distance $a = 500$ m, track a plane $C$ by recording the angles $\alpha$ and $\beta$ at one-second intervals.  Your goal, back at air traffic control, is to determine the speed of the plane.

#+ATTR_LATEX: :float t  :width 0.7\linewidth
[[../img/plane_diagram.png]]

* DONE Chapter 6: Initial Value Problems for ODEs
** DONE Basics of Initial Value Problems
:PROPERTIES:
:EXPORT_FILE_NAME: 30-ivp-intro
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

% \begin{frame}{Contents}
%   \tableofcontents
% \end{frame}
#+END_EXPORT
*** Introduction                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
**** Initial Value Problem
***** Problem Statement (IVP: 1st-Order Scalar)
Find a function $u$ which satisfies a scalar, first-order initial value problem (IVP)
\begin{equation*}
  \left\{ \;
    \begin{aligned}
      & u' = f(t, u), \quad t_0 \le t \le T, \\
      & u(t_0) = u_0.
    \end{aligned}
  \right.
\end{equation*}

**** Example
***** Population Growth                                                          :B_example:
:PROPERTIES:
:BEAMER_env: example
:END:
Let $u(t)$ be the size of a population at time $t$ and let $u_0$ be the initial population. The growth of population can be modeled using IVPs:
\vs

*Exponential Growth.*
\[
\frac{du}{dt} = ku, \quad u(0) = u_0
\quad\Longrightarrow\quad
u(t) = u_0 e^{kt}.
\]

*Logistic Model.*
\[
\frac{du}{dt} = ku - ru^2, \quad u(0) = u_0
\quad\Longrightarrow\quad
u(t) = \frac{ k/r }{ 1 + \left( \frac{k}{ru_0} - 1 \right) e^{-kt} }.
\]

**** Analytical Solutions
 - A linear first-order differential equation of the form
  \[
  u' = \underbrace{ g(t) + h(t) u }_{=f(t,u)},
  \]
  can be solved in terms of standard integrals:
  \[
  u(t) = \frac{1}{\mu(t)} \left( u_0 + \int_a^t \mu(s) g(s) \; ds \right),
  \quad\text{where $\ds \mu(t) = \exp \left[ - \int h(t) \; dt \right]$.}
  \]
  But the integrals cannot be done in closed form most of the times.
 - For a nonlinear equation, no analytical formula may be available for its solutions.

**** Is Studying 1st-Order ODEs Enough?
Consider an arbitrary $n^{\text{th}}$ order equation
\[
y^{(n)} = f(t, y, y', \ldots, y^{(n-1)}).
\]

Introduce $u_1, u_2, \ldots, u_n$ defined by
\[
u_1 = y,\; u_2 = y', \; u_3 = y'', \ldots, \; u_n = y^{(n-1)}.
\]
Then
\begin{align*}
u_1' & = u_2, \\
u_2' & = u_3, \\
& \vdots \\
u_{n-1}' & = u_n,
\end{align*}
and the given differential equation becomes
\[
u_n' = f(t, u_1, u_2, \ldots, u_n).
\]

**** Is Studying 1st-Order ODEs Enough? {{{cont}}}
The above is a system of $n$ first-order differential equations for $n$ unknown functions, the more general form of which is
#+BEGIN_EXPORT latex
\begin{equation*}
  \left\{ \;
    \begin{aligned}
      u_1' & = f_1(t, u_1, u_2, \ldots, u_n), \\
      u_2' & = f_2(t, u_1, u_2, \ldots, u_n), \\
           & \vdots \\
      u_{n}' & = f_n(t, u_1, u_2, \ldots, u_n), \\
    \end{aligned}
  \right. \tag{$\star$}
\end{equation*}
#+END_EXPORT

Not only a single higher-order equation, but also a system of arbitrary-order equations can be transformed to a system of the form ($\star$).

**** Initial Value Problem
***** Problem Statement (IVP: 1st-Order System)
Find functions $u_1, u_2, \ldot, u_n$ satisfying
\begin{equation*}
\left\{ \;
\begin{aligned}
u_1' &= f_1(t, u_1, u_2, \ldots, u_n), \\
& \hspace{0.5em} \vdots \\
u_n' &= f_n(t, u_1, u_2, \ldots, u_n),
\end{aligned}
\qquad \text{for $t_0 \le t \le T,$}
\end{equation*}
and
\[
u_1(t_0) = u_{1,0}, \; \ldots,\; u_n(t_0) = u_{n,0}.
\]
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs

The above can be written compactly using vector notation:
\[
\bu' = \mathbf{f} (t, \bu), \quad \bu(t_0) = \bu_0.
\]

**** Example
***** Mechanical Vibration; Spring-Mass System                            :B_example:
:PROPERTIES:
:BEAMER_env: example
:END:
A *spring-mass system* consisting of a block attached to a (massless) spring hanging from a rigid support with is described by a second-order linear constant coefficient IVP
\[
my'' + \gamma y' + k y = 0, \quad u(0) = u_0, \quad u'(0) = v_0,
\]
where $m$ is the mass of block, $\gamma$ the damping coefficient, and $k$ the spring constant.

**** MATLAB's =ODE45=
MATLAB has many built-in ODE solvers, one of which is =ode45=.
\vs

*Syntax:*
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
[t,u] = ode45(odefun,tspan,ics)   % tspan = [t0 T]
[tvec,u] = ode45(odefun,tvec,ics) % tvec = vector of time nodes
#+END_SRC

**** Example: Solution Values at Automatically Selected Times
There is no closed-form formula for the solution of $u' = \sin \left[ (u+t)^2 \right]$, but we can find its numerical solution using =ode45=.
\vs

***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.48
:END:

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
 f = @(t, u) sin( (t+u).^2 );
 [t, u] = ode45(f, [0, 4], -1);
 plot(t, u), grid on
#+END_SRC

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.52
:END:
#+LATEX: \vspace{0pt}
#+ATTR_LATEX: :width 0.8\linewidth
[[../img/ode45demo.eps]]

****** code used to generate figure                                              :noexport:
#+BEGIN_SRC matlab
f = @(t,u) sin( (t+u).^2 );
[t,u] = ode45(f, [0,4], -1);
plot(t,u,'LineWidth',2), grid on
title('Solution of $u'' = \sin[(t+u)^2]$', 'Interpreter', 'LaTeX', 'FontSize', 13)
cd ~/3607/img
print -depsc 'ode45demo'
#+END_SRC

#+RESULTS:
#+begin_example
f = @(t,u) sin( (t+u).^2 );
[t,u] = ode45(f, [0,4], -1);
plot(t,u,'LineWidth',2), grid on
tCouldn't create JOGL canvas--using painters
itle('Solution of $u'' = \sin[(t+u)^2]$', 'Interpreter', 'LaTeX', 'FontSize', 13)
cd ~/3607/img
print -depsc 'ode45demo'
Couldn't create JOGL canvas--using painters
'org_babel_eoe'
ans =
    'org_babel_eoe'
#+end_example

**** Example: Solution Values at User-Determined Time Nodes
We revisit the same equation $u' = \sin \left[ (u+t)^2 \right]$ here. This time, we require =ode45= to solve for values at prescribed time nodes.
\vs

***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.48
:END:

#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
 f = @(t, u) sin( (t+u).^2 );
 t = linspace(0, 4, 60)';
 [t, u] = ode45(f, t, -1);
 plot(t, u, '.'), grid on
#+END_SRC

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.52
:END:
#+LATEX: \vspace{0pt}
#+ATTR_LATEX: :width 0.8\linewidth
[[../img/ode45demo2.eps]]

****** code used to generate figure                                              :noexport:
#+BEGIN_SRC matlab
f = @(t, u) sin( (t+u).^2 );
[t, u] = ode45(f, [0, 4], -1);
plot(t, u,'.','MarkerSize',15), grid on
title('Solution at 60 Points', 'Interpreter', 'LaTeX', 'FontSize', 13)
cd ~/3607/img
print -depsc 'ode45demo2'
#+END_SRC

#+RESULTS:
#+begin_example
f = @(t, u) sin( (t+u).^2 );
[t, u] = ode45(f, [0, 4], -1);
plot(t, u,'.','MarkerSize',15), grid on
tCouldn't create JOGL canvas--using painters
itle('Solution at 60 Points', 'Interpreter', 'LaTeX', 'FontSize', 13)
cd ~/3607/img
print -depsc 'ode45demo2'
Couldn't create JOGL canvas--using painters
'org_babel_eoe'
ans =
    'org_babel_eoe'
#+end_example

** Euler's Method
:PROPERTIES:
:EXPORT_FILE_NAME: 31-euler-and-rk
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

% \begin{frame}{Contents}
%   \tableofcontents
% \end{frame}
#+END_EXPORT
*** Euler's Method
**** Numerical Solutions
***** Problem Statement (IVP: 1st-Order Scalar)
Find a function $u$ which satisfies a scalar, first-order initial value problem (IVP)
\begin{equation*}
  \left\{ \;
    \begin{aligned}
      & u' = f(t, u), \quad t_0 \le t \le T, \\
      & u(t_0) = u_0.
    \end{aligned}
  \right.
\end{equation*}
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs

*Strategy to find numerical solutions.*
 1. Divide $[t_0, T]$ into $n$ equispaced subintervals:
    \[
    t_j = t_0 + j h, \quad h = \frac{T - t_0}{n}, \quad j = 0, \ldots, n.
    \]
 2. Start with initial date $u_0$ and iterate to obtain $u_1, u_2, \ldots, u_n$ where
    \[
    u_j \approx u(t_j).
    \]

**** Euler's Method
***** Euler's Method (vector form)
Euler's method for the initial value problem $\bu' = \mathbf{f}(t,\bu)$, $\bu(t_0) = \bu_0$ is given by
\[
\bu_{j+1} = \bu_j + h \mathbf{f}(t_j, \bu_j), \quad j = 0, \ldots, n-1.
\]
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs

 - Euler's method is the simplest method to solve 1st-order IVPs.
 - It is an explicit method because the answer at the new time level is explicitly given in terms of the older time level(s).
 - Idea: Replace $u'(t_j)$ by the forward difference formula:

***** notes                                                                       :noexport:
 - It is in the abstract form
   \[
   u_{j+1} = u_j + h \phi(t_j, u_j, h), \quad j = 0, \ldots, n-1,
   \]
   which is called a general *one-step method*.

**** Accuracy of Euler's Method

**** Implementation of Euler's Method (for Scalar IVPs)
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
function [t,u] = eulerivp(dudt,tspan,u0,n)
% EULERIVP   Euler's method for a scalar initial value problem.
% Input:
%   dudt     defines f in u'(t)=f(t,u). (function)
%   tspan    endpoints of time interval (2-vector)
%   u0       initial value              (scalar)
%   n        number of time steps       (integer)
% Output:
%   t        selected nodes             (vector, length n+1)
%   u        solution values            (vector, length n+1)
  t = linspace(tspan(1), tspan(2), n+1)';
  h = t(2)-t(1);
  u = zeros(n+1,1);
  u(1) = u0;
  for j = 1:n
      u(j+1) = u(j) + h*dudt(t(j),u(j));
  end
end
#+END_SRC

**** Implementation of Euler's Method (for System of IVPs)
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
function [t,u] = eulersys(dudt,tspan,u0,n)
% EULERSYS   Euler's method for a first-order IVP system.
% Input:
%   dudt     defines f in u'(t)=f(t,u). (function)
%   tspan    endpoints of time interval (2-vector)
%   u0       initial value              (vector, length m)
%   n        number of time steps       (integer)
% Output:
%   t        selected nodes             (vector, length n+1)
%   u        solution values            (array, (n+1)-by-m)
  t = linspace(tspan(1), tspan(2), n+1)';
  h = t(2)-t(1);
  m = length(u0);
  u = zeros(n+1,m);
  u(1,:) = u0(:)';
  for j = 1:n
      u(j+1,:) = u(j,:) + h*dudt(t(j),u(j,:));
  end
end
#+END_SRC

*** Runge--Kutta Methods
**** Runge--Kutta Methods
/Runge--Kutta methods/ are one of the most-used types of methods for IVPs. They boost the accuracy beyond the first-order by evaluating the ODE function $f(t,u)$ more than once each time step.

 - *Idea*: Come up with a better ``slope'' approximation.
 - *RK Model* ($k\text{-stage}$ method) Determine $c_1, \ldots, c_{k-1}$; $a_{11}, a_{21}, a_{22}, \ldots, a_{k-1,1}, \ldots, a_{k-1,k-1}$; and $b_1, \ldots, b_k$ such that the solution values approximated by
   \begin{align*}
     s_1 & = f(t_j, \bu_j) \\
     s_2 & = f(t_j + c_1 h, \bu_j + a_{11}s_1 h) \\
     \vdots \\
     s_k & = f(t_j + c_{k-1} h, \bu_j + (a_{k-1,1}s_1 + a_{k-1,2}s_2 + \cdots + a_{k-1,k-1}s_{k-1}) h) \\
     \bu_{j+1} & = \bu_j + (b_1 s_1 + b_2 s_2 + \cdots + b_k s_k)h
   \end{align*}
   reduces errors.
**** Runge--Kutta Methods {{{cont}}}
An RK method is often presented as just a table of the coefficients to be determined, /e.g./,

#+ATTR_LATEX: :align r|ccccc
| $0$       |             |          |        |               |       |
| $c_1$     | $a_{11}$    |          |        |               |       |
| $c_2$     | $a_{21}$    | $a_{22}$ |        |               |       |
| \vdots    | \vdots      |          | \ddots |               |       |
| $c_{k-1}$ | $a_{k-1,1}$ | \cdots   | \cdots | $a_{k-1,k-1}$ |       |
|-----------+-------------+----------+--------+---------------+-------|
|           | $b_1$       | $b_2$    | \cdots | $b_{k-1}$     | $b_k$ |

**** Second-Order RK Methods
There are several 2nd-order RK methods.
\vs

#+ATTR_LATEX: :options {0.5\linewidth}
#+BEGIN_minipage
 - *Improved Euler Method*:

   #+ATTR_LATEX: :align c|cc
   | $0$           |               |     |
   | $\frac{1}{2}$ | $\frac{1}{2}$ |     |
   |---------------+---------------+-----|
   |               | $0$           | $1$ |

 - *Modified Euler Method*:

   #+ATTR_LATEX: :align c|cc
   | $0$ |               |               |
   | $1$ | $1$           |               |
   |-----+---------------+---------------|
   |     | $\frac{1}{2}$ | $\frac{1}{2}$ |

 - *Heun's Method*:

   #+ATTR_LATEX: :align c|cc
   | $0$           |               |               |
   | $\frac{2}{3}$ | $\frac{2}{3}$ |               |
   |---------------+---------------+---------------|
   |               | $\frac{1}{4}$ | $\frac{3}{4}$ |
#+END_minipage

**** Implementation of Improved Euler Method
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
function [t,u] = ie2(dudt,tspan,u0,n)
% IE2   Improved Euler method for a first-order IVP system.
% Input:
%   dudt     defines f in u'(t)=f(t,u). (function)
%   tspan    endpoints of time interval (2-vector)
%   u0       initial value              (vector, length m)
%   n        number of time steps       (integer)
% Output:
%   t        selected nodes             (vector, length n+1)
%   u        solution values            (array, (n+1)-by-m)
  t = linspace(tspan(1), tspan(2), n+1)';
  h = t(2)-t(1);
  m = length(u0);
  u = zeros(n+1,m);
  u(1,:) = u0(:)';
  for j = 1:n
      s1 = dudt(t(j), u(j,:));
      s2 = dudt(t(j)+h, u(j,:)+.5*s1*h);
      u(j+1,:) = u(j,:) + s2*h;
  end
end
#+END_SRC

**** The RK Method
The most commonly used RK method is the following 4th-order method:

#+ATTR_LATEX: :align c|cccc
| $0$           |               |               |               |               |
| $\frac{1}{2}$ | $\frac{1}{2}$ |               |               |               |
| $\frac{1}{2}$ | $0$           | $\frac{1}{2}$ |               |               |
| $1$           | $0$           | $0$           | $1$ |               |
|---------------+---------------+---------------+---------------+---------------|
|               | $\frac{1}{6}$ | $\frac{1}{3}$ | $\frac{1}{3}$ | $\frac{1}{6}$ |

**** Implementation of the RK4 Method
:PROPERTIES:
:BEAMER_opt: shrink=2
:END:
#+ATTR_LATEX: :options style=matlab
#+BEGIN_SRC matlab
function [t,u] = rk4(dudt,tspan,u0,n)
% RK4        4th-order Runge-Kutta for a first-order IVP system.
% Input:
%   dudt     defines f in u'(t)=f(t,u). (function)
%   tspan    endpoints of time interval (2-vector)
%   u0       initial value              (vector, length m)
%   n        number of time steps       (integer)
% Output:
%   t        selected nodes             (vector, length n+1)
%   u        solution values            (array, (n+1)-by-m)
  t = linspace(tspan(1), tspan(2), n+1)';
  h = t(2)-t(1);
  m = length(u0);
  u = zeros(n+1,m);
  u(1,:) = u0(:)';
  for j = 1:n
      % Fill in the blank
  end
end
#+END_SRC

** scratch
#+BEGIN_SRC matlab
f = @(t,y) 6*t^2*y^2 - y^3;

#+END_SRC

* TODO Module 5: Spectral Theory (EVD and SVD) <-- Not this semester
** TODO Eigenvalue Decomposition
:PROPERTIES:
:EXPORT_FILE_NAME: 25-evd
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT
*** Complex Numbers and Complex Arrays
**** Complex Numbers
In what follows, we assume all scalars, vectors, and matrices may be complex.
\vs

*Notation.*
 - \(\RR\): the set of all real numbers
 - \(\CC\): the set of all complex numbers, \emph{i.e.},
   \[
     \{ z = x+iy \,|\, x, y \in \RR \}
     \quad\text{where  \(i = \sqrt{-1}\).}
   \]

**** Complex Numbers in MATLAB
Let $z = x + iy \in \CC$.
# \vs
#  - =real(z)=: the real part of $z$, $\re z = x$
#  - =imag(z)=: the imaginary part of $z$, $\im z = y$
#  - =conj(z)=: the (complex) conjugate of $z$, $\conj{z} = x - iy$
#  - =abs(z)=: the modulus of $z$, $\abs{z} = \sqrt{x^2 + y^2}$
#  - =angle(z)=: the argument of $z$: if $\theta = \arg(z)$, then $\tan \theta = y/x$

#+ATTR_LATEX: :align lcc
| MATLAB     | Name                  | Notation   |
|------------+-----------------------+------------|
| =real(z)=  | real part of $z$      | $\re z$    |
| =imag(z)=  | imaginary part of $z$ | $\im z$    |
| =conj(z)=  | conjugate of $z$      | $\conj{z}$ |
| =abs(z)=   | modulus of $z$        | $\abs{z}$  |
| =angle(z)= | argument of $z$       | $\arg(z)$  |

**** Euler's Formula
 - Recall that the Maclaurin series for \(e^t\) is
   \[
     e^t = 1 + t + \frac{t^2}{2} + \cdots + \frac{t^n}{n!} + \cdots = \sum_{n=0}^{\infty} \frac{t^n}{n!}, \quad -\infty < t < \infty.
   \]

 - Replacing \(t\) by \(it\) and separating real and imaginary parts (using the cyclic behavior of powers of \(i\)), we obtain
   \begin{align*}
     e^{it}
     & = \underbrace{\sum_{k=0}^{\infty} \frac{(-1)^k t^{2k}}{(2k)!}}_{\cos(t)} {}+{} i \underbrace{\sum_{k=0}^{\infty} \frac{(-1)^k t^{2k+1}}{(2k+1)!}}_{\sin(t)}
   \end{align*}
 - The result is called the *Euler's formula*.
   \[
   \boxed{e^{it} = \cos(t) + i \sin(t).}
   \]

**** Polar Representation and Complex Exponential
 - *Polar representation:*  A complex number $z = x + iy \in \CC$ can be written as $z = r e^{i\theta}$ where
   \[
     r = \abs{z}, \quad \tan\theta = \frac{y}{x}.
   \]
 - *Complex exponentiation:*
   \[
     e^z = e^{x + iy} = e^x e^{iy} = e^x \left(\cos y + i \sin y\right).
   \]

**** Complex Vectors
Denote by $\CC^n = \CC^{n\times 1}$ the space of all column vectors of $n$ /complex/ elements.
\vs
 - The *hermitian* or *conjugate transpose* of $\bu \in \CC^n$ is denoted by $\bu^*$:
   \[
     \bu^* \in \CC^{1 \times n}.
   \]
 - The inner product of $\bu, \bv \in \CC^n$ is defined by
   \[
     \bu^* \bv = \sum_{k=1}^{n} \conj{u}_k v_k.
   \]
   The 2-norm for complex vectors is defined in terms of this inner product:
   \[
     \Norm{\bu}{2}^2 = \bu^* \bu.
   \]
**** Complex Matrices
Denote by $\CC^{m \times n}$ the space of all complex matrices with $m$ rows and $n$ columns.
\vs
 - The *hermitian* or conjugate transpose of $A \in \CC^{m \times n}$ is denoted by $A^*$:
   \[
     A^* = \big( \conj{A} \big)\tp = \conj{\big(A\tp\big)} \in \CC^{n \times m}.
   \]
 - A *unitary* matrix is a complex analogue of an orthogonal matrix. If $U \in \CC^{n \times n}$ is unitary, then
   \[
   U^*U = UU^* = I
   \]
   and
   \[
   \Norm{U\bz}{2} = \Norm{\bz}{2}, \quad \text{for any $\bz \in \CC^{n}$.}
   \]

**** Complex Matrices: Some Analogies
#+ATTR_LATEX: :align l|c|c
|                | Real                                          | Complex                                      |
|----------------+-----------------------------------------------+----------------------------------------------|
|                |                                               |                                              |
| Norm           | $\Norm{\bv}{2} = \sqrt{\bv\tp \bv}$           | $\Norm{\bu}{2} = \sqrt{\bu^* \bu}$           |
|                |                                               |                                              |
|----------------+-----------------------------------------------+----------------------------------------------|
|                |                                               |                                              |
| Symmetry       | $S\tp = S$                                    | $S^* = S$                                    |
|                | @@latex: \footnotesize @@ (symmetric matrix)  | @@latex: \footnotesize @@ (hermitian matrix) |
|                |                                               |                                              |
|----------------+-----------------------------------------------+----------------------------------------------|
|                |                                               |                                              |
| Orthonormality | $Q\tp Q = I$                                  | $U^* U = I$                                  |
|                | @@latex: \footnotesize @@ (orthogonal matrix) | @@latex: \footnotesize @@ (unitary matrix)   |
|                |                                               |                                              |
|----------------+-----------------------------------------------+----------------------------------------------|
|                |                                               |                                              |
| Householder    | $\ds H = I - \frac{2}{\bv\tp \bv} \bv \bv\tp$ | $\ds H = I - \frac{2}{\bu^* \bu} \bu \bu^*$  |
|                |                                               |                                              |


**** COMMENT Complex Matrices: Analogies
 - $Q \in \RR^{n \times n}$ is orthogonal if $Q^{-1} = Q\tp$; $U \in \CC^{n \times n}$ is unitary if $U^{-1} = U^*$. \vs
 - For $\bv \in \RR^n$ is $\Norm{\bv}{2} = \sqrt{\bv\tp \bv}$; for $\bu \in \CC^n$, $\Norm{\bu}{2} = \sqrt{\bu^* \bu}$. \vs
 - The Householder transformation for $\bv \in \RR^n$ is
   \[
     H = I - \frac{2}{\bv\tp \bv} \bv \bv\tp;
   \]
   for $\bu \in \CC^n$, it is
   \[
     H = I - \frac{2}{\bu^* \bu} \bu \bu^*,
   \]
   so $H$ is unitary.
*** Eigenvalue Decomposition (EVD)
**** Eigenvalue Decomposition
***** Eigenvalue Problem
  Find a scalar \textbf{eigenvalue} $\lambda$ and an associated nonzero \textbf{eigenvector} $\bv$ satisfying
  \[
    A\bv = \lambda \bv. \vs
  \]
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - The *spectrum* of $A$ is the set of all eigenvalues; the *spectral radius* is $\max_{j} \abs{\lambda_j}$.
 - The problem is equivalent to
   #+LATEX: \visible<0|handout:1>{
     \[
       (\lambda I - A) \bv = \bzero.
     \]
   #+LATEX: }
 - An eigenvalue of $A$ is a root of the *characteristic polynomial*
   #+LATEX: \visible<0|handout:1>{
     \[
       \det(\lambda I - A).
     \]
   #+LATEX: }

**** Eigenvalue Decomposition {{{cont}}}
#+LATEX: \begingroup \small
Let $A \in \CC^{n \times n}$ and suppose that $A \bv_k = \lambda_k \bv_k$ for $k \in \NN[1,n]$.

 - Then
   #+LATEX: \visible<1|handout:1>{
     \begin{align*}
       \begin{bmatrix}
         A\bv_1 & A\bv_2 & \cdots & A\bv_n
       \end{bmatrix}
       & =
         \begin{bmatrix}
           \lambda_1 \bv_1 & \lambda_2 \bv_2 & \cdots & \lambda_n \bv_n
         \end{bmatrix}, \\
       A
       \begin{bmatrix}[c|c|c|c]
         &&&\\
         \bv_1 & \bv_2 & \cdots & \bv_n \\
         &&&
       \end{bmatrix}
                                  & =
                                    \begin{bmatrix}[c|c|c|c]
                                      &&&\\
                                      \bv_1 & \bv_2 & \cdots & \bv_n \\
                                      &&&
                                    \end{bmatrix}
                                          \begin{bmatrix}
                                            \lambda_1 &&&\\
                                            &\lambda_2&&\\
                                            &&\ddots&\\
                                            &&&\lambda_n
                                          \end{bmatrix} \\
       \Longrightarrow \quad AV = VD. \tag{works for any square matrix.}
     \end{align*}
   #+LATEX: }
 - If $V$ is nonsingular, we can further write
   #+LATEX: \visible<1|handout:1>{
   \[
     A = V D V^{-1},
   \]
   #+LATEX: }
   which is called an \textbf{eigenvalue decomposition (EVD)} of $A$. If $\bv$ is an eigenvector of $A$, then so is $c \bv$, $c \neq 0$. Thus an EVD is not unique.
#+LATEX: \endgroup
**** Eigenvalue Decomposition {{{cont}}}
If $A$ has an EVD, we say that $A$ is *diagonalizable*; otherwise *nondiagonalizable*.
***** Diagonalizability                                                          :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
  If $A \in \CC^{n \times n}$ has $n$ distinct eigenvalues, then $A$ is diagonalizable.
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
*Notes.*
 - Let $A, B \in \CC^{n \times n}$. We say that $B$ is *similar* to $A$ if there exists a nonsingular matrix $X$ such that
   \[
     B = X A X^{-1}.
   \]
 - So /diagonalizability/ is /similarity to a diagonal matrix/.
 - Similar matrices share the same eigenvalues.

**** Calculating EVD in MATLAB
 - ~E = eig(A)~ \\
   produces a column vector =E= containing the eigenvalues of =A=.
 - ~[V, D] = eig(A)~ \\
   produces $V$ and $D$ in an EVD of $A$, $A = VDV^{-1}$.

*** Notes on EVD
**** Understanding EVD: Change of Basis
Let $X \in \CC^{n \times n}$ be a nonsingular matrix.
 - The columns $\bx_1, \bx_2, \ldots, \bx_n$ of $X$ form a basis of $\CC^{n}$.
 - Any $\bz \in \CC^{n}$ is uniquely written as
   \[
     \bz = X \bu = u_1 \bx_1 + u_2 \bx_2 + \cdots + u_n \bx_n.
   \]
   \vspace{-1.5em}
   - The scalars $u_1, \ldots, u_n$ are called the *coordinates* of $\bz$ with respect to the columns of $X$.
   - The vector $\bu = X^{-1} \bz$ is the representation of $\bz$ with respect to the basis consisting of the columns of $X$.

\vs
***** Upshot
Left-multiplication by $X^{-1}$ performs a *change of basis* into the coordinates associated with the columns of $X$.

**** Understanding EVD: Change of Basis {{{cont}}}
Suppose $A \in \CC^{n \times n}$ has an EVD $A = VDV^{-1}$. Then, for any $\bz \in \CC^n$, $\by = A \bz$ can be written as
#+LATEX: \visible<1|handout:1>{
  \[
    V^{-1} \by = D V^{-1} \bz.
  \]
#+LATEX: }

\vs
***** Interpretation
The matrix $A$ is a diagonal transformation in the coordinates with respect to the $V\text{-basis}$.

**** What Is EVD Good For?
Suppose $A \in \CC^{n \times n}$ has an EVD $A = V D V^{-1}$.
\vs
 - Economical computation of powers $A^k$:
   #+LATEX: \visible<1|handout:1>{
     \[
       A^k = V D^k V^{-1}.
     \]
   #+LATEX: }
 - Analyzing convergence of iterates $(\bx_1, \bx_2, \ldots)$ constructed by
   \[
     \bx_{j+1} = A \bx_j, \quad j = 1, 2, \ldots
   \]
   #+LATEX: \visible<1|handout:1>{
     If $\bx_1$ is an eigenvector associated to eigenvalue $\lambda$, then
     \[
       \bx_1
       \longrightarrow \lambda \bx_1
       \longrightarrow \lambda^2 \bx_1
       \longrightarrow \cdots
       \longrightarrow \lambda^{k-1} \bx_1
       \longrightarrow \cdots
     \]
   #+LATEX: }

**** Conditioning of Eigenvalues
***** Bauer-Fike                                                                 :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Let $A \in \CC^{n \times n}$ be diagonalizable, $A = VDV^{-1}$, with eigenvalues $\lambda_1, \ldots, \lambda_n$. If $\mu$ is an eigenvalue of $A + \delta A$ for a complex matrix $\delta A$, then
\[
  \min_{1 \le j \le n} \abs{\mu - \lambda_j} \le \kappa_2(V) \Norm{\delta A}{2}.
\]

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
# \vs
#  - If $V$ is ill-conditioned,
#  - If $\kappa(V) = 1$,

*** Code                                                                            :noexport:
#+BEGIN_SRC matlab
A = randn(800);
E = eig(A);

r = max(abs(E))  % spectral radius
th = linspace(0, 2*pi, 361);
z = r*exp(1i*th);  % e^(i theta) = cos(th) + i sin(th)

clf
plot(real(E), imag(E), '.')
axis equal, hold on
plot(real(z), imag(z))
#+END_SRC

** TODO Singular Value Decomposition
:PROPERTIES:
:EXPORT_FILE_NAME: 26-svd-overview
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Different Names for SVD                                                         :noexport:
The SVD is one of the most important ideas in applied mathematics and is ubiquitous in science and engineering. This is known by many different names such as
 - Karhunen--Lo\{`}eve expansion
 - principal component analysis
 - factor analysis
 - empirical orthogonal decomposition
 - proper orthogonal decomposition
 - conjoint analysis
 - the Hotelling transform
 - latent semantic analysis
 - eigenfaces

*** Singular Value Decomposition: Overview
**** Singular Value Decomposition
***** SVD                                                                        :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Let $A \in \CC^{m \times n}$. Then $A$ can be written as
\[
  A = U \Sigma V^*, \tag{SVD}
\]
where $U \in \CC^{m \times m}$ and $V \in \CC^{n \times n}$ are unitary and $\Sigma \in \RR^{m \times n}$ is diagonal. If $A$ is real, then so are $U$ and $V$.
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
#+LATEX: \begingroup \small
 - The columns of $U$ are called the \textbf{left singular vectors} of $A$; \vs
 - The columns of $V$ are called the \textbf{right singular vectors} of $A$; \vs
 - The diagonal entries of $\Sigma$, written as $\sigma_1, \sigma_2, \ldots, \sigma_r$, for $r = \min\{m,n\}$, are called the \textbf{singular values} of $A$ and they are nonnegative numbers ordered as
   \[
     \sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_r \ge 0.
   \]
#+LATEX: \endgroup

**** Singular Value Decomposition {{{cont}}}

**** Thick vs Thin SVD
#+LATEX: \bgroup \small
Suppose that $m > n$ and observe that:
#+LATEX: \visible<1|handout:1>{
  \begin{align*}
    U\Sigma
    & =
      \begin{bmatrix}[ccc|ccc]
        &&&&&\\
        &&&&&\\
        \bu_1 & \cdots & \bu_{n-1} & \bu_n & \cdots & \bu_m \\
        &&&&&\\
        &&&&&
      \end{bmatrix}
              \begin{bmatrix}
                \sigma_{1} &         & \\
                &  \ddots & \\
                &         & \sigma_{n} \\ \hline
                &         & \\
                & \bigzero & \\
                &         &
              \end{bmatrix} \\
    & =
      \begin{bmatrix}[ccc]
        &&\\
        &&\\
        \bu_1 & \cdots & \bu_{n-1} \\
        &&\\
        &&
      \end{bmatrix}
           \begin{bmatrix}
             \sigma_{1} &         & \\
             &  \ddots & \\
             &         & \sigma_{n}
           \end{bmatrix} = \widehat{U} \widehat{\Sigma}.
  \end{align*}
#+LATEX: }

**** SVD in MATLAB
 - Thick SVD: ~[U,S,V] = svd(A);~
 - Thin SVD: ~[U,S,V] = svd(A, 0);~

*** Understanding SVD
**** Geometric Perspective
Write $A = U\Sigma V^*$ as $AV = U\Sigma$:
\[
  A \bv_k = \sigma_k \bu_k, \quad k = 1, \ldots, r = \min\{m, n\}.
\]
#+LATEX: \visible<1|handout:1>{
 - Each right singular vector $\bv_k$ is mapped by $A$ to a scaled left singular vector $\sigma_k \bu_k$; $\sigma_k$ is the magnitude of scaling.

\vspace{0.35\textheight}
#+LATEX: }

#+BEGIN_CENTER
#+ATTR_LATEX: :options [width=0.85\linewidth,arc=0mm,boxrule=0.7pt,colback=white,nobeforeafter]
#+BEGIN_tcolorbox
The image of the unit sphere under any $m \times n$ matrix is a hyperellipse.
#+END_tcolorbox
#+END_CENTER

**** Algebraic Perspective
Alternately, note that $\by = A \bz \in \CC^m$ for any $\bz \in \CC^n$ can be written as
\[
  \left(U^* \by \right)= \Sigma \left(V^* \bz\right).
\]
#+LATEX: \visible<1|handout:1>{
\vfill
 - Since $U$ and $V$ are unitary, $U^* = U^{-1}$ and $V^* = V^{-1}$.
 - $U^* \by$ is the coordinates of $\by \in \CC^m$ with respect to the basis consisting of columns of $U$, which is an ONB.
 - $V^* \bz$ is the coordinates of $\bz \in \CC^n$ with respect to the basis consisting of columns of $V$, which is an ONB.
\vfill
#+LATEX: }

#+BEGIN_CENTER
#+ATTR_LATEX: :options [width=0.9\linewidth,arc=0mm,boxrule=0.7pt,colback=white,nobeforeafter]
#+BEGIN_tcolorbox
Any matrix $A \in \CC^{m \times n}$ can be viewed as a diagonal transformation from $\CC^n$ (source space) to $\CC^m$ (target space) with respect to suitably chosen orthonormal bases for both spaces.
#+END_tcolorbox
#+END_CENTER

**** SVD vs. EVD
#+LATEX: \begingroup \small
Recall that a diagonalizable $A = VDV^{-1} \in \CC^{n \times n}$ satisfies
\[
  \by = A\bz
  \quad\longrightarrow\quad
  \left(V^{-1} \by\right) = D \left(V^{-1} \bz\right).
\]
This allowed us to view any diagonalizable square matrix $A \in \CC^{n \times n}$ as a diagonal transformation from $\CC^n$ to itself\footnote{The source and the target spaces of the transformation coincide.} with respect to the basis formed by a set of eigenvector of $A$.
\vfill

*Differences.*
 - *Basis:* SVD uses two ONBs (left and right singular vectors); EVD uses one, usually non-orthogonal basis (eigenvectors).
 - *Universality:* all matrices have an SVD; not all matrices have an EVD.
 - *Utility:* SVD is useful in problems involving the behavior of $A$ or $A^+$; EVD is relevant to problems involving $A^k$.
#+LATEX: \endgroup

** TODO Notes on SVD
:PROPERTIES:
:EXPORT_FILE_NAME: 27-notes-on-svd
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT
*** Properties of SVD
**** SVD and the 2-Norm
***** @@latex:  @@                                                               :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Let $A \in \CC^{m \times n}$ have an SVD $A = U \Sigma V^*$. Then
 1. $\Norm{A}{2} = \sigma_1$ and $\Norm{A}{F} = \sqrt{\sigma_1^2 + \sigma_2^2 + \cdots + \sigma_r^2}$.
 2. The rank of $A$ is the number of nonzero singular values.
 3. Let $r = \min\{m, n\}$. Then
    \[
      \kappa_2(A) = \Norm[2]{A}{2} \Norm[2]{A^+}{2} = \frac{\sigma_1}{\sigma_r}.
    \]

**** Connection to EVD
Let $A = U\Sigma V^* \in \CC^{m \times n}$ and $B = A^* A$. Observe that

 - $B \in \CC^{n \times n}$ is a \textit{hermitian matrix}[fn::This is the $\CC\text{-extension}$ of real symmetric matrices.], \textit{i.e.}, $B^* = B$.
 - $B$ has an EVD:
   #+LATEX: \visible<1|handout:1>{
     \[
       B
       = \left( V \Sigma^* U^* \right) \left( U \Sigma V^* \right)
       = V \Sigma^* \Sigma V^*
       = V \left( \Sigma^* \Sigma \right) V^{-1}.
     \]
     \vspace{-1.5em}
   #+LATEX: }
   - The squares of singular values of $A$ are eigenvalues of $B$.
   - An EVD of $B = A^*A$ reveals the singular values and a set of right singular vectors of $A$.

**** Connection to EVD {{{cont}}}
***** @@latex:  @@                                                               :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
The nonzero singular values of $A \in \CC^{m \times n}$ are the square roots of the nonzero eigenvalues of $A^*A$ or $AA^*$.

*** Reduction of Dimensions
**** Low-Rank Approximations
Let $A \in \CC^{m \times n}$ with $m \ge n$. Its thin SVD $A = \widehat{U} \widehat{\Sigma} V^*$ can be written as
\begin{align*}
  A
  & =
    \begin{bmatrix}
      \bu_1 & \bu_2 & \cdots & \bu_n\\
    \end{bmatrix}
                               \begin{bmatrix}
                                 \sigma_1 &&\\
                                 &\ddots&\\
                                 &&\sigma_n
                               \end{bmatrix}
                                    \begin{bmatrix}
                                     \bv_1^*\\
                                     \vdots \\
                                     \bv_n^*
                                    \end{bmatrix} \\
  & =
    \begin{bmatrix}
      \sigma_1\bu_1 & \cdots & \sigma_n\bu_n
    \end{bmatrix}
                                    \begin{bmatrix}
                                      \bv_1^* \\
                                      \vdots  \\
                                      \bv_n^*
                                    \end{bmatrix}
   = \sum_{j=1}^{r} \sigma_j \bu_j \bv_j^*,
\end{align*}
where $r$ is the rank of $A$.

\vs
 - Each outer product $\bu_j \bv_j^*$ is a rank-1 matrix.
 - Since $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_r > 0$, important contributions to $A$ come from terms with small $j$.

**** Low-Rank Approximations {{{cont}}}
For $1 \le k \le r$, define
\[
  A_k = \sum_{j=1}^{k} \sigma_j \bu_j \bv_j^* = U_k \Sigma_k V_k^*,
\]
where
\vs
 - $U_k$ is the first $k$ columns of $U$; \vs
 - $V_k$ is the first $k$ columns of $V$; \vs
 - $\Sigma_k$ is the upper-left $k\times k$ submatrix of $\Sigma$.
\vs
This is a rank-$k$ approximation of $A$.

**** Best Rank-$k$ Approximation

***** Eckart-Young                                                               :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Let $A \in \CC^{m \times n}$. Suppose $A$ has rank $r$ and let $A = U\Sigma V^*$ be an SVD. Then
 - $\Norm{A - A_k}{2} = \sigma_{k+1}$, for $k = 1, \ldots, r-1$.
 - For any matrix $B$ with $\rk(B) \le k$, $\Norm{A-B}{2} \ge \sigma_{k+1}$.

*** Appendix: Unitary Diagonalization and SVD
**** Unitary Diagonalization of Hermitian Matrices
  The previous discussion is relevant to hermitian matrices constructed in a specific manner. For a generic hermitian matrix, we have the following result.
  \vs
***** Spectral Decomposition                                                     :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
Let $A \in \CC^{n \times n}$ be hermitian. Then $A$ has a unitary diagonalization
\[
  A = VDV^{-1},
\]
where $V \in \CC^{n \times n}$ is unitary and $D \in \RR^{n \times n}$ is diagonal.
***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
In words, a hermitian matrix (or symmetric matrix) has a complete set of orthonormal eigenvectors and all its eigenvalues are real.

**** Notes on Unitary Diagonalization and Normal Matrices
 - A unitarily diagonalizable matrix $A = VDV^{-1}$ with $D \in \CC^{n \times n}$, is called a *normal matrix*[fn::Usual defintion: $A \in \CC^{n \times n}$ is normal if $AA^* = A^*A$.]. All hermitian matrices are normal.
 - Let $A = VDV^{-1}\in \CC^{n \times n}$ be normal. Since $\kappa_2(V) = 1$ (why?), Bauer-Fike implies that eigenvalues of $A$ can be changed by no more than $\Norm{\delta A}{2}$.

**** Unitary Diagonalization and SVD
***** @@latex:  @@                                                               :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
<<thm:evd_svd>>
Let $A \in \CC^{n \times n}$ be hermitian. Then the singular values of $A$ are the absolute values of the eigenvalues of $A$.

#+LATEX: \vspace{0.05\textwidth}
#+LATEX: \begingroup \small
Precisely, if $A = VDV^{-1}$ is a unitary diagonalization of $A$, then
\[
  A = \left( V \sign(D) \right) \abs{D} V^*
\]
is an SVD, where
\begin{equation*}
 \sign(D)  =
 \begin{bmatrix}
   \sign(d_1) &&\\
   &\ddots&\\
   &&\sign(d_n)
 \end{bmatrix}, \qquad
 \abs{D}  =
 \begin{bmatrix}
   \abs{d_1} &&\\
   &\ddots&\\
   &&\abs{d_n}
 \end{bmatrix}.
\end{equation*}
\vs
#+LATEX: \endgroup

**** When Do Unitary EVD and SVD Coincide?
***** @@latex:  @@                                                               :B_theorem:
:PROPERTIES:
:BEAMER_env: theorem
:END:
If $A = A^*$, then the following statements are equivalent:
 1. Any unitary EVD of $A$ is also an SVD of $A$.
 2. The eigenvalues of $A$ are positive numbers.
 3. $\bx^* A \bx > 0$ for all nonzero $\bx \in \CC^{n}$. \hfill (HPD)

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
 - The equivalence of 1 and 2 is immediate from Theorem [[thm:evd_svd]]
 - The property in 3 is called the *hermitian positive definiteness*, /c.f./, symmetric positive definiteness.
 - The equivalence of 2 and 3 can be shown conveniently using *Rayleigh quotient*; see next slide.

**** Note: Rayleigh Quotient

Let $A \in \RR^{n \times n}$ be fixed. The *Rayleigh quotient* is the map $R_A:\RR^n \to \RR$ given by
\[
  R_A(\bx) = \frac{\bx\tp A \bx}{\bx\tp \bx}.
\]
\vs

 - $R_A$ maps an eigenvector of $A$ into its associated eigenvalue, \textit{i.e.}, if $A\bv = \lambda\bv$, then $R_A(\bv) = \lambda$.
 - If $A = A\tp$, then $\grad R_A(\bv) = \bzero$ for an eigenvector $\bv$, and so
   \[
     R_A(\bv + \epsilon \bz) = R_A(\bv) + 0 + O(\epsilon^2) = \lambda + O(\epsilon^2), \quad \text{as $\epsilon \to 0$.}
   \]
   The Rayleigh quotient is a quadratic approximation of an eigenvalue.
* Exam Reviews
** TODO Review for MATLAB Basics
:PROPERTIES:
:EXPORT_TITLE: Review for Midterm 1
:EXPORT_FILE_NAME: 10-review-for-mt1
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Summations
**** Loops                                                                         :noexport:
 - To kick start a while-loop even when part of loop header is not valid:
   #+BEGIN_EXAMPLE
 n = 0;
 while n == 0 || err > tol
     n = n + 1;
     q_approx = ...;
     err = abs(q - q_approx);
 end
   #+END_EXAMPLE

**** Different Ways of Forming Sums
To calculate $\sum_{j=1}^n a_j b_j$:
\vs
#+LATEX: \begin{multicols}{3}
 - using a loop
 - using =sum=
 - *inner product*
#+LATEX: \end{multicols}

**** Sequence of Partial Sums
To study the convergence of an infinite series $\ds \sum_{j=1}^{\infty} a_j$, form the sequence of partial sums $\{s_n\}$ where
\[
s_n = \sum_{j=1}^{n} a_j = a_1 + \cdots + a_n.
\]
#+LATEX: \begin{multicols}{2}
 - using a loop
 - using =cumsum=
#+LATEX: \end{multicols}

*** Simulations
**** Biased Coin
***** Question
Simulate the tossing of a biased coin with
\[
P(\text{T}) = p, \quad P(\text{H}) = 1-p.
\]

**** Biased Coin -- Notes
*Ideas.*
 - random number generators
 - traditional tools: loops and conditional statements
 - the /powerful/ =find= function
 - one-liner using =ceil= or =floor=
\vs
*Explore.*
 - How would you handle similar situations with multiple states with non-uniform probability profile, /e.g./, a biased dice?

**** Dice Rolls
***** Question
Write a script simulating $n = 10,000$ throws of two 6-sided fair dice. What is the probability of obtaining two same numbers? Provide both analytical and numerical answers.
*** Data Manipulation
**** Data Manipulation
Download =grades.dat= into your current directory and load it using
#+BEGIN_EXAMPLE
 >> grades = load('grades.dat');
#+END_EXAMPLE
To read about how the data are organized, use =type grades.dat=.
***** Question
1. Determine the number of students.
2. Compute the total grade according to the weights specified in the header. Do this without using a loop.
3. The letter grades are determined by
   #+LATEX: \begin{multicols}{3}
   - A: $[90, 100]$
   - B: $[80, 90)$
   - C: $[70, 80)$
   - D: $[60, 70)$
   - E: $[0, 60)$
   #+LATEX: \end{multicols}
   Find the number of students earning each of the letter grades.
*** Finding Factors                                                                 :noexport:
**** Finding Factors
***** Question
Given a positive integer $n$, finds all factors. Do it using a single MATLAB statement.

**** Finding Factors -- Notes
*Ideas.*
 - the =mod= function: detecting a factor
 - the =find= function: do it in one scoop
\vs
*Explore.*
 - The built-in function =factor= finds all prime factors. Use it to write a prime factorization of an integer.

*** Spiral Triangle: Tying Up Loose Ends                                            :noexport:
**** Recall: Entire Code
\begingroup\small
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth, numbers=left, numberstyle={\tiny \color{gray}}, numbersep=7pt
#+BEGIN_SRC matlab
 m = 21; d_angle = 4.5;
 th = linspace(0, 360, 4) + 90;
 V = [cosd(th);
      sind(th)];
 C = colormap(hsv(m));
 s = sind(150 - abs(d_angle))/sind(30);
 R = [cosd(d_angle) -sind(d_angle);
      sind(d_angle) cosd(d_angle)];
 hold off
 for i = 1:m
     if i > 1
         V = s*R*V;
     end
     plot(V(1,:), V(2,:), 'Color', C(i,:))
     hold on
 end
 set(gcf, 'Color', 'w')
 axis equal, axis off
#+END_SRC
\endgroup

**** Understanding Line 6
To create the desired spiraling effect, the scaling factor must be calculated carefully.
***** columns                                                                    :B_columns:
:PROPERTIES:
:BEAMER_env: columns
:BEAMER_opt: [onlytextwidth,t]
:END:
****** col1                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.6
:END:
 - Useful:
   #+ATTR_LATEX: :options [width=0.7\linewidth,arc=0mm,boxrule=0.7pt,colback=white]
   #+BEGIN_tcolorbox
   \vspace{-1em}
   \[
   \frac{\sin \alpha}{a} = \frac{\sin \beta}{b} = \frac{\sin \gamma}{c}
   \]
   #+END_tcolorbox
 - Compute the scaling factor $s$:

****** col2                                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+LATEX: \begin{figure}[ht] \centering
\incfig{../img/spiral_triangle_explained}{0.9\linewidth}
#+LATEX: \end{figure}

**** Understanding Line 12
#+ATTR_LATEX: :options style=matlab, linewidth=\linewidth
#+BEGIN_SRC matlab
 th = linspace(0, 360, 4) + 90;
 V = [cosd(th);
      sind(th)];
 s = sind(150 - abs(d_angle))/sind(30);
 R = [cosd(d_angle) -sind(d_angle);
      sind(d_angle) cosd(d_angle)];
 V = s*R*V;    % <----
#+END_SRC

**** Understanding Line 5 (More on Coloring)
 - Using RGB colors in plots
 - =colormap=

** DONE Midterm 1 Review
:PROPERTIES:
:EXPORT_FILE_NAME: mt1review
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** MATLAB Basics
**** Conventional Programming Constructs
Key points:
- conditional statements
- for-loop and while-loop
\vfill

Important problems to review:
- approximation of $\pi$ given a series known to converge to it
- quadratic equation solver
\vfill

**** Arrays in MATLAB
Key points:
- construction of vectors and matrices
- getting/setting/modifying parts of an array
- elementwise operations
- data manipulation functions
- one-liners (ONE MATLAB statement)
\vfill

Important problems to review:
- array construction
- grade analysis
- birthday problem
\vfill

**** Graphics
Key points:
- 2-D: cartesian vs. polar graphs
- 3-D: curves and surfaces
- log-linear and log-log
\vfill

Important problems to review:
- regular polygons inscribed in circles
- analyzing convergence of sequences
- parametric (closed) surfaces (spheres, tori, etc)
\vfill

*** Preliminaries
**** Two Types of Errors                                                           :noexport:
 - Absolute error
 - Relative error
**** Floating-Point Numbers
 - Binary scientific notation:
   \[
   \pm \left(1 + \frac{b_1}{2} + \frac{b_2}{2^2} + \cdots + \frac{b_d}{2^d}\right) 2^E,
   \]
   where $b_i$ is 0 or 1 and $E$ is an integer.
   - $d$ determines the /resolution/
   - the range of $E$ determines the /scope/ or /extent/
 - IEEE Standard (double-precision; 64 bits)
   * $d = 52$ and $-1022 \le E \le 1023$
   * $\fbox{eps} = 2^{-52} \approx 2\times 10^{-16}$
   * =realmin=, =realmax=
**** Floating-Point Numbers
 - Key features
   - On any interval of the form $[2^E, 2^{E+1})$, there are $2^d$ evenly-spaced f-p numbers.
   - The spacing between two adjacent f-p numbers in $[2^E, 2^{E+1})$ is $2^{E-d} = 2^E \, \fbox{eps}$.
   - The gap between 1 and the next f-p number is $\fbox{eps}$, the machine epsilon.
   - Representation error (in relative sense) is bounded by $\frac{1}{2} \fbox{eps}$.

**** Conditioning (of a problem)
 - The condition number measures the ratio of error in the result (or output) to error in the data (or input).
 - Recall the definition of condition number $\kappa_f(x)$
 - A large condition number implies that the error in a result may be much greater than the round-off error used to compute it.
 - /Catastrophic cancellation/ is one of the most common sources of loss of precision.

**** Stability (of an algorithm)
 - When an algorithm produces much more error than can be explained by the condition number, the algorithm is unstable.

#+LATEX: \newpage

*** Square Linear Systems
**** Polynomial Interpolation
 - Polynomial interpolation leads to a square linear system of equations with a Vandermonde matrix.

**** Gaussian Elimination and (P)LU Factorization
 - A triangular linear system is solved by backward substitution or forward elimination.
 - A general linear system is solved by Gaussian elimination.
 - Gaussian elimination (with partial pivoting) is equivalent to (P)LU factorization.
 - Solving a triangular linear system of size $n \times n$ takes $\sim n^2$ flops.
 - PLU factorization takes $\sim \frac{2}{3} n^2$ flops.

**** Row and Column Operations
Various row and column operations can be emulated by matrix multiplications. ("Left-multiplication for row actions, right-multiplication for column actions")

 - row/column extraction (unit vector)
 - row/column swap (elementary permutation matrix)
 - row/column rearrangement (permutation matrix)
 - row replacement $R_i \to R_i + c R_j$ (Gaussian transformation matrix)

**** Norms
A /norm/ generalizes the notion of length for vectors and matrices.
 - *Vector /p/-norm*
   \[
   \Norm{\bv}{p} = \Biggl( \sum_{i=1}^{n} \abs{b_i}^p \Biggr)^{1/p}, \quad p \in [1, \infty)
   \]
   and
   \[
   \Norm{\bv}{\infty} = \max_{i} \abs{v_i}
   \]
 - *Matrix /p/-norm* (induced)
   \[
   \Norm{A}{p} = \max_{\Norm{\bx}{p}=1} \Norm{A\bx}{p}, \quad p \in [1, \infty]
   \]
 - *Frobenius norm* (non-induced)
   \[
   \Norm{A}{F} = \Biggl( \sum_{i} \sum_{j} \abs{a_{i,j}}^2 \Biggr)^{1/2}
   \]
 - MATLAB: =norm= can calculate both vector and matrix norms

**** Conditioning/Stability
 - Partial pivoting is needed for numerical stability.
 - The matrix condition number is equal to the condition number of solving a linear system of equations.

**** Programming Notes
 - Built-in functionalities
   - backslash (=\=)
   - =lu=
   - =norm=
   - =cond=, =condest=, =linsolve=
 - Demonstration/Instructional codes
   - =backsub= and =forelim=
   - =GEnp= and =GEpp=
   - =mylu= and =myplu=

#+LATEX: \newpage

*** Overdetermined Linear Systems                                                   :noexport:
**** Polynomial Approximation
 - The most common solution to overdetermined systems is obtained by /least squares/, which minimizes the 2-norm of the residual vector.
 - Least squares is used to find fitting functions that depend linearly on the unknown parameters.
**** Normal Equation
 - The LLS problem $A \bx \, \text{``=''} \, \bb$ is mathematically equivalent to the normal equation $A\tp A \bx = A\tp \bb$:
   - linear algebra viewpoint
   - calculus viewpoint
   - geometric interpretation
 - The solution of the normal equation can be written as $\bx = A^{+} \bb$, where $A^+$ is the pseudoinverse of $A$.
 - The backslash in MATLAB is theoretically equivalent to left-multiplying by a pseudoinverse or inverse.

**** QR Factorization                                                              :noexport:
 - Orthogonal sets of vectors are preferred to nonorthogonal ones in computing. (no catastrophic cancellation)
 - Matrices with orthonormal columns and orthogonal matrices enjoy many /nice/ analytical properties.
 - QR factorization plays a role in LLS similar to that of LU factorization in square linear systems.

**** Two Types of QR Factorization                                                 :noexport:
For $A \in \RR^{m \times n}$, $m \ge n$:
 - Thick QR factorization: $A = QR$
   - $Q \in \RR^{m \times m}$ orthogonal
   - $R \in \RR^{m \times n}$ upper triangular
   - obtained by using successive Householder transformation matrices for /triangularization/
 - Thin: $A = \widehat{Q} \widehat{R}$
   - $\widehat{Q} \in \RR^{m \times n}$ orthonormal columns
   - $\widehat{R} \in \RR^{n \times n}$ upper triangular
   - obtained by Gram-Schmidt /orthonormalization/ procedure

**** Householder Transformation Matrices                                           :noexport:
 - A Householder transformation matrix $H$ (associated with a vector $\bz$) is a /reflection/ matrix which is
   - symmetric,
   - orthogonal, and
   - transforms $\bz$ to $\pm\Norm{\bz}{2} \be_1$.

**** Programming Notes                                                             :noexport:
 - Built-in functionalities
   - backslash (=\=)
   - =qr=
 - Demonstration/Instructional codes
   - =lsqrfact=: solving least squares using QR
#   - =gs=: Gram-Schmidt (for homework)

*** Appendix: Row and Column Operations                                             :noexport:
**** Notation: Standard Unit Basis Vectors

Let $n \in \NN$ be fixed. Let $I$ be the $n \times n$ identity matrix and denote by $\be_j$ its $j\text{th}$ column, /i.e./,
\begin{equation*}
  I =
  \begin{bmatrix}
    1 & 0 & \cdots & 0 \\
    0 & 1 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & 1
  \end{bmatrix}
  =
  \begin{bmatrix}[c|c|c|c]
    & & & \\
    & & & \\
    \be_1 & \be_2 & \cdots & \be_n \\
    & & & \\
    & & &
  \end{bmatrix}.
\end{equation*}

That is,

\begin{equation*}
  \be_1 =
  \begin{bmatrix}
    1 \\ 0 \\ \vdots \\ 0
  \end{bmatrix},\quad
  \be_2 =
  \begin{bmatrix}
    0 \\ 1 \\ \vdots \\ 0
  \end{bmatrix},\quad\cdots,\quad
  \be_n =
  \begin{bmatrix}
    0 \\ 0 \\ \vdots \\ 1
  \end{bmatrix}.
\end{equation*}
***** col1                                                                        :noexport:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
Then for any $A \in \RR^{n \times n}$,

\vs

 - $\be_i\tp A = \balpha_i\tp$, the $i\text{th}$ \blue{row} of $A$.
 - $A \be_j = \ba_j$, the $j\text{th}$ \blue{column} of $A$.
 - $\be_i\tp A \be_j = A_{i,j}$, the $(i,j)$ entry of $A$.

***** col2                                                                        :noexport:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
\begin{equation*}
  A =
  \begin{bmatrix}[c|c|c|c]
    & & & \\
    & & & \\
    \ba_1 & \ba_2 & \cdots & \ba_n \\
    & & & \\
    & & &
  \end{bmatrix}.
\end{equation*}

**** Notation: Concatenation
Let $A \in \RR^{n \times n}$. We can view it as a concatenation of its rows or columns as visualized below in which case we write
#+LATEX: \begingroup\small
\begin{equation*}
  A =
  \begin{bmatrix}
    \rule[-0.5em]{0pt}{1.3em} a_{11} & a_{12} & \cdots & a_{1n} \\
    \rule[-0.5em]{0pt}{1.5em} a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    \rule[-0.3em]{0pt}{1.3em} a_{n1} & a_{n2} & \cdots & a_{nn} \\
  \end{bmatrix}
  =
  \begin{bmatrix}[c|c|c|c]
    \rule[0pt]{0pt}{1.3em}& & & \\
    & & & \\
    \ba_1 & \ba_2 & \cdots & \ba_n \\
    & & & \\
    \rule[-0.3em]{0pt}{0pt}& & &
  \end{bmatrix}
  =
  \begin{bmatrix}
    \rule[-0.5em]{0pt}{1.3em}& && \balpha_1\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_2\tp && & \\ \hline
    & && \vdots && & \\ \hline
    \rule[-0.3em]{0pt}{1.3em}& && \balpha_n\tp && &
  \end{bmatrix}.
\end{equation*}
#+LATEX: \endgroup

**** Row or Column Extraction
A row or a column of $A$ can be extracted using columns of $I$.
#+ATTR_LATEX: :align l|c|c
| Operation                              | Mathematics        | MATLAB                              |
|----------------------------------------+--------------------+-------------------------------------|
| extract the $i\text{th}$ row of $A$    | $\be_i\tp A$       | =A(i,:)=  \rule[-0.5em]{0pt}{1.7em}  |
| extract the $j\text{th}$ column of $A$ | $A \be_j$          | =A(:,j)=  \rule[-0.5em]{0pt}{1.5em}  |
| extract the $(i,j)$ entry of $A$       | $\be_i\tp A \be_j$ | =A(i,j)=  \rule[-0.5em]{0pt}{1.5em} |

**** Elementary Permutation Matrices
***** Elementary Permutation Matrix                                           :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
For $i, j \in \NN[1,n]$ distinct, denote by $P(i,j)$ the $n \times n$ matrix obtained by interchanging the $i\text{th}$ and $j\text{th}$ rows of the $n\times n$ identity matrix. Such matrices are called /elementary permutation matrices/.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs
*Example.* ($n = 4$)
\begin{equation*}
  P(1,2) =
  \begin{bmatrix}
    0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1
  \end{bmatrix},
  \quad
  P(1,3) =
  \begin{bmatrix}
    0 & 0 & 1 & 0 \\
    0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1
  \end{bmatrix},
  \quad \cdots
\end{equation*}

*Notable Properties.*
#+LATEX: \begin{multicols}{2}
 - $P(i,j)=P(j,i)$
 - $P(i,j)^2 = I$
#+LATEX: \end{multicols}

**** Row or Column Interchange
Elementary permutation matrices are useful in interchanging rows or columns.
\vs
#+ATTR_LATEX: :align c|c|c
| Operation                                                             | Mathematics | MATLAB                        |
|-----------------------------------------------------------------------+-------------+-------------------------------|
| $\balpha_i\tp \leftrightarrow \balpha_j\tp$ \rule[-0.5em]{0pt}{1.8em} | $P(i,j) A$  | ~A([i,j],:)=A([j,i],:)~       |
| $\ba_i \leftrightarrow \ba_j$                                         | $A P(i,j)$  | ~A(:,[i,j])=A(:,[j,i])~       |

**** Permutation Matrices
***** Permutation Matrix                                                      :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
A /permutation matrix/ $P \in \RR^{n \times n}$ is a square matrix obtained from the same-sized identity matrix by re-ordering of rows.

***** ...                                                                  :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
\vs

*Notable Properties.*
 - $P\tp = P^{-1}$
 - A product of /elementary permutation matrices/ is a permutation matrix.
\vs
*Row and Column Operations.* For any $A \in \RR^{n \times n}$,
 - $PA$ permutes the rows of $A$.
 - $AP$ permutes the columns of $A$.

**** Row or Column Rearrangement
***** Question
Let $A \in \RR^{6\times6}$, and suppose that it is stored in MATLAB. Rearrange rows of $A$ by moving 1st to 2nd, 2nd to 3rd, 3rd to 5th, 4th to 6th, 5th to 4th, and 6th to 1st, that is,
#+LATEX: \begingroup\small
\begin{equation*}
  \begin{bmatrix}
    \rule[-0.5em]{0pt}{1.2em}& && \balpha_1\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_2\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_3\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_4\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_5\tp && & \\ \hline
    \rule[-0.3em]{0pt}{1.5em}& && \balpha_6\tp && &
  \end{bmatrix}
  \;\longrightarrow\;
  \begin{bmatrix}
    \rule[-0.5em]{0pt}{1.2em}& && \balpha_6\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_1\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_2\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_5\tp && & \\ \hline
    \rule[-0.5em]{0pt}{1.5em}& && \balpha_3\tp && & \\ \hline
    \rule[-0.3em]{0pt}{1.5em}& && \balpha_4\tp && &
  \end{bmatrix}
\end{equation*}
#+LATEX: \endgroup

**** Row or Column Rearrangement
*Solution.*
\vs
 - Mathematically: $PA$ where
   #+LATEX: \begingroup\footnotesize
   \begin{equation*}
     P =
     \begin{bmatrix}
       \rule[-0.5em]{0pt}{1.2em}& && \be_6\tp && & \\ \hline
       \rule[-0.5em]{0pt}{1.5em}& && \be_1\tp && & \\ \hline
       \rule[-0.5em]{0pt}{1.5em}& && \be_2\tp && & \\ \hline
       \rule[-0.5em]{0pt}{1.5em}& && \be_5\tp && & \\ \hline
       \rule[-0.5em]{0pt}{1.5em}& && \be_3\tp && & \\ \hline
       \rule[-0.3em]{0pt}{1.5em}& && \be_4\tp && &
     \end{bmatrix}
     =
     \begin{bmatrix}
       \rule[-0.5em]{0pt}{1.5em} 0 & 0 & 0 & 0 & 0 & 1 \\
       \rule[-0.5em]{0pt}{1.5em} 1 & 0 & 0 & 0 & 0 & 0 \\
       \rule[-0.5em]{0pt}{1.5em} 0 & 1 & 0 & 0 & 0 & 0 \\
       \rule[-0.5em]{0pt}{1.5em} 0 & 0 & 0 & 0 & 1 & 0 \\
       \rule[-0.5em]{0pt}{1.5em} 0 & 0 & 1 & 0 & 0 & 0 \\
       \rule[-0.3em]{0pt}{1.5em} 0 & 0 & 0 & 1 & 0 & 0
     \end{bmatrix}.
   \end{equation*}
   #+LATEX: \endgroup
 - MATLAB:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 A = A([6 1 2 5 3 4], :)
 % short for  A([1 2 3 4 5 6], :) = A([6 1 2 5 3 4], :)
   #+END_SRC

**** Elementary Row Operation and GTM
Let $1 \le j < i \le n$.
\vs
 - The row operation $R_i \to R_i + c R_j$ on $A \in \RR^{n \times n}$, for some $c \in \RR$, can be emulated by a matrix multiplication[fn::Many linear algebra texts refer to the matrix in parentheses as an /elementary matrix/.]
   \[
     (I + c \, \be_i \be_j\tp) A.
   \]
 - In the context of Gaussian elimination, the operation of introducing zeros below the $j\text{th}$ diagonal entry can be done via
   \[
     \underbrace{(I + \sum_{i=j+1}^{n} c_{i,j}\, \be_i \be_j\tp)}_{=G_j}A, \quad 1 \le j < n.
   \]
   The matrix $G_j$ is called a /Gaussian transformation matrix/ (GTM).

**** Elementary Row Operation and GTM {{{cont}}}
 - To emulate $(I + c \be_i \be_j\tp)A$ in MATLAB:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 A(i,:) = A(i,:) + c*A(j,:);
   #+END_SRC
 - To emulate
   \[
   G_j A = (I + \sum_{i=j+1}^n c_{i,j} \be_i \be_j\tp)A
   \]
   in MATLAB:
   #+ATTR_LATEX: :options style=matlab
   #+BEGIN_SRC matlab
 for i = j+1:n
     c = ....
     A(i,:) = A(i,:) + c*A(j,:);
 end
   #+END_SRC
   This can be done without using a loop.

**** Analytical Properties of GTM
 - GTMs are /unit/ lower triangular matrices.
 - The product of GTMs is another unit lower triangular matrix.
 - The inverse of a GTM is also a unit lower triangular matrix.
** TODO Midterm 2 Review (3.2--)
:PROPERTIES:
:EXPORT_TITLE: Review for Midterm 2
:EXPORT_FILE_NAME: 24-review-for-mt2
:EXPORT_OPTIONS: H:2 date:nil
:EXPORT_AUTHOR:
:END:
*** Frontmatters                                                             :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
#+BEGIN_EXPORT latex
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contents}
  \tableofcontents
\end{frame}
#+END_EXPORT

*** Preliminaries
**** Two Types of Errors
 - absolute error
 - relative error
**** Floating-Point Numbers
 - binary scientific notation:
   \[
   \pm \left(1 + \frac{b_1}{2} + \frac{b_2}{2^2} + \cdots + \frac{b_d}{2^d}\right) 2^E,
   \]
   where $b_i$ is 0 or 1 and $E$ is an integer.
   - $d$ determines the /resolution/
   - the range of $E$ determines the /scope/ or /extent/
 - IEEE Standard (double-precision; 64 bits)
   * $d = 52$ and $-1022 \le E \le 1023$
   * $\fbox{eps} = 2^{-52} \approx 2\times 10^{-16}$
   * =realmin=, =realmax=
**** Floating-Point Numbers {{{cont}}}
 - Key features
   - On any interval of the form $[2^E, 2^{E+1})$, there are $2^d$ evenly-spaced f-p numbers.
   - The spacing between two adjacent f-p numbers in $[2^E, 2^{E+1})$ is $2^{E-d} = 2^E \, \fbox{eps}$.
   - The gap between 1 and the next f-p number is $\fbox{eps}$, the machine epsilon.
   - Representation error (in relative sense) is bounded by $\frac{1}{2} \fbox{eps}$.

**** Conditioning (of a problem)
 - The condition number measures the ratio of error in the result (or output) to error in the data (or input).
 - Recall the definition of condition number $\kappa_f(x)$
 - A large condition number implies that the error in a result may be much greater than the round-off error used to compute it.
 - Catastrophic cancellation is one of the most common sources of loss of precision.

**** Stability (of an algorithm)
 - When an algorithm produces much more error than can be explained by the condition number, the algorithm is unstable.

#+LATEX: \newpage

*** Square Linear Systems
**** Polynomial Interpolation
 - Polynomial interpolation leads to a square linear system of equations with a Vandermonde matrix.

**** Gaussian Elimination and (P)LU Factorization
 - A triangular linear system is solved by backward substitution or forward elimination.
 - A general linear system is solved by Gaussian elimination.
 - Gaussian elimination (with partial pivoting) is equivalent to (P)LU factorization.
 - Solving a triangular linear system of size $n \times n$ takes $\sim n^2$ flops.
 - PLU factorization takes $\sim \frac{2}{3} n^2$ flops.

**** Norms
A /norm/ generalizes the notion of length for vectors and matrices.
 - *Vector /p/-norm*
   \[
   \Norm{\bv}{p} = \Biggl( \sum_{i=1}^{n} \abs{b_i}^p \Biggr)^{1/p}, \quad p \in [1, \infty)
   \]
   and
   \[
   \Norm{\bv}{\infty} = \max_{i} \abs{v_i}
   \]
 - *Matrix /p/-norm* (induced)
   \[
   \Norm{A}{p} = \max_{\Norm{\bx}{p}=1} \Norm{A\bx}{p}, \quad p \in [1, \infty]
   \]
 - *Frobenius norm* (non-induced)
   \[
   \Norm{A}{F} = \Biggl( \sum_{i} \sum_{j} \abs{a_{i,j}}^2 \Biggr)^{1/2}
   \]
 - MATLAB: =norm= can calculate both vector and matrix norms

**** Row and Column Operations
Various row and column operations can be emulated by matrix multiplications. ("Left-multiplication for row actions, right-multiplication for column actions")

 - row/column extraction (unit vector)
 - row/column swap (elementary permutation matrix)
 - row/column rearrangement (permutation matrix)
 - row replacement $R_i \to R_i + c R_j$ (Gaussian transformation matrix)

**** Conditioning/Stability
 - Partial pivoting is needed for numerical stability.
 - The matrix condition number is equal to the condition number of solving a linear system of equations.

**** Programming Notes
 - Built-in functionalities
   - backslash (=\=)
   - =lu=
   - =norm=
   - =cond=, =condest=, =linsolve=
 - Demonstration/Instructional codes
   - =backsub= and =forelim=
   - =GEnp= and =GEpp=
   - =mylu= and =myplu=

#+LATEX: \newpage

*** Overdetermined Linear Systems
**** Polynomial Approximation
 - The most common solution to overdetermined systems is obtained by /least squares/, which minimizes the 2-norm of the residual vector.
 - Least squares is used to find fitting functions that depend linearly on the unknown parameters.
 - Equivalence of the LLS problem and the normal equation
   - linear algebra proof
   - calculus proof

**** QR Factorization
 - Orthogonal sets of vectors are preferred to nonorthogonal ones in computing. (no catastrophic cancellation)
 - Matrices with orthonormal columns and orthogonal matrices enjoy many /nice/ analytical properties.
 - QR factorization plays a role in LLS similar to that of LU factorization in square linear systems.

**** Two Types of QR Factorization
For $A \in \RR^{m \times n}$, $m \ge n$:
 - Thick QR factorization: $A = QR$
   - $Q \in \RR^{m \times m}$ orthogonal
   - $R \in \RR^{m \times n}$ upper triangular
   - obtained by using successive Householder transformation matrices for /triangularization/
 - Thin: $A = \widehat{Q} \widehat{R}$
   - $\widehat{Q} \in \RR^{m \times n}$ orthonormal columns
   - $\widehat{R} \in \RR^{n \times n}$ upper triangular
   - obtained by Gram-Schmidt /orthonormalization/ procedure

**** Householder Transformation Matrices
 - A Householder transformation matrix $H$ (associated with a vector $\bz$) is a /reflection/ matrix which is
   - symmetric,
   - orthogonal, and
   - transforms $\bz$ to $\pm\Norm{\bz}{2} \be_1$.

**** Programming Notes
 - Built-in functionalities
   - backslash (=\=)
   - =qr=
 - Demonstration/Instructional codes
   - =lsqrfact=: solving least squares using QR
   - =gs=: Gram-Schmidt (for homework)

** Chapter Reviews
* Scratch
:PROPERTIES:
:EXPORT_FILE_NAME: scratch
:EXPORT_OPTIONS: H:1 date:nil
:END:
** Triangular system example (FNC Example 2.3.3)
#+BEGIN_EXPORT latex
$\left\lbrack
\begin{array}{ccccc}
1 & -1 & 0 & \alpha - \beta & \beta \\
0 & 1 & -1 & 0 & 0 \\
0 & 0 & 1 & -1 & 0 \\
0 & 0 & 0 & 1 & -1 \\
0 & 0 & 0 & 0 & 1
\end{array}
\right\rbrack
\left\lbrack
\begin{array}{c}
1 \\ 1 \\ 1 \\ 1 \\ 1
\end{array}
\right\rbrack
= \left\lbrack
\begin{array}{c}
\alpha \\ 0 \\ 0 \\ 0 \\ 1
\end{array}
\right\rbrack$
#+END_EXPORT

** G.E. with Partial Pivoting: Example
#+LATEX: \begingroup\small

*1st column:*
#+LATEX: \begingroup\scriptsize
#+LATEX: \hfsetfillcolor{blue!10}
#+LATEX: \hfsetbordercolor{blue!10}
\begin{equation*}
  \begin{bmatrix}
    \begin{array}{rrr|r}
      \tikzmarkin{aa}(0.17,-0.15)(-0.2,0.23)
      2 & 2 & 1 & 6 \\
      -4 & 6 & 1 & -8 \\
      \hfsetbordercolor{blue!60}
      \tikzmarkin{a}(0.08,-0.07)(-0.08,0.22)5\tikzmarkend{a}\tikzmarkend{aa} & -5 & 3 & 4
    \end{array}
  \end{bmatrix}
  \xrightarrow[{\rm (1)}]{{\rm pivot}}
  \begin{bmatrix*}
    \begin{array}{rrr|r}
      5 & -5 & 3 & 4 \\ -4 & 6 & 1 & -8 \\ 2 & 2 & 1 & 6
    \end{array}
  \end{bmatrix*}
  \xrightarrow[{\rm (2)}]{{\rm zero}}
  \begin{bmatrix}
    \begin{array}{rrr|r}
      5 & -5 & 3 & 4 \\ 0 & 2 & 3.4 & -4.8 \\ 0 & 4 & -0.2 & 4.4
    \end{array}
  \end{bmatrix}
\end{equation*}
#+LATEX: \endgroup

\vs

*2nd column:*
#+LATEX: \begingroup\scriptsize
#+LATEX: \hfsetfillcolor{blue!10}
#+LATEX: \hfsetbordercolor{blue!10}
\begin{equation*}
  \begin{bmatrix}
    \begin{array}{rrr|r}
      5 & -5 & 3 & 4 \\
      0 & \tikzmarkin{bb}(0.15,-0.15)(-0.15,0.2)2 & 3.4 & -4.8 \\
      0 & \hfsetbordercolor{blue!60}\tikzmarkin{b}(0.08,-0.07)(-0.08,0.22)4\tikzmarkend{b}\tikzmarkend{bb} & -0.2 & 4.4
    \end{array}
  \end{bmatrix}
  \xrightarrow[{\rm (3)}]{{\rm pivot}}
  \begin{bmatrix}
    \begin{array}{rrr|r}
      5 & -5 & 3 & 4 \\ 0 & 4 & -0.2 & 4.4 \\ 0 & 2 & 3.4 & -4.8
    \end{array}
  \end{bmatrix}
  \xrightarrow[{\rm (4)}]{{\rm zero}}
  \begin{bmatrix}
    \begin{array}{rrr|r}
      5 & -5 & 3 & 4 \\ 0 & 4 & -0.2 & 4.4 \\ 0 & 0 & 3.5 & -7
    \end{array}
  \end{bmatrix}
\end{equation*}
#+LATEX: \endgroup

#+LATEX: \endgroup

** myplu
#+BEGIN_SRC matlab
 function [L,U,P,s] = myplu(A)
 % MYPLU    PLU factorization
 % Input:
 %   A      square matrix
 % Output:
 %   P,L,U  permutation, unit lower triangular, and upper triangular
 %          matrices such that LU=PA
 %   s      number of row swaps

     %% Initialization/Preallocation
     n = length(A);
     P = eye(n);   % preallocate P
     L = eye(n);   % preallocate L
     s = 0;        % initialize s

     %% Main Loop
     for j = 1:n-1
         %% [FILL IN] Pivoting
         % (An if-statement is allowed.)

         %% [FILL IN] Introducing Zeros Below Diagonal
         % (A for-loop is NOT allowed.)

     end

     %% Clean-Up
     U = triu(A);
 end
#+END_SRC
