% Created 2024-08-05 Mon 12:52
% Intended LaTeX compiler: pdflatex
\documentclass[]{ximera}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{color}
\usepackage{listings}
\usepackage{lipsum}
\input{tk_packages}
\input{tk_macros}
\input{tk_environ}
\input{tk_ximera}
\author{Tae Eun Kim}
\date{}
\title{Floating Point Numbers}
\hypersetup{
 pdfauthor={Tae Eun Kim},
 pdftitle={Floating Point Numbers},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.3 (Org mode 9.6.15)}, 
 pdflang={English}}
\begin{document}

\maketitle
\section{Errors}
Let $x$ be some number of interest and let $\tilde{x}$ be an approximation to it obtained by an \textbf{algorithm}.

We estimate of the accuracy of the computed value via
\begin{align*}
  &\text{(absolute error)}
    = \abs{\tilde{x}-x}, \qquad
  \text{(relative error)}
    = \abs{\frac{\tilde{x}-x}{x}}
    = \abs{\frac{\tilde{x}}{x} - 1}, \\
  &\text{(\# of accurate digits)}
    = -\log_{10} \abs{\frac{\tilde{x}-x}{x}}.
\end{align*}
One thing to note is that an absolute error has the same unit as $x$, while a relative error is dimensionless due to the division. If an absolute error or relative error is small, we say that $\tilde{x}$ is \textbf{accurate}.

\section{Example: Stirling's Formula}
Stirling's formula provides a ``good'' approximation to $n!$ for large $n$:
\begin{equation}
  \label{eq:stirling}
  n! \approx \sqrt{2 \pi n} {\left( \frac{n}{e} \right)}^n \,.
  \tag{$\star$}
\end{equation}
Assuming that the exact value of $n!$ is found by \texttt{factorial}, estimate $n!$ using Stirling's formula. Show the accuracy of this approximation for various values of $n$.

\begin{lstlisting}[language=matlab,label= ,caption= ,captionpos=b,numbers=none,style=matlab, linewidth=\linewidth]
 n = ....;
 x = factorial(n)
 x0 = sqrt(2*pi*n)*(n/exp(1))^n
 abs_err = abs(x0-x)
 rel_err = abs((x0-x)/x)
 acc_digits = -log10(rel_err)
\end{lstlisting}

\begingroup\scriptsize
\begin{minipage}{0.31\linewidth}
\textbf{Results with \texttt{n=20}}:
\begin{verbatim}
x  = 2.4329e+18
x0 = 2.4228e+18
abs_err = 3.0104e+04
rel_err = 0.0083
acc_digits = 2.0811
\end{verbatim}
\end{minipage}
\hfill
\begin{minipage}{0.31\linewidth}
\textbf{Results with \texttt{n=100}}:
\begin{verbatim}
x  = 9.3326e+157
x0 = 9.3248e+157
abs_err = 7.7739e+154
rel_err = 8.3298e-04
acc_digits = 3.0794
\end{verbatim}
\end{minipage}
\hfill
\begin{minipage}{0.31\linewidth}
\textbf{Results with \texttt{n=200}}:
\begin{verbatim}
x  = Inf
x0 = Inf
abs_err = NaN
rel_err = NaN
acc_digits = NaN
\end{verbatim}
\end{minipage}
\endgroup

\section{Sources of Errors}
So the big question now is about where errors are coming from. The main sources of errors in the context of numerical computation are:

\begin{itemize}
\item From how numbers are handled in the computer \hfill  {\color{gray}{(floting point numbers)}}
\item From the problem itself \hfill  {\color{gray}{(conditioning)}}
\item From the numerical algorithm \hfill  {\color{gray}{(stability)}}
\end{itemize}

\section{Floating Point Numbers}
\subsection{Limitations of Digital Representations}
A digital computer uses a finite number of bits to represent a real number and so it cannot represent all real numbers.
\begin{itemize}
\item The represented numbers cannot be arbitrarily large or small;
\item There must be gaps between them.
\end{itemize}
So for all operations involving real numbers, it uses a subset of $\RR$ called the \textbf{floating point numbers}, $\mathbb{F}$.

\subsection{Toy Model: Base-10}
Consider a base-10 calculator which represents a nonzero number $x$ in the following format:
\begin{equation*}
x = (-1)^s \times d_0 . d_1 d_2 \times 10^e
\quad\text{where }
\left\{ \;
\begin{array}{rcl}
 0 \le & s   & \le 1 \\
 1 \le & d_0 & \le 9 \\
 0 \le & d_1 & \le 9 \\
 0 \le & d_2 & \le 9 \\
-9 \le & e   & \le 9
\end{array}
\right.
\end{equation*}
Such numbers are called the \emph{floating point numbers}.

\begin{itemize}
\item The precision of the calculator is determined by the length of $d_0 . d_1 d_2$.
\[
   \pi \longrightarrow (3.14 \times 10^0).
   \]
\item Results of most arithmetic operations must be rounded.
\[
   \underbrace{(1.23 \times 10^5) + (4.56 \times 10^2)}_{=123456} \longrightarrow (1.23 \times 10^5).
   \]
\item Zero must be represented with a special convention.
\[
   0 \longrightarrow (0.00 \times 10^0).
   \]
\item There is a smallest positive floating point number $N_{\text{min}}$ and there is a largest positive floating point number $N_{\text{max}}$.
\[
   N_{\text{min}} = (1.00 \times 10^{-9})
   \quad\text{and}\quad
   N_{\text{max}} = (9.99 \times 10^{9}).
   \]
\item The set of floating point numbers is finite. For this toy model, there are
\[
   2 \times 9 \times 10 \times 10 \times 19 + 1 = 34201 \text{ floating point numbers.}
   \]
\item The spacing of floating numbers varies. Between $1.00 \times 10^e$ and $1.00 \times 10^{e+1}$, the spacing is $10^{e-2}$.
\end{itemize}

\subsection{Floating Point Numbers: Theoretical Description}
In the computer, numbers are stored using binary bits (0 or 1). So a floating point number must be expressed in base 2:
\[
x = (-1)^s \times 1.b_1 b_2 \ldots {b_d}_{(2)} \times 2^{a_1 a_2 \ldots {a_n}_{(2)} - \beta}
\]
where $s, a_1, \ldots, a_n, b_1, \ldots, b_d$ are either 0 or 1 and $\beta$ is some integer which introduces an offset in the exponent. If we let
\[
E = a_1 a_2 \ldots {a_n}_{(2)} - \beta = \sum_{i=1}^n a_i 2^{n-i} - \beta, \tag{exponent}
\]
and
\[
F = 0.b_1 b_2 \ldots {b_d}_{(2)} = \sum_{i=1}^{d} b_i 2^{-i}, \tag{mantissa}
\]
then a floating point number can be succinctly written as
\[
x = \pm (1 + F) \times 2^E.
\]

Since $E = a_1 a_2 \ldots {a_n}_{(2)} - \beta = \sum_{i=1}^n a_i 2^{n-i} - \beta$,
\[
E_{\text{min}} := -\beta \le E \le 2^n - 1 - \beta =: E_{\text{max}}.
\]
Thus if $x \in \FF$ is positive, then
\[
2^{E_\text{min}} \le x \le (2-2^{-d})2^{E_\text{max}}
\]

Note that
\[
F = \sum_{i=1}^{d} b_i 2^{-i} = 2^{-d} \underbrace{\sum_{k=0}^{d-1} b_{d-k} 2^k}_{=:M},
\quad \text{where } M \in \NN[0, 2^d - 1].
\]
Consequently:

\begin{itemize}
\item The smallest element of $\FF$ that is greater than $1$ is $1 + 2^{-d}$. We define \textbf{machine epsilon} $\meps$ as the gap between these two:
\[
   \meps = 2^{-d}.
   \]
\item There are $2^d$ evenly-spaced numbers on $[2^E, 2^{E+1})$ in $\FF$. In other words:
\vs
\begin{center}
\begin{tcolorbox}[width=0.8\linewidth,arc=0mm,boxrule=0.7pt,colback=white,nobeforeafter]
The spacing in $\FF$ between $2^E$ and $2^{E+1}$ is $2^{E-d}$.
\end{tcolorbox}
\end{center}
\end{itemize}

\subsection{Rounding Function and Relative Error}
Let $\text{fl}:\RR \to \FF$ be a rounding function which maps every real number $x$ to the nearest element of $\FF$. Consider a positive real number $x \in [2^E, 2^{E+1})$ for some allowed exponent $E$.
\begin{itemize}
\item Since the spacing of $\FF$ in the interval is $2^{E-d}$,
\[
   \abs{ \text{fl}(x) - x } \le \frac{1}{2} 2^{E - d}.
   \]
\item It follows that
\[
   \frac{ \abs{\text{fl}(x) - x} }{ \abs{x} } \le \frac{2^{E-d-1}}{2^E} = \frac{1}{2} \meps.
   \]
\item The previous equation is equivalent to
\[
   \text{fl}(x) = x(1+\epsilon) \quad\text{for some } \abs{\epsilon} \le \frac{1}{2} \meps.
   \]
\end{itemize}

\begin{center}
\begin{tcolorbox}[width=0.8\linewidth,arc=0mm,boxrule=0.7pt,colback=white,nobeforeafter]
Every real number is represented as a floating point number with a uniformly bounded relative error.
\end{tcolorbox}
\end{center}

\subsection{IEEE 754 Standard: Double Precision Floating Point Numbers}
MATLAB, by default, uses IEEE \emph{double precision} floating point numbers, stored in memory in 64 bits (or 8 bytes).
\begin{equation*}
  x = (-1)^s \times 1.b_1 b_2 \ldots {b_{52}}_{(2)} \times 2^{a_1 a_2 \ldots {a_{11}}_{(2)} - 1023}
\end{equation*}

\begin{itemize}
\item one bit for the sign ($s$)
\item 11 bits for the exponent ($a_1, a_2, \ldots, a_{11}$, that is, $n = 11$)
\item 52 bits for the mantissa ($b_1, b_2, \ldots, b_{52}$, that is, $d = 52$)
\item the bias in the exponent is $\beta = 1023$.
\end{itemize}

\textbf{Note}. Since $n=11$ and $\beta = 1023$,
\[
-1023 \le E \le 1024.
\]

\begin{itemize}
\item However, the two extreme values $E = -1023$ and $E = 1024$ are used for encoding special quantities.
\item The integers in $-1022 \le E \le 1023$ are used for normal representation of floating point numbers. Thus,
\[
   E_{\text{min}} = -1022
   \quad\text{and}\quad
   E_{\text{max}} = 1023.
   \]
\end{itemize}

The following quantities capture important aspects of double precision numbers, so they are predefined in MATLAB:

\begin{itemize}
\item \textbf{\texttt{eps}} = the \textbf{machine epsilon}
\[
     \mathtt{eps} = 2^{-d}  = 2^{-52} \approx 2.2204 \times 10^{-16}.
   \]
\item \textbf{\texttt{realmin}} = the smallest positive floating point number\footnote{The actual smallest positive number in MATLAB is \texttt{realmin/2\textasciicircum{}52} $\approx 4.9407 \times 10^{-324}$.}
\[
     \mathtt{realmin} = 2^{E_\text{min}} = 2^{-1022} \approx 2.2251 \times 10^{-308}.
   \]
\item \textbf{\texttt{realmax}} = the largest positive floating point number
\[
     \mathtt{realmax} = (2-2^{-d})2^{E_\text{max}} = (2-2^{-52}) 2^{1023} \approx 1.7977 \times 10^{308}.
   \]
\item Numerical results that should be larger than \texttt{realmax} \emph{overflows} to \textbf{\texttt{Inf}}.
\item The result of an undefined arithmetic operation yields \textbf{\texttt{NaN}}.
\end{itemize}

The IEEE standard guarantees that the \emph{relative representation error} and the \emph{relative computational error} have sizes smaller than $\meps$, the \emph{machine epsilon}:
\vs

\begin{itemize}
\item \textbf{Representation}: The floating point representation, $\hat{x} \in \FF$, of $x \in \RR$ satisfies
\[
     \hat{x} = x(1 + \epsilon_1), \qquad \text{for some $\abs{\epsilon_1} \le \frac{1}{2}\,\meps$.}
   \]

\item \textbf{Arithmetic}: The floating point representation, $\hat{x} \oplus \hat{y}$, of the result of $\hat{x} + \hat{y}$ with $\hat{x}, \hat{y} \in \FF$ satisfies
\[
    \hat{x} \oplus \hat{y} = (\hat{x} + \hat{y})(1 + \epsilon_2), \qquad \text{for some $\abs{\epsilon_2} \le \frac{1}{2}\, \meps$.}
  \]
Similarly with $\ominus, \otimes, \odiv$ corresponding to $-, \times, \div$, respectively.
\end{itemize}

\textbf{Computers CANNOT usually}
\begin{itemize}
\item represent a number correctly;
\item add, subtract, multiply, or divide correctly!!
\end{itemize}

Run the following and examine the answers:
\begin{center}
\begin{minipage}[t]{0.8\linewidth}
\begin{lstlisting}[language=matlab,label= ,caption= ,captionpos=b,numbers=none,style=matlab, linewidth=\linewidth]
 format long
 1.2345678901234567890
 12345678901234567890
 (1 + eps) - 1
 (1 + .5*eps) - 1
 (1 + .51*eps) - 1
 n = input(' n = '); ( n^(1/3) )^3 - n
\end{lstlisting}
\end{minipage}
\end{center}
\end{document}
